{"id":"2Qhhdda6Qnbf8RCfUPd4nB9sSt2WDQfEpF7H3gCnZZ4AsfbGMy3RmrCa6gigGY6TkbrrJn4wmHXXNYcVj1bK","title":"top scoring links : rust","displayTitle":"Reddit - Rust","url":"https://www.reddit.com/r/rust/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/rust/top/?sort=top&t=day&limit=6","items":[{"title":"Memory-safe PNG decoders now vastly outperform C PNG libraries","url":"https://www.reddit.com/r/rust/comments/1ha7uyi/memorysafe_png_decoders_now_vastly_outperform_c/","date":1733744525,"author":"/u/Shnatsel","unread":true,"desc":"","content":"<!-- SC_OFF --><div class=\"md\"><p><strong>TL;DR:</strong> Memory-safe implementations of PNG (<a href=\"https://crates.io/crates/png\">png</a>, <a href=\"https://crates.io/crates/zune-png\">zune-png</a>, <a href=\"https://github.com/google/wuffs/\">wuffs</a>) now dramatically outperform memory-unsafe ones (<a href=\"http://www.libpng.org/\">libpng</a>, <a href=\"https://libspng.org/\">spng</a>, <a href=\"https://github.com/nothings/stb\">stb_image</a>) when decoding images.</p> <p>Rust <a href=\"https://crates.io/crates/png\">png</a> crate that tops our benchmark shows <strong>1.8x</strong> improvement over <code>libpng</code> on x86 and <strong>1.5x</strong> improvement on ARM.</p> <h3>How was this measured?</h3> <p>Each implementation is slightly different. It&#39;s easy to show a single image where one implementation has an edge over the others, but this would not translate to real-world performance.</p> <p>In order to get benchmarks that are more representative of real world, we measured decoding times across the entire <a href=\"https://qoiformat.org/benchmark/\">QOI benchmark corpus</a> which contains many different types of images (icons, screenshots, photos, etc).</p> <p>We&#39;ve configured the C libraries to use <a href=\"https://github.com/zlib-ng/zlib-ng\">zlib-ng</a> to give them the best possible chance. Zlib-ng is still not widely deployed, so the gap between the C PNG library you&#39;re probably using is even greater than these benchmarks show!</p> <p>Results on x86 (Zen 4):</p> <pre><code>Running decoding benchmark with corpus: QoiBench image-rs PNG: 375.401 MP/s (average) 318.632 MP/s (geomean) zune-png: 376.649 MP/s (average) 302.529 MP/s (geomean) wuffs PNG: 376.205 MP/s (average) 287.181 MP/s (geomean) libpng: 208.906 MP/s (average) 173.034 MP/s (geomean) spng: 299.515 MP/s (average) 235.495 MP/s (geomean) stb_image PNG: 234.353 MP/s (average) 171.505 MP/s (geomean) </code></pre> <p>Results on ARM (Apple silicon):</p> <pre><code>Running decoding benchmark with corpus: QoiBench image-rs PNG: 256.059 MP/s (average) 210.616 MP/s (geomean) zune-png: 221.543 MP/s (average) 178.502 MP/s (geomean) wuffs PNG: 255.111 MP/s (average) 200.834 MP/s (geomean) libpng: 168.912 MP/s (average) 143.849 MP/s (geomean) spng: 138.046 MP/s (average) 112.993 MP/s (geomean) stb_image PNG: 186.223 MP/s (average) 139.381 MP/s (geomean) </code></pre> <p>You can reproduce the benchmark on your own hardware using the instructions <a href=\"https://github.com/fintelia/corpus-bench/tree/3d0b8b82244400ee8bc33539d18d79fcb329b8fb\">here</a>.</p> <h3>How is this possible?</h3> <p>PNG format is just <a href=\"https://en.wikipedia.org/wiki/Deflate\">DEFLATE</a> compression (same as in <code>gzip</code>) plus <a href=\"https://en.wikipedia.org/wiki/PNG#Filtering\">PNG-specific filters</a> that try to make image data easier for DEFLATE to compress. You need to optimize both PNG filters and DEFLATE to make PNG fast.</p> <h4>DEFLATE</h4> <p>Every memory-safe PNG decoder brings their own DEFLATE implementation. WUFFS gains performance by decompressing entire image at once, which <a href=\"https://nigeltao.github.io/blog/2021/fastest-safest-png-decoder.html#running-off-a-cliff\">lets them go fast without running off a cliff</a>. <code>zune-png</code> uses a similar strategy in its DEFLATE implementation, <a href=\"https://crates.io/crates/zune-inflate\">zune-inflate</a>.</p> <p><code>png</code> crate takes a different approach. It uses <a href=\"https://crates.io/crates/fdeflate\">fdeflate</a> as its DEFLATE decoder, which supports streaming instead of decompressing the entire file at once. Instead it gains performance via clever tricks such as <a href=\"https://fastcompression.blogspot.com/2015/10/huffman-revisited-part-4-multi-bytes.html\">decoding multiple bytes at once</a>.</p> <p>Support for streaming decompression makes <code>png</code> crate more widely applicable than the other two. In fact, there is ongoing experimentation on using Rust <code>png</code> crate as the PNG decoder in Chromium, replacing <code>libpng</code> entirely. <strong>Update:</strong> WUFFS also supports a form of streaming decompression, see <a href=\"https://www.reddit.com/r/rust/comments/1ha7uyi/memorysafe_png_decoders_now_vastly_outperform_c/m19zz77/?context=3\">here</a>.</p> <h4>Filtering</h4> <p>Most libraries use explicit <a href=\"https://medium.com/@anilcangulkaya7/what-is-simd-and-how-to-use-it-3d1125faac89\">SIMD</a> instructions to accelerate filtering. Unfortunately, they are architecture-specific. For example, <code>zune-png</code> is slower on ARM than on x86 because the author hasn&#39;t written SIMD implementations for ARM yet.</p> <p>A notable exception is stb_image, which doesn&#39;t use explicit SIMD and instead came up with a <a href=\"https://github.com/nothings/stb/blob/5c205738c191bcb0abc65c4febfa9bd25ff35234/stb_image.h#L4657-L4668\">clever formulation</a> of the most common and compute-intensive filter. However, due to architectural differences it also only benefits x86.</p> <p>The <code>png</code> crate once again takes a different approach. Instead of explicit SIMD it relies on <a href=\"https://en.wikipedia.org/wiki/Automatic_vectorization\">automatic vectorization</a>. Rust compiler is actually excellent at turning your code into SIMD instructions <a href=\"https://matklad.github.io/2023/04/09/can-you-trust-a-compiler-to-optimize-your-code.html\">as long as you write it in a way that&#39;s amenable to it</a>. This approach lets you write code once and have it perform well everywhere. Architecture-specific optimizations can be added on top of it in the few select places where they are beneficial. Right now x86 uses the <code>stb_image</code> formulation of a single filter, while the rest of the code is the same everywhere.</p> <h3>Is this production-ready?</h3> <p>Yes! </p> <p>All three memory-safe implementations support APNG, reading/writing auxiliary chunks, and other features expected of a modern PNG library.</p> <p><code>png</code> and <code>zune-png</code> have been tested on a wide range of real-world images, with over 100,000 of them in the test corpus alone. And <code>png</code> is used by every user of the <code>image</code> crate, so it has been thoroughly battle-tested.</p> <p>WUFFS PNG v0.4 <a href=\"https://github.com/fintelia/corpus-bench/blob/3d0b8b82244400ee8bc33539d18d79fcb329b8fb/src/main.rs#L404-L405\">seems to fail on grayscale images with alpha</a> in our tests. We haven&#39;t investigated this in depth, it might be a configuration issue on our part rather than a bug. Still, we cannot vouch for WUFFS like we can for Rust libraries.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Shnatsel\"> /u/Shnatsel </a> <br/> <span><a href=\"https://www.reddit.com/r/rust/comments/1ha7uyi/memorysafe_png_decoders_now_vastly_outperform_c/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1ha7uyi/memorysafe_png_decoders_now_vastly_outperform_c/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Flawless Replay - Time traveling debugger for Rust workflows","url":"https://www.reddit.com/r/rust/comments/1haa58t/flawless_replay_time_traveling_debugger_for_rust/","date":1733752254,"author":"/u/bkolobara","unread":true,"desc":"","content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/bkolobara\"> /u/bkolobara </a> <br/> <span><a href=\"https://flawless.dev/replay/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1haa58t/flawless_replay_time_traveling_debugger_for_rust/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Just launched NerveMQ - a SQLite-powered message queue that speaks SQS ðŸš€","url":"https://www.reddit.com/r/rust/comments/1haddsd/just_launched_nervemq_a_sqlitepowered_message/","date":1733761069,"author":"/u/majorpog","unread":true,"desc":"","content":"<!-- SC_OFF --><div class=\"md\"><p>Hey all, I&#39;ve been working on this for a few weeks and wanted to share! It&#39;s called <a href=\"https://github.com/fortress-build/nervemq\">NerveMQ</a> - basically AWS SQS but running locally with SQLite as the backend.</p> <p>Why? Because sometimes you just want a simple message queue that: - Works exactly like SQS (you can use existing SDKs, or use it as a local mock SQS for testing) - Doesn&#39;t need a whole cluster setup - Actually persists your data (sorry Redis) - Is stupid simple to deploy (it&#39;s just one binary)</p> <p>I also just thought it would be fun to build. I may have come up with the other &quot;why&quot;s after the fact.</p> <p>The cool stuff: - Multi-tenant support with namespaces - Built-in auth system - bearer-based auth or AWS SIGv4 - Admin UI for managing everything (+ an admin API) - Apache 2.0 licensed</p> <p>Future plans: - DB-per-queue for higher throughput - High availability via replication and raft consensus</p> <p>It&#39;s still early days but the core functionality is there. Would love to get some feedback from anyone who wants to try it out (and of course, stars) :)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/majorpog\"> /u/majorpog </a> <br/> <span><a href=\"https://www.reddit.com/r/rust/comments/1haddsd/just_launched_nervemq_a_sqlitepowered_message/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1haddsd/just_launched_nervemq_a_sqlitepowered_message/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"State of the Crates 2025","url":"https://www.reddit.com/r/rust/comments/1hafdai/state_of_the_crates_2025/","date":1733765991,"author":"/u/ohrv","unread":true,"desc":"","content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ohrv\"> /u/ohrv </a> <br/> <span><a href=\"https://ohadravid.github.io/posts/2024-12-state-of-the-crates/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1hafdai/state_of_the_crates_2025/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Dioxus 0.6 - Massive Tooling Improvements: Mobile Simulators, Magical Hot-Reloading, Interactive CLI, RSX Autocomplete, Streaming HTML, WGPU Overlays, and more!","url":"https://www.reddit.com/r/rust/comments/1hahy2d/dioxus_06_massive_tooling_improvements_mobile/","date":1733772264,"author":"/u/jkelleyrtp","unread":true,"desc":"","content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/jkelleyrtp\"> /u/jkelleyrtp </a> <br/> <span><a href=\"https://dioxuslabs.com/blog/release-060\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1hahy2d/dioxus_06_massive_tooling_improvements_mobile/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"December 2024 Leadership Council Update","url":"https://www.reddit.com/r/rust/comments/1haj713/december_2024_leadership_council_update/","date":1733775352,"author":"/u/ehuss","unread":true,"desc":"","content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/ehuss\"> /u/ehuss </a> <br/> <span><a href=\"https://blog.rust-lang.org/inside-rust/2024/12/09/leadership-council-update.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/rust/comments/1haj713/december_2024_leadership_council_update/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""}]}