{"id":"BRWJ2YmdUFp1VkEaibbVg9gmctcLUWiZeYcbFJGMkd7Cuwto","title":"Stories by Kyodo Tech on Medium","displayTitle":"Dev - Kyodo-Tech","url":"https://medium.com/@kyodo-tech/feed","feedLink":"https://medium.com/@kyodo-tech?source=rss-ac02ab142942------2","items":[{"title":"Data Flow and Motion in High-Performance Computing","url":"https://medium.com/@kyodo-tech/data-flow-and-motion-in-high-performance-computing-6ec8340c38e9?source=rss-ac02ab142942------2","date":1734237430,"author":"Kyodo Tech","unread":true,"desc":"","content":"<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*NvY_Yl_JB6sA2U5DwgRCfg.png\" /></figure><p>In high-performance computing (HPC), the disparity between compute speed and data movement dominates performance considerations. CPUs execute billions of instructions per second, yet moving data — whether between memory levels or across subsystems — remains orders of magnitude slower. This discrepancy is rooted in physical constraints, such as the speed of light and the time required for electrons to traverse memory buses. Memory bandwidth, latency, and caching hierarchy (L1, L2, L3 caches) are foundational in determining system performance. Understanding and optimizing these factors is central to streamlining data flow and minimizing data motion. The challenge becomes even more significant when working in managed languages like Go, where garbage collection and memory abstraction add further complexity.</p><h3>The Cost of Data Motion vs. Compute</h3><p>The principle that “compute is cheaper than data motion” arises because modern processors are highly optimized for localized operations on data already present in caches. Moving data between caches, main memory, or across interconnects introduces latency that dwarfs computation times. For example, accessing L1 cache can take as little as 1–3 CPU cycles, while accessing main memory may take 100+ cycles. These latencies magnify when data transfers traverse NUMA nodes or I/O subsystems.</p><h4>Memory Bandwidth and Caching</h4><p>Memory bandwidth refers to the rate at which data can be read from or written to memory. It becomes a bottleneck when a program’s working set exceeds the capacity of higher-level caches. Memory-aware optimizations aim to bridge this gap by focusing on how data is accessed and structured.</p><ul><li><strong>Exploiting Cache Hierarchies:</strong> Structure data to maximize temporal and spatial locality. Temporal locality refers to reusing the same data frequently, while spatial locality involves accessing contiguous memory regions. For instance, iterating over a matrix row-wise rather than column-wise takes advantage of how modern CPUs prefetch adjacent memory blocks, drastically reducing cache misses.</li><li><strong>Avoiding Cache Pollution:</strong> Access only the data required for computation, reducing unnecessary evictions of useful cache lines. Polluting the cache with extraneous data results in thrashing, where useful data is repeatedly evicted and reloaded. In Go, this might involve careful iteration patterns for slices and maps to ensure efficient cache usage.</li><li><strong>Leverage Blocking Techniques:</strong> Divide computations into chunks that fit within the L1 or L2 cache size, minimizing the need for repeated memory accesses. For example, matrix multiplication algorithms can be restructured to process submatrices that fit entirely into cache.</li><li><strong>Profile and Optimize Memory Behavior:</strong> Tools such as perf, pprof, and cache simulators can identify bandwidth bottlenecks and optimize data placement. Profiling also reveals whether TLB misses—caused by frequent page table lookups—are affecting performance. This insight is vital when processing large datasets in Go, where heap allocations may exacerbate memory fragmentation.</li></ul><h4>Speed-of-Light Constraints</h4><p>Data motion is constrained by physical distances within the hardware. For example, even within a single CPU, moving data between registers, caches, and RAM involves electrical signal propagation limited by the speed of light. At large scales, such as distributed systems or memory-bound HPC tasks, these propagation delays dominate execution times. Optimizing local data usage minimizes these delays and improves system efficiency.</p><h3>Streamlining Data Flow</h3><p>Streamlining data flow requires designing systems to process data efficiently within the constraints of hardware and software. In Go, the choice of data structures, serialization formats, and concurrency primitives directly impacts data flow and execution efficiency.</p><h4>Serialization: JSON vs. Protobuf vs. FlatBuffers</h4><p>Serialization formats are a key consideration in data-intensive applications. JSON is widely used but inefficient due to its verbose structure and reliance on runtime parsing. Protobuf, in contrast, is compact and well-suited for binary serialization but involves additional memory copies during encoding and decoding. FlatBuffers take this further by enabling zero-copy deserialization, which avoids unnecessary data duplication entirely.</p><ul><li><strong>JSON:</strong> Best for human-readable data exchange but expensive in terms of data motion and parsing. Large JSON payloads frequently trigger multiple allocations during deserialization in Go.</li><li><strong>Protobuf:</strong> Compact and fast, with schema enforcement. However, Protobuf involves intermediate memory allocations due to the need for temporary message structures during decoding.</li><li><strong>FlatBuffers:</strong> Ideal for HPC workloads where zero-copy deserialization reduces data motion and garbage collection pressure. FlatBuffers store serialized data in a manner that allows direct access without parsing.</li></ul><p>For systems requiring high throughput, FlatBuffers often outperform Protobuf by avoiding redundant memory copies, especially for large datasets. However, JSON remains useful for debugging and configuration due to its human-readable format.</p><p>Another <strong>pattern to Avoid</strong> is using JSON for serialization in high-throughput systems.</p><pre>jsonData, _ := json.Marshal(data)<br>json.Unmarshal(jsonData, &amp;parsedData)</pre><p>A <strong>better alternative</strong> is using FlatBuffers for zero-copy deserialization.</p><pre>// Serialize using FlatBuffers<br>builder := flatbuffers.NewBuilder(1024)<br>... // FlatBuffers schema-based data setup<br>buf := builder.FinishedBytes()<br><br>// Access serialized data without copying<br>flatData := GetRootAsFlatData(buf, 0)<br>value := flatData.SomeField()</pre><p>This eliminates the overhead of intermediate allocations and redundant parsing.</p><h4>In-Place Data Transformations</h4><p>Modifying data in place rather than creating copies avoids unnecessary memory allocations and reduces data motion. In Go, this practice is particularly important because it minimizes interactions with the garbage collector. For instance:</p><p>A common <strong>pattern to avoid</strong> is duplicating data during transformations.</p><pre>func doubleSlice(data []int) []int {<br>    result := make([]int, len(data))<br>    for i := range data {<br>        result[i] = data[i] * 2<br>    }<br>    return result<br>}</pre><p>The <strong>better alternative</strong> modifies the data in place.</p><pre>func doubleSlice(data []int) {<br>    for i := range data {<br>        data[i] *= 2<br>    }<br>}</pre><p>This approach eliminates redundant allocations and keeps the data within the cache hierarchy, significantly improving performance in iterative computations. Combining this technique with batched operations ensures that data remains cache-resident during intensive computations.</p><h4>Favor Asynchronous Communication</h4><p>Asynchronous communication can eliminate unnecessary blocking and improve throughput in applications where tasks can progress independently. In Go, channels are the primary mechanism for implementing asynchronous workflows. They decouple sender and receiver goroutines, allowing the system to process data in a pipeline or fan-out/fan-in pattern. However, note that managing concurrency well is equally important, poorly implemented, it can amplify contention.</p><p>Note a <strong>pattern to avoid</strong> with channels is coupling goroutines with frequent intercommunication via channels.</p><pre>func worker(input &lt;-chan int, output chan&lt;- int) {<br>    for num := range input {<br>        output &lt;- num * 2<br>    }<br>}</pre><p>The <strong>better alternative</strong> uses partitioned data processing with independent goroutines.</p><pre>func worker(data []int) {<br>    for i := range data {<br>        data[i] *= 2<br>    }<br>}<br><br>func main() {<br>    data := make([]int, 1000)<br>    chunkSize := 100<br><br>    var wg sync.WaitGroup<br>    for i := 0; i &lt; len(data); i += chunkSize {<br>        wg.Add(1)<br>        go func(slice []int) {<br>            defer wg.Done()<br>            worker(slice)<br>        }(data[i:min(i+chunkSize, len(data))])<br>    }<br>    wg.Wait()<br>}</pre><p>This minimizes synchronization overhead while maintaining concurrency. Asynchronous patterns like this reduce contention and improve scalability by allowing different stages of computation to progress concurrently.</p><h4>Minimize Inter-Goroutine Communication</h4><p>While Go’s lightweight goroutines make concurrency easy, excessive communication between goroutines can lead to bottlenecks. Minimize inter-goroutine communication by designing systems where each goroutine operates independently on localized data.</p><h4>Avoid Locks and Mutexes</h4><p>Locks and mutexes serialize access to shared resources, removing concurrency benefits and increasing contention. Instead, use channels for synchronization or atomic operations for simple counters. When data sharing is unavoidable, minimize critical sections to reduce contention.</p><h3>Scalability as an Architecture Choice</h3><p>Designing for scalability begins at the architectural level. For single-application designs, consider:</p><ul><li><strong>Stateless Services:</strong> Avoid coupling application state to individual processes. Use external stores for state management, enabling horizontal scaling.</li><li><strong>Partitioning Workloads:</strong> Divide tasks into smaller, independent units that can be distributed across multiple worker processes or threads.</li><li><strong>Backpressure Management:</strong> Use bounded channels or rate-limiting techniques to ensure that the system can handle varying workloads without becoming overwhelmed.</li></ul><h3>Memory Allocation and Garbage Collection in Go</h3><p>Go’s garbage collector (GC) is optimized for low-latency applications, but HPC workloads often require precise control over memory to minimize GC interruptions.</p><h4>Escape Analysis</h4><p>Escape analysis determines whether a variable can be allocated on the stack instead of the heap. Stack allocations are much cheaper as they avoid GC involvement. Writing functions that limit pointer escapes helps reduce GC overhead:</p><pre>func createBuffer(size int) []byte {<br>    buf := make([]byte, size)<br>    return buf // Avoid returning a pointer to escape heap allocation<br>}</pre><p>Profiling escape analysis in Go with go build -gcflags=&quot;-m&quot; identifies opportunities for stack allocation. Avoiding heap allocations in performance-critical code paths ensures consistent, low-latency execution.</p><h4>Object Pooling</h4><p>Using sync.Pool for object pooling reduces heap allocations by recycling objects. This approach is particularly effective for short-lived objects like buffers or temporary data structures.</p><p>The <strong>pattern to avoid</strong> is frequent heap allocations leading to GC overhead.</p><pre>func createBuffers(count int, size int) [][]byte {<br>    buffers := make([][]byte, count)<br>    for i := range buffers {<br>        buffers[i] = make([]byte, size)<br>    }<br>    return buffers<br>}</pre><p>A <strong>better alternative</strong> preallocates memory and recycle buffers using sync.Pool.</p><pre>var bufferPool = sync.Pool{<br>    New: func() interface{} {<br>        return make([]byte, 1024)<br>    },<br>}<br><br>func process() {<br>    buf := bufferPool.Get().([]byte)<br>    defer bufferPool.Put(buf)<br>    // Process data with buf<br>}</pre><p>Pooling reduces heap pressure and improves memory reuse, i.e. reduces memory fragmentation and ensures that buffers are allocated contiguously, maximizing cache performance. Over-pooling, however, may retain unused objects, inflating memory usage and requiring careful monitoring.</p><h4>Managing Allocation Patterns</h4><ul><li><strong>Batch Allocation:</strong> Allocate slices and objects in large batches to amortize allocation overhead. Batch allocation improves cache performance by reducing scattered allocations.</li><li><strong>Avoid Fragmentation:</strong> Use contiguous slices rather than fragmented structures to maintain cache-friendly layouts. Fragmentation leads to increased TLB misses and degraded memory access times.</li><li><strong>Minimize Pointer Chaining:</strong> Excessive pointers degrade cache performance due to scattered memory accesses. Flattening data structures minimizes this overhead.</li></ul><p>An exmaple <strong>pattern to avoid</strong> that is not immediately obvious: Iterating over a 2D slice inefficiently by accessing columns (which leads to non-contiguous memory access).</p><pre>for j := 0; j &lt; len(matrix[0]); j++ {<br>    for i := 0; i &lt; len(matrix); i++ {<br>        matrix[i][j] *= 2<br>    }<br>}</pre><p>A <strong>better alternative</strong> iterates row-by-row to leverage spatial locality.</p><pre>for i := 0; i &lt; len(matrix); i++ {<br>    for j := 0; j &lt; len(matrix[0]); j++ {<br>        matrix[i][j] *= 2<br>    }<br>}</pre><p>Adjacent elements in memory are accessed sequentially, reducing cache misses.</p><h4>Memory Alignment</h4><p>Memory alignment ensures that data structures are positioned at memory addresses that are multiples of their size. Misaligned data can lead to performance penalties on some architectures. Go’s memory allocator aligns objects to their size by default, but manual alignment may be necessary for specific use cases, such as SIMD operations.</p><p>To aid memory alignment, <strong>pattern to avoid</strong> is using pointer-heavy or fragmented data structures, leading to scattered memory access.</p><pre>type Node struct {<br>    Value int<br>    Next  *Node<br>}</pre><p>A <strong>better alternative</strong> is to use contiguous slices for data storage where possible.</p><pre>type Node struct {<br>    Values []int<br>}</pre><p>Flattening structures improves cache performance and minimizes pointer dereferencing.</p><h4>GC Tuning in HPC</h4><p>Garbage collection in Go introduces predictable but non-negligible latency. Tuning GC behavior for HPC workloads involves:</p><ul><li><strong>Reduce Allocation Frequency:</strong> Preallocate memory and reuse objects to reduce pressure on the GC.</li><li><strong>Profile GC Activity:</strong> Use pprof to identify regions with high allocation rates and optimize them to minimize GC overhead.</li><li><strong>Adjust GOGC:</strong> The GOGC environment variable controls GC aggressiveness. Lowering it reduces memory usage but increases GC frequency.</li></ul><h3>Managing Performance Counters and Logging</h3><p>Performance counters and logging are essential for understanding application behavior. However, excessive instrumentation can degrade performance, especially in HPC environments.</p><h4>Lock-Free Metrics Collection</h4><p>Using atomic operations for counters ensures thread-safe, low-latency updates without locks:</p><pre>var requestCount uint64<br><br>func incrementRequestCount() {<br>    atomic.AddUint64(&amp;requestCount, 1)<br>}</pre><p>Atomic Compare-and-Swap (CAS) operations can also be used for complex metrics updates, maintaining high performance even under heavy contention.</p><h4>Efficient Logging</h4><p>Logging should balance granularity and overhead. Techniques include:</p><ul><li><strong>Structured Logging:</strong> Use lightweight formats like JSON for logs, ensuring they can be parsed efficiently.</li><li><strong>Batch Logging:</strong> Aggregate log entries in memory and write them in batches to reduce I/O overhead.</li><li><strong>Asynchronous Logging:</strong> Offload log writing to separate goroutines to minimize impact on application performance.</li></ul><h4>Push-Based Metrics Collection</h4><p>Aggregating and occasionally pushing metrics balances real-time monitoring with low overhead. For example:</p><pre>func reportMetrics() {<br>    total := atomic.LoadUint64(&amp;requestCount)<br>    fmt.Printf(&quot;Total Requests: %d\\n&quot;, total)<br>}</pre><p>Batching metrics ensures that reporting does not interfere with computation, aligning with lock-free principles for minimal contention.</p><h3>Conclusion</h3><p>Optimizing data flow and minimizing data motion requires understanding hardware limitations, such as memory bandwidth and cache hierarchies. Go, while a garbage-collected language, is well-suited for HPC when its memory management features are leveraged effectively. Techniques like serialization optimization, object pooling, in-place transformations, asynchronous communication, and lock-free metrics collection help Go applications achieve high throughput and low latency. When paired with careful profiling and tuning, these strategies elevate Go as a practical choice for HPC, balancing developer productivity with computational efficiency.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6ec8340c38e9\" width=\"1\" height=\"1\" alt=\"\">","flags":null,"enclosureUrl":"","enclosureMime":""}]}