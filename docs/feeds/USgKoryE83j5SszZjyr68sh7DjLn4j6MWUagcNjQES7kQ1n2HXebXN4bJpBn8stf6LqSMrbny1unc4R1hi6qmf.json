{"id":"USgKoryE83j5SszZjyr68sh7DjLn4j6MWUagcNjQES7kQ1n2HXebXN4bJpBn8stf6LqSMrbny1unc4R1hi6qmf","title":"top scoring links : golang","displayTitle":"Reddit - Go","url":"https://www.reddit.com/r/golang/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/golang/top/?sort=top&t=day&limit=6","items":[{"title":"Weak Pointers in Go 1.24","url":"https://www.reddit.com/r/golang/comments/1h8svee/weak_pointers_in_go_124/","date":1733579364,"author":"/u/callcifer","unread":true,"desc":"","content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/golang/comments/1h8svee/weak_pointers_in_go_124/\"> <img src=\"https://external-preview.redd.it/SDX5zXKJRN1gxOESbz8_oI7ccQ0_NT6U2V6SCY3E780.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=47fe7188474d79d238a167044ac1063e9e41b8a3\" alt=\"Weak Pointers in Go 1.24\" title=\"Weak Pointers in Go 1.24\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/callcifer\"> /u/callcifer </a> <br/> <span><a href=\"https://victoriametrics.com/blog/go-weak-pointer/index.html\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1h8svee/weak_pointers_in_go_124/\">[comments]</a></span> </td></tr></table>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Is JSON hard in go","url":"https://www.reddit.com/r/golang/comments/1h8tpwi/is_json_hard_in_go/","date":1733582049,"author":"/u/Fantastic-Length5962","unread":true,"desc":"","content":"<!-- SC_OFF --><div class=\"md\"><p>I might just be an idiot but getting even simple nested JSON to work in go has been a nightmare.</p> <p>Would someone be able to point me to some guide or documentation, I haven&#39;t been able to find any thats clear. I want to know how I need to write nested structs and then how I need to structure a hard coded JSON variable. I&#39;ve tried every permutation I can think of and always get weird errors, any help would be appreciated.</p> <p>Also as a side note, is it easier in go to just use maps instead of structs for JSON?</p> <p>Example of what I&#39;m trying to do <a href=\"https://go.dev/play/p/8rw5m5rqAFX\">https://go.dev/play/p/8rw5m5rqAFX</a> (obviously it doesnt work because I dont know what I&#39;m doing)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Fantastic-Length5962\"> /u/Fantastic-Length5962 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1h8tpwi/is_json_hard_in_go/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1h8tpwi/is_json_hard_in_go/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"stropt - statistics and optimization for C structs, written in go","url":"https://www.reddit.com/r/golang/comments/1h8u3ep/stropt_statistics_and_optimization_for_c_structs/","date":1733583204,"author":"/u/Abathargh","unread":true,"desc":"","content":"<!-- SC_OFF --><div class=\"md\"><p>Hi everyone!</p> <p>I&#39;m an embedded sw dev and I&#39;ve been writing a small tool in go that collects statistics and suggests layout optimizations for C aggregate types/structs: <code>stropt</code>.</p> <p><a href=\"https://github.com/Abathargh/stropt\">on github @Abathargh/stropt</a></p> <p>This is something that came up a lot of times in my personal sw dev journey and I wanted to have a tool that allowed me to quickly get this information without having to use a compiler with a custom file each time.</p> <p>I love using go for this kind of stuff (CLI tools, data processing), and the fact that we have great projects like the ultra-nice modernc.org/cc compiler frontend, and the wonderful lipgloss library from charm is a huge plus!</p> <p>An example usage for <code>stropt</code> would this: let&#39;s say you have a C struct:</p> <p>```c struct inner { char flags; int dlen; };</p> <p>struct test { const char * str; short stats; struct inner meta; short inlen; int len; }; ```</p> <p>and you keep this in a file named <code>opt.c</code>; then you can call stropt on this file like this:</p> <p><code>bash stropt -optimize -verbose -file tests/opt.c &quot;struct test&quot; </code></p> <p>And get the information (and optimization suggestions) in a nicely formatted fashion:</p> <p><a href=\"https://raw.githubusercontent.com/Abathargh/stropt/refs/heads/master/opt.png\">https://raw.githubusercontent.com/Abathargh/stropt/refs/heads/master/opt.png</a></p> <p>Hope you like it!</p> <p>Edit: forgot the link to the repo!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Abathargh\"> /u/Abathargh </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1h8u3ep/stropt_statistics_and_optimization_for_c_structs/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1h8u3ep/stropt_statistics_and_optimization_for_c_structs/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Help Optimizing Memory Usage in Go Decompression Implementation","url":"https://www.reddit.com/r/golang/comments/1h8udtb/help_optimizing_memory_usage_in_go_decompression/","date":1733584037,"author":"/u/rasparac","unread":true,"desc":"","content":"<!-- SC_OFF --><div class=\"md\"><p>Hi everyone,</p> <p>I&#39;m working on a Go project that involves a listener for Google Cloud Storage (GCS). The listener periodically checks a bucket, lists the files, and processes them one by one. As part of the processing, files are decompressed if they are in formats like <code>gzip</code>, <code>bzip2</code>, or <code>zip</code>.While profiling my implementation, I noticed significant memory usage, particularly during the <code>gzip.Reader.Reset</code> and <code>bufio.NewReaderSize</code> stages. Here are some details:</p> <ol> <li>I&#39;m using <code>gzip.Reader</code> with a <code>sync.Pool</code> to reduce allocations. However, <code>gzip.Reader.Reset</code> alone is consuming over 1GB of memory in my benchmarks.</li> <li><code>bufio.NewReaderSize</code> also appears to allocate a lot of memory (over 11GB in some profiles), which seems excessive for a 64KB buffer.</li> <li>I&#39;m currently using <code>io.ReadAll</code> to read decompressed data, which I suspect is contributing to the memory overhead since it reads everything into memory.</li> </ol> <ul> <li>How can I optimize memory usage for decompressing <code>gzip</code> files in Go while maintaining good performance?</li> <li>Are there alternatives to <code>gzip.Reader</code> that might help reduce memory overhead?</li> <li>Could my use of <code>bufio.NewReaderSize</code> or the way I&#39;m pooling readers be causing this unexpected memory usage?</li> </ul> <p>Any advice or insights would be greatly appreciated!<br/> Thanks in advance for your help!</p> <p>Implementation:</p> <p>This is the command i used for running benchmark:</p> <pre><code>go test -bench=. -benchmem -cpuprofile cpu_single.prof -memprofile mem_single.prof -benchtime=3s </code></pre> <p>Results:</p> <pre><code>File: decompress.test Type: alloc_space Time: Dec 7, 2024 at 3:58pm (CET) Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options) (pprof) top 10 Showing nodes accounting for 12.54GB, 99.48% of 12.61GB total Dropped 34 nodes (cum &lt;= 0.06GB) Showing top 10 nodes out of 13 flat flat% sum% cum cum% 10.87GB 86.23% 86.23% 10.87GB 86.23% bufio.NewReaderSize (inline) 1.33GB 10.55% 96.78% 1.33GB 10.55% io.ReadAll 0.13GB 1.06% 97.84% 0.13GB 1.06% bytes.NewReader 0.11GB 0.9% 98.75% 0.11GB 0.9% compress/flate.(*dictDecoder).init (inline) 0.06GB 0.46% 99.21% 12.41GB 98.47% github.com/nstrlabs/GoogleCloudStorageListener/v0_0_1/internal/decompress.Decompress 0.02GB 0.14% 99.35% 0.13GB 1.04% compress/flate.NewReader 0.02GB 0.13% 99.48% 11.02GB 87.42% github.com/xxx/GoogleCloudStorageListener/v0_0_1/internal/decompress.Reader 0 0% 99.48% 10.87GB 86.23% bufio.NewReader (inline) 0 0% 99.48% 11GB 87.27% compress/gzip.(*Reader).Reset 0 0% 99.48% 0.13GB 1.04% compress/gzip.(*Reader).readHeader </code></pre> <p>When i run list on</p> <pre><code>github.com/xxxx/GoogleCloudStorageListener/v0_0_1/internal/decompress.Reader </code></pre> <p>i get this</p> <pre><code>17MB 11.02GB (flat, cum) 87.42% of Total . . 105:func Reader(ext string, r io.ReadCloser) (io.ReadCloser, error) { . . 106: switch ext { . . 107: case &quot;.gz&quot;: . 2.50MB 108: gzReader := gzipReaderPool.Get().(*gzip.Reader) . . 109: . 11GB 110: if err := gzReader.Reset(r); err != nil { . . 111: gzReader.Close() . . 112: gzipReaderPool.Put(gzReader) . . 113: return nil, fmt.Errorf(&quot;failed to reset gzip reader: %w&quot;, err) . . 114: } . . 115: 17MB 17MB 116: return &amp;gzipWrap{ . . 117: gzip: gzReader, . . 118: }, nil . . 119: case &quot;.bz2&quot;: . . 120: return io.NopCloser(bzip2.NewReader(r)), nil . . 121: default: </code></pre> <p>reader io.ReadCloser parameter in the Decompress function is from google cloud storage object reader</p> <pre><code>func (gs *gcpStorage) ObjectReader( ctx context.Context, name string, ) (io.ReadCloser, error) { return gs.bucket.Object(name).NewReader(ctx) } </code></pre> <p>Implementation:</p> <pre><code>func Decompress( reader io.ReadCloser, fileExtension, contentType string, ) ([][]byte, error) { // Handle ZIP files if IsZip(fileExtension, contentType) { return handleZip(reader) } // Handle other supported compressions compressReader, err := Reader(fileExtension, reader) if err != nil { return nil, fmt.Errorf(&quot;failed to create decompression reader: %w&quot;, err) } defer compressReader.Close() data, err := io.ReadAll(compressReader) if err != nil { return nil, fmt.Errorf(&quot;failed to read compressed data: %w&quot;, err) } return [][]byte{data}, nil } func Reader(ext string, r io.ReadCloser) (io.ReadCloser, error) { switch ext { case &quot;.gz&quot;: gzReader := gzipReaderPool.Get().(*gzip.Reader) reader := bufio.NewReader(r) if err := gzReader.Reset(reader); err != nil { gzipReaderPool.Put(gzReader) return nil, fmt.Errorf(&quot;failed to reset gzip reader: %w&quot;, err) } return &amp;gzipWrap{ gzip: gzReader, }, nil case &quot;.bz2&quot;: return io.NopCloser(bzip2.NewReader(r)), nil default: return nil, fmt.Errorf(&quot;unsupported compression type: %s&quot;, ext) } } </code></pre> <p>Here is my benchmark test:</p> <pre><code>var data [][]byte func BenchmarkDecompress(b *testing.B) { // Generate gzip data once b.ReportAllocs() // Track memory allocations gzipData, err := generateGzipData([]byte(&quot;content&quot;)) if err != nil { b.Fatalf(&quot;failed to generate gzip data: %v&quot;, err) } var d [][]byte for i := 0; i &lt; b.N; i++ { b.StopTimer() reader := io.NopCloser(bytes.NewReader(gzipData)) // Create new reader each time b.StartTimer() d, err = Decompress(reader, &quot;.gz&quot;, &quot;application/gzip&quot;) if err != nil { b.Fatalf(&quot;failed to decompress: %v&quot;, err) } reader.Close() } data = d } </code></pre> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/rasparac\"> /u/rasparac </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1h8udtb/help_optimizing_memory_usage_in_go_decompression/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1h8udtb/help_optimizing_memory_usage_in_go_decompression/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Seeking GitHub Repositories for End-to-End Monitoring & Observability in Golang + Kubernetes + MongoDB + MySQL Stack","url":"https://www.reddit.com/r/golang/comments/1h8zrrg/seeking_github_repositories_for_endtoend/","date":1733598748,"author":"/u/mnmadhukar02","unread":true,"desc":"","content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;m building an application where all cron jobs and long-running processors are developed in <strong>Golang</strong> and deployed in a <strong>Kubernetes</strong> environment. The application also leverages <strong>MongoDB</strong> and <strong>MySQL</strong> for database needs.</p> <p>I’m looking to implement <strong>end-to-end monitoring and observability</strong> to ensure high performance, reliability, and scalability. The requirements include:</p> <h1>Monitoring and Observability Needs</h1> <ol> <li><strong>Metrics Monitoring</strong> <ul> <li>Tools like Prometheus, kube-state-metrics, and Grafana.</li> <li>Support for custom metrics from Golang applications and databases.</li> </ul></li> <li><strong>Centralized Logging</strong> <ul> <li>ELK Stack (or alternatives like Fluentd, Loki).</li> <li>Kubernetes and application logs in structured formats for analysis.</li> </ul></li> <li><strong>Distributed Tracing</strong> <ul> <li>Tools like Jaeger or OpenTelemetry for request tracing across services.</li> </ul></li> <li><strong>Alerting</strong> <ul> <li>Rule-based alerts with tools like Alertmanager or integrations with Opsgenie/PagerDuty.</li> </ul></li> <li><strong>Database Monitoring</strong> <ul> <li>Query analytics, replication metrics, and performance insights for MongoDB/MySQL.</li> </ul></li> <li><strong>Error and Security Monitoring</strong> <ul> <li>Tools like Sentry for error tracking and Falco/Trivy for Kubernetes security.</li> </ul></li> <li><strong>Visualization</strong> <ul> <li>Dashboards integrating metrics, logs, and traces in one place using Grafana.</li> </ul></li> <li><strong>Chaos Engineering, Backup, and Recovery</strong> <ul> <li>Tools like LitmusChaos/Chaos Mesh for resilience testing.</li> <li>Velero for Kubernetes backups.</li> </ul></li> </ol> <p><strong>My Ask:</strong><br/> Are there any <strong>open-source GitHub repositories</strong> or starter projects that integrate most (or all) of these tools together? Something that’s ready to deploy or provides a solid foundation would be ideal.</p> <p>If you’ve worked on similar setups or know of repos that match these needs, please share!</p> <p>Thanks in advance for your help!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/mnmadhukar02\"> /u/mnmadhukar02 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1h8zrrg/seeking_github_repositories_for_endtoend/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1h8zrrg/seeking_github_repositories_for_endtoend/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"is there a scenario in Go that it doesn't matter to use buffered or unbuffered channel?","url":"https://www.reddit.com/r/golang/comments/1h9emaq/is_there_a_scenario_in_go_that_it_doesnt_matter/","date":1733647343,"author":"/u/UnusualAgency2744","unread":true,"desc":"","content":"<!-- SC_OFF --><div class=\"md\"><p>I am new to go concurrency, and I am trying to make sense of buffered/unbuffered channel. Assume the following hypothetical scenario:</p> <p>Main starts a bunch of goroutines, and these goroutines all scrape data continuously from an API, and the data is stored in a scoped slice in each go routine (no sending to channel at this point). After a context timed out, these go routine loop through the slice and send each data to the channel.</p> <p>The main go routine then reads the data from the channel and do some operation e.g write to csv.</p> <p>Does it matter then, to use a buffered or unbuffered channel to achieve the correctness of the program? How about if we consider performance?</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/UnusualAgency2744\"> /u/UnusualAgency2744 </a> <br/> <span><a href=\"https://www.reddit.com/r/golang/comments/1h9emaq/is_there_a_scenario_in_go_that_it_doesnt_matter/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/golang/comments/1h9emaq/is_there_a_scenario_in_go_that_it_doesnt_matter/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""}]}