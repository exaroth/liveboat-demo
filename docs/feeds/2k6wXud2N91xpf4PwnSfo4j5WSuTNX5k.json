{"id":"2k6wXud2N91xpf4PwnSfo4j5WSuTNX5k","title":"Tech News - Last 2 days","displayTitle":"Tech News - Last 2 days","url":"","feedLink":"","items":[{"title":"Darktable 5.0 Released With Many UI/UX Improvements","url":"https://www.phoronix.com/news/Darktable-5.0","date":1734806707,"author":"Michael Larabel","unread":true,"desc":"","content":"In time for editing any end-of-year/holiday photos, Darktable 5.0 is out today as a major update to this open-source RAW photography workflow application...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Hydroxychloroquine-Promoting COVID Study Retracted After 4 Years","url":"https://science.slashdot.org/story/24/12/20/141202/hydroxychloroquine-promoting-covid-study-retracted-after-4-years?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1734806040,"author":"EditorDavid","unread":true,"desc":"","content":" Nature magazine reports that \"A study that stoked enthusiasm for the now-disproven idea that a cheap malaria drug can treat COVID-19 has been retracted &mdash; more than four-and-a-half years after it was published.\"\n\nResearchers had critiqued the controversial paper many times, raising concerns about its data quality and an unclear ethics-approval process. Its eventual withdrawal, on the grounds of concerns over ethical approval and doubts about the conduct of the research, marks the 28th retraction for co-author Didier Raoult, a French microbiologist, formerly at Marseille's Hospital-University Institute Mediterranean Infection (IHU), who shot to global prominence in the pandemic. French investigations found that he and the IHU had violated ethics-approval protocols in numerous studies, and Raoult has now retired. \n\nThe paper, which has received almost 3,400 citations according to the Web of Science database, is the highest-cited paper on COVID-19 to be retracted, and the second-most-cited retracted paper of any kind.... \nBecause it contributed so much to the HCQ hype, \"the most important unintended effect of this study was to partially side-track and slow down the development of anti-COVID-19 drugs at a time when the need for effective treatments was critical\", says Ole S&oslash;gaard, an infectious-disease physician at Aarhus University Hospital in Denmark, who was not involved with the work or its critiques. \"The study was clearly hastily conducted and did not adhere to common scientific and ethical standards....\" \nThree of the study's co-authors had asked to have their names removed from the paper, saying they had doubts about its methods, the retraction notice said. \n\nNature includes this quote from a scientific-integrity consultant in San Francisco, California. \"This paper should never have been published &mdash; or it should have been retracted immediately after its publication.\" \n\n\"The report caught the eye of the celebrity doctor Mehmet Oz,\" the Atlantic reported in April of 2020 (also noting that co-author Raoult \"has made news in recent years as a pan-disciplinary provocateur; he has questioned climate change and Darwinian evolution...\") \n\nAnd Nature points out that while the study claimed good results for the 20 patients treated with HCQ, six more HCQ-treated people in the study actually dropped out before it was finished. And of those six people, one died, while three more \"were transferred to an intensive-care unit.\" \n\n\n\nThanks to Slashdot reader backslashdot for sharing the news.\n<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Hydroxychloroquine-Promoting+COVID+Study+Retracted+After+4+Years%3A+https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F20%2F141202%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F20%2F141202%2Fhydroxychloroquine-promoting-covid-study-retracted-after-4-years%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://science.slashdot.org/story/24/12/20/141202/hydroxychloroquine-promoting-covid-study-retracted-after-4-years?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23557047&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"OpenAI’s GPT-5 reportedly falling short of expectations","url":"https://techcrunch.com/2024/12/21/openais-gpt-5-reportedly-falling-short-of-expectations/","date":1734805832,"author":"Anthony Ha","unread":true,"desc":"","content":"<p>OpenAI’s efforts to develop its next major model, GPT-5, are running behind schedule, with results that don’t yet justify the enormous costs, according to a new report in The Wall Street Journal. This echoes an earlier report in The Information suggesting that OpenAI is looking to new strategies as GPT-5 might not represent as big [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"OpenAI announces new o3 model — but you can’t use it yet","url":"https://techcrunch.com/2024/12/21/openai-announces-new-o3-model-but-you-cant-use-it-yet/","date":1734804300,"author":"Cody Corrall","unread":true,"desc":"","content":"<p>Welcome back to Week in Review. This week, we’re looking at OpenAI’s last — and biggest — announcement from its “12 Days of OpenAI” event; Apple’s potential entrance into the foldable market; and why Databricks is choosing to wait to go public. Let’s get into it. P.S. We’re off for the holidays! Week in Review [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"WEF President Advocates for a Supervisory Algorithm to Guide AI for the Greater Good","url":"https://hackernoon.com/wef-president-advocates-for-a-supervisory-algorithm-to-guide-ai-for-the-greater-good?source=rss","date":1734800413,"author":"The Sociable","unread":true,"desc":"","content":"<p>World Economic Forum (WEF) president Børge Brende suggests that there should be an algorithm above other algorithms in order to better serve humankind.</p>\n<p>\\\nSpeaking at the&nbsp;<a href=\"https://dohaforum.org/\">Doha Forum</a>&nbsp;in Qatar during a session on “<strong>The Geopolitics of Artificial Intelligence</strong>” over the weekend,&nbsp;<a href=\"https://www.youtube.com/watch?v=m-ZlGMaVHpk\">Brende said</a>&nbsp;that China and the US have realized that whoever comes out on top with AI will be the most powerful nation, and there should be an algorithm to keep other algorithms in check for the sake of humankind.</p>\n<p>\\</p>\n<blockquote>\n  <p><em>“<strong>I think we need an agreement on the algorithm above the algorithms, maybe an algorithm that can control that the algorithms do work in the interest of humankind</strong>“</em></p>\n  <p>WEF President Børge Brende, Doha Forum, December 2024</p>\n</blockquote>\n<p><a href=\"https://x.com/TimHinchliffe/status/1866142245860094070?embedable=true\">https://x.com/TimHinchliffe/status/1866142245860094070?embedable=true</a></p>\n<p>\\\n“<strong><em>These are superpowers without superpowers, and they know that the future superpower is GenAI, and GenAI can be a force for good; it can increase productivity with 10 percent in the coming decade — that’s a lot of prosperity, but</em></strong>&nbsp;<strong><em>I think it’s a good idea that the humans are on top of the algorithms and not the algorithms on top of the humans</em></strong>,” said the WEF president.</p>\n<p>\\\n“<strong><em>I think we need an agreement on the algorithm above the algorithms, maybe an algorithm that can control that the algorithms do work in the interest of humankind</em></strong>,” he added.</p>\n<p>\\</p>\n<blockquote>\n  <p><strong><em>“I see that the power of AI can be so much for good, something that nuclear weapons have never been, but we have to make sure we don’t lose the baby with the bathwater”</em></strong></p>\n  <p>WEF President Børge Brende, Doha Forum, December 2024</p>\n</blockquote>\n<p><a href=\"https://x.com/TimHinchliffe/status/1866146866594754610?embedable=true\">https://x.com/TimHinchliffe/status/1866146866594754610?embedable=true</a></p>\n<p>\\\nWhile recognizing the potential of AI to create productivity and prosperity in the world, Brende also said that one of the downsides is the trillions of dollars lost to cybercrime, and that China and the US should come to regulatory agreements to combat that.</p>\n<p>\\\n<strong><em>“I see that the power of AI can be so much for good, something that nuclear weapons have never been, but we have to make sure we don’t lose the baby with the bathwater</em></strong>,” said Brende.</p>\n<p>\\\n“<strong><em>And $10 trillion USD can be lost annually in cybercrime by 2025, and I think both China and the US have to agree that that is not a good use of money — that should be stopped now</em></strong>,” he added.</p>\n<p>\\\nWhile the US and China battle for AI supremacy, the WEF president said he was concerned about the “global south” being left behind, especially Africa, where only 20 percent of the population has internet access and large swaths still don’t have reliable electricity.</p>\n<p>\\</p>\n<blockquote>\n  <p><strong><em>“In Africa, 20 percent of the population have access to internet. How can you then be part of this revolution? One thing is you don’t have access to electricity, then you don’t have access to the internet”</em></strong></p>\n  <p>WEF President Børge Brende, Doha Forum, December 2024</p>\n</blockquote>\n<p><a href=\"https://www.youtube.com/watch?v=m-ZlGMaVHpk&embedable=true\">https://www.youtube.com/watch?v=m-ZlGMaVHpk&embedable=true</a></p>\n<p>\\\n“<strong><em>In Africa, 20 percent of the population have access to internet</em></strong>,” said Brende.</p>\n<p>\\\n“<strong><em>How can you then be part of this revolution? One thing is you don’t have access to electricity, then you don’t have access to the internet,</em></strong>” he added.</p>\n<p>\\\nWith developed nations and big tech platforms being the “winners” in the AI race, Brende turned back to the US and China, stating:</p>\n<p>“<strong><em>I do hope that there is enough self-interest, and they do see that it is in their self-interest from the big players to agree on some traffic rules</em></strong>.”</p>\n<p>\\\nApart from regulations and competition between the US and China, another key point concerning the geopolitics of AI was how to power data centers and where that power was coming from.</p>\n<p>\\\nYou can find out more about that in our follow-up story:&nbsp;<a href=\"https://sociable.co/big-tech/us-natural-gas-power-ai-esg-logistics-goldman-sachs/\">here</a>.</p>\n<hr />\n<p>:::info\nTim Hinchliffe, Editor, The Sociable</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"California's Population Jumps Back to Near Pre-Pandemic Levels","url":"https://news.slashdot.org/story/24/12/21/056208/californias-population-jumps-back-to-near-pre-pandemic-levels?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1734798840,"author":"EditorDavid","unread":true,"desc":"","content":"\"California's population grew this year by nearly a quarter of a million residents,\" reports the Los Angeles Daily News, \"closing in on record-high population levels the Golden State reached before the pandemic, the U.S. Census Bureau reported Thursday.\" \nAlthough \"Data showed the state is growing more slowly than the country as a whole and other large states in the South...\"\n\nThe Census Bureau's Vintage 2024 population estimates show California's population on July 1, 2024 was 39,431,000, an increase of 233,000 from the year before, and just 125,000 short of the 2020 high point. \nFor Jeff Bellisario, executive director of the Bay Area Council Economic Institute, there are two ways to look at the new data. \"There's the optimistic look that in the past year, we have seen the population increase... bigger increases than we have in a decade, so I do think there is some truth to the narrative of folks coming back to California,\" he said. On the other hand, California is still far behind the population gains made in states like Florida and Texas. \"We are still trying to claw back to where we were pre-pandemic,\" Bellisario said. \"It's going to take us a few more years to get to solid population growth numbers.\" California had the third most new residents, with the population growing by about 0.59%. Florida and Texas saw more new residents, and top the list of states with the largest increases by raw numbers... \n\nOverall, the population of the entire country grew by about 0.9%, slightly outpacing California's growth.\n \n\nA graph accompanying the article shows California's population increasing steadily until the pandemic &mdash; which produced a sudden drop that the article seems to attribute to pandemic restrictions (including restrictions on entering the country). And then this year there was a sudden spike back to nearly where it was before the pandemic.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=California's+Population+Jumps+Back+to+Near+Pre-Pandemic+Levels%3A+https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F21%2F056208%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F21%2F056208%2Fcalifornias-population-jumps-back-to-near-pre-pandemic-levels%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://news.slashdot.org/story/24/12/21/056208/californias-population-jumps-back-to-near-pre-pandemic-levels?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23557719&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Developer Kirill Sergeev Speaks on Empowering Healthcare System with Latest AI-solutions","url":"https://hackernoon.com/developer-kirill-sergeev-speaks-on-empowering-healthcare-system-with-latest-ai-solutions?source=rss","date":1734798615,"author":"Jon Stojan Media","unread":true,"desc":"","content":"<p>\\\nThe pressure on healthcare systems to process vast amounts of patient data efficiently has never been greater. The growing need for real-time insights in clinical trials, patient management, and diagnostics is driving the demand for advanced data solutions. According to a 2023 report by IDC, the global healthcare data analytics market is projected to reach $45 billion by 2027. However, as data volumes continue to grow, traditional systems are struggling to keep up, leading to delays in drug development and clinical trials.</p>\n<p>\\\nOne of the most critical challenges today is how to handle large datasets without compromising speed or accuracy. For pharmaceutical companies, timely data analysis can mean the difference between a successful trial and costly delays. This is where innovative solutions in data processing become essential for accelerating drug development and improving patient outcomes.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/InxBRjRIs6M1kdhuWcyNHiiUrxm1-mx034y7.jpeg\" alt=\"Kirill Sergeev\" /></p>\n<p>\\\nRecognizing these challenges, Kirill Sergeev, a seasoned backend developer and machine learning engineer, has been developing cutting-edge systems that streamline the processing of medical data. By leveraging high-performance technologies, Sergeev has optimized data pipelines to enable companies to handle up to 100 terabytes of data daily with remarkable efficiency.</p>\n<p>\\</p>\n<blockquote>\n  <p><strong>\"The key to managing large datasets efficiently lies in creating systems that are not just fast but also flexible enough to handle complex, dynamic data flows. In the medical field, where data needs to be processed quickly and securely, we cannot afford inefficiencies,\"</strong></p>\n  <p>\\\n  -Kirill Sergeev</p>\n</blockquote>\n<p>\\\nOne of the significant challenges in clinical data management is the lengthy process of deploying new machine learning models. Previously, deploying algorithms for clinical trials could take anywhere from two to three days, slowing down the ability to respond to new data. By redesigning data pipelines and integrating robust CI/CD processes, Sergeev has successfully reduced this time to just 1-2 hours.</p>\n<p>\\\nThis streamlined process allows pharmaceutical companies to test and integrate new findings more quickly, ultimately accelerating the drug development timeline.</p>\n<p>\\\n\"In any system dealing with high volumes of data, efficiency is key. It’s not just about handling data faster; it’s about ensuring that the results are accurate, actionable, and available immediately when needed,\" Sergeev adds.</p>\n<p>\\\nSpeed and accuracy are essential in sectors where real-time insights can significantly impact patient outcomes. Sergeev’s work in optimizing data systems has cut response times for processing large volumes of medical data from 1.5 minutes to just 500 milliseconds. This level of performance is crucial for enabling healthcare providers to make timely decisions based on up-to-date information.</p>\n<p>\\\nBy mostly adapting two types of approaches:&nbsp; batch based and lambda-based, Sergeev has developed a hybrid architecture that ensures secure, scalable, and efficient data processing. This approach enables rapid data retrieval and real-time analysis, which is vital for managing clinical trials and patient records.</p>\n<p>\\\nWhile the healthcare sector benefits significantly from these advancements, Sergeev’s methodologies are also transforming other industries, such as fintech and e-commerce. By applying similar techniques, companies in these sectors have achieved substantial gains in efficiency.</p>\n<p>In a fintech project, for instance, Sergeev’s microservice architecture reduced transaction processing times by 35%, while also enhancing system security. In the e-commerce domain, his methods led to a 40% boost in operational efficiency by optimizing real-time inventory management systems.</p>\n<p>\\\n\"The approach is universal,\" Sergeev notes. \"Whether it’s healthcare, finance, or retail, the key is to build systems that are scalable and resilient to handle the increasing demands of modern data workloads.\"</p>\n<p>\\\nThe future of healthcare lies in leveraging real-time data to drive faster and more accurate decision-making. As the sector continues to embrace data-driven practices, innovations like those developed by Sergeev will be crucial for enhancing patient care and speeding up drug development.</p>\n<p>\\\n\"I think the future of data processing in healthcare lies in real-time insights that can inform quicker decisions. We’re only scratching the surface, but the potential to revolutionize patient care and drug development is immense,\" Sergeev concludes.</p>\n<p>\\\nBy addressing the pressing needs of modern healthcare data management, Kirill Sergeev’s work is paving the way for a more efficient, data-driven approach that not only benefits the medical industry but also sets new standards for data processing in other high-load sectors.</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Google pushes back against DOJ’s ‘interventionist’ remedies in antitrust case","url":"https://techcrunch.com/2024/12/21/google-pushes-back-against-dojs-interventionist-remedies-in-antitrust-case/","date":1734797574,"author":"Anthony Ha","unread":true,"desc":"","content":"<p>Google has offered up its own proposal in a recent antitrust case that saw the US Department of Justice argue that Google must sell its Chrome browser. US District Court Judge Amit Mehta ruled in August that Google had acted illegally to maintain a monopoly in online search, with the DOJ then proposing a number [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The HackerNoon Newsletter: Next Holiday Season, Ignore Everyone Except One Customer (12/21/2024)","url":"https://hackernoon.com/12-21-2024-newsletter?source=rss","date":1734797073,"author":"Noonification","unread":true,"desc":"","content":"\n              \n        <p><strong>How are you, hacker?</strong></p>\n        <br />\n        <p>🪐 What’s happening in tech today, December 21, 2024?</p>\n        <br />\n        <p>\n          The\n          <a href=\"https://hackernoon.com/noonification\" target=\"_blank\" rel=\"noopener\"> HackerNoon Newsletter</a>\n          brings the HackerNoon \n          <a href=\"https://hackernoon.com\" target=\"_blank\" rel=\"noopener\">homepage</a>\n          straight to your inbox.\n          <a href=\"https://hackernoon.com/on-this-day\" target=\"_blank\" rel=\"noopener\">On this day,</a>\n          \n            <strong>The Mayan Long Count Calendar Ended</strong> in 2012,  <strong>Gangnam Style Reached One Billion Views</strong> in 2012,  <strong>The First Ever Basketball Game is Played</strong> in 1891, \n          \n          and  we present you with these top quality stories. \n          \n            From \n        <a href=\"https://hackernoon.com/how-i-made-$700-a-month-with-my-open-source-scheduling-tool\" class=\"eventTitle\"><strong>How I Made $700 a Month With My Open-source Scheduling Tool</strong></a>\n       to \n        <a href=\"https://hackernoon.com/new-framework-by-beeble-researchers-promises-to-bring-realistic-glow-to-digital-portraits-using-ai\" class=\"eventTitle\"><strong>New Framework by Beeble Researchers Promises to Bring Realistic Glow to Digital Portraits Using AI</strong></a>,\n       let’s dive right in.\n          \n        </p>\n      \n              \n          <h2><a href=\"https://hackernoon.com/new-framework-by-beeble-researchers-promises-to-bring-realistic-glow-to-digital-portraits-using-ai\">New Framework by Beeble Researchers Promises to Bring Realistic Glow to Digital Portraits Using AI</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/2jqChkrv03exBUgkLrDzIbfM99q2-8s02svm.jpeg\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/autoencoder\">@autoencoder</a> [ 4 Min read ] Researchers at Beeble AI have developed a method for improving how light and shadows can be applied to human portraits in digital images. <a href=\"https://hackernoon.com/new-framework-by-beeble-researchers-promises-to-bring-realistic-glow-to-digital-portraits-using-ai\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/three-sexy-ai-api-keys-to-access-more-models-than-hugh-hefner\">Three Sexy AI API Keys to Access More Models than Hugh Hefner</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/SQfK3yj7LZfIYwS6962nUdsbg9L2-c3122ij.png\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/pauldentro\">@pauldentro</a> [ 3 Min read ] 3 Sexy AI API Keys to Access More Models than Hugh Hefner <a href=\"https://hackernoon.com/three-sexy-ai-api-keys-to-access-more-models-than-hugh-hefner\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/next-holiday-season-ignore-everyone-except-one-customer\">Next Holiday Season, Ignore Everyone Except One Customer</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/pC9ETKtznagNvUGUgZ8sqefPnUA3-7x035ea.jpeg\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/bigmao\">@bigmao</a> [ 5 Min read ] Holiday marketing sucks. Next year, ditch the ads and discount vouchers for a campaign targeting one person. Heres how.  <a href=\"https://hackernoon.com/next-holiday-season-ignore-everyone-except-one-customer\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/how-i-made-$700-a-month-with-my-open-source-scheduling-tool\">How I Made $700 a Month With My Open-source Scheduling Tool</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/Fyq3lKdN20Xy6A5PTptozSOTd5K2-ve03427.png\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/nevodavid10\">@nevodavid10</a> [ 3 Min read ] I built a social media scheduling tool (many exist in the market) and created an open-source version. <a href=\"https://hackernoon.com/how-i-made-$700-a-month-with-my-open-source-scheduling-tool\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/uae-researchers-say-new-ai-model-can-watch-videos-understand-audio\">UAE Researchers Say New AI Model Can Watch Videos, Understand Audio</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/2jqChkrv03exBUgkLrDzIbfM99q2-wa02s7q.jpeg\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/autoencoder\">@autoencoder</a> [ 4 Min read ] Researchers in UAE have developed an AI model that can find and focus on objects in videos and beats other models in doing so. <a href=\"https://hackernoon.com/uae-researchers-say-new-ai-model-can-watch-videos-understand-audio\">Read More.</a></p>\n        \n              \n        <br />\n        <p>🧑‍💻 What happened in your world this week?</p>\n        <p>\n          It's been said that\n          <a href=\"https://hackernoon.com/developers-the-why-and-how-to-writing-technical-articles-54e824789ef6\">writing can help consolidate technical knowledge</a>,\n          <a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\">establish credibility</a>,\n          <a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\"> and contribute to emerging community standards</a>.\n          Feeling stuck? We got you covered ⬇️⬇️⬇️\n        </p>\n        <br />\n        <p>\n          <a href=\"https://app.hackernoon.com/mobile/lZx3fmlPdlPJpVBIdble\">ANSWER THESE GREATEST INTERVIEW QUESTIONS OF ALL TIME</a>\n        </p>\n        <br />\n        <p>We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, \n The HackerNoon Team ✌️</p>\n        <br />\n        <p><img src=\"https://cdn.hackernoon.com/images/the-hackernoon-newsletter-footer.png\" alt /></p>\n      \n            ","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"AI Is Learning How to Perfect Lighting in Photos","url":"https://hackernoon.com/ai-is-learning-how-to-perfect-lighting-in-photos?source=rss","date":1734796857,"author":"Auto Encoder: How to Ignore the Signal Noise","unread":true,"desc":"","content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Hoon Kim, Beeble AI, and contributed equally to this work;</p>\n<p>(2) Minje Jang, Beeble AI, and contributed equally to this work;</p>\n<p>(3) Wonjun Yoon, Beeble AI, and contributed equally to this work;</p>\n<p>(4) Jisoo Lee, Beeble AI, and contributed equally to this work;</p>\n<p>(5) Donghyun Na, Beeble AI, and contributed equally to this work;</p>\n<p>(6) Sanghyun Woo, New York University, and contributed equally to this work.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's Note: This is Part 5 of 14 of a study introducing a method for improving how light and shadows can be applied to human portraits in digital images. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<ul>\n<li><a href=\"https://hackernoon.com/preview/Hc7lpYDzMy2xThjUoa2k\">Abstract and 1. Introduction</a></li>\n<li><a href=\"http://hackernoon.com/preview/BKBBCSTLqFavQm5Eoyh8\">2. Related Work</a></li>\n<li><a href=\"http://hackernoon.com/preview/XXnT1h5ctddmY9nrTnBm\">3. SwitchLight and 3.1. Preliminaries</a></li>\n<li><a href=\"http://hackernoon.com/preview/aVXXfGWipMDIduPr3etO\">3.2. Problem Formulation</a></li>\n<li><a href=\"http://hackernoon.com/preview/rRt5z9pIkPLhKrnq1MbF\">3.3. Architecture</a></li>\n<li><a href=\"http://hackernoon.com/preview/i6DMgA4RQkBbi0Id5AZP\">3.4. Objectives</a></li>\n<li><a href=\"http://hackernoon.com/preview/X8vk9VjowrvJSYUzEJE5\">4. Multi-Masked Autoencoder Pre-training</a></li>\n<li><a href=\"http://hackernoon.com/preview/fKxaVAijUM9spIBcvVIN\">5. Data</a></li>\n<li><a href=\"http://hackernoon.com/preview/cUeHVmjeXxzOYcxhw4sU\">6. Experiments</a></li>\n<li><a href=\"http://hackernoon.com/preview/1oJNS5kpGcewcRdvmW3K\">7. Conclusion</a></li>\n</ul>\n<p>\\\nAppendix</p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/R0qEhdQamd6t6A0pi9mj\">A. Implementation Details</a></li>\n<li><a href=\"http://hackernoon.com/preview/UiI85lHciL9Y30TZ2Tjz\">B. User Study Interface</a></li>\n<li><a href=\"http://hackernoon.com/preview/ePN0zoobP6j7wBjjnoQo\">C. Video Demonstration</a></li>\n<li><a href=\"https://hackernoon.com/preview/HBKRTpuRwXxGKUw1oP3l\">D. Additional Qualitative Results & References</a></li>\n</ul>\n<h2 id=\"33architecture\">3.3. Architecture</h2>\n<p><img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-gz830vj.png\" alt=\"\" /></p>\n<p>\\\n<em>Illum Net.</em> The network infers the lighting conditions in the given image captured in an HDRI format. Specifically, it computes the convolved <strong>HDRIs</strong>:</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-dr930zw.png\" alt=\"\" /></p>\n<p>\\\nThe network employs a cross-attention mechanism at its core, where predefined Phong reflectance lobes serve as queries, and the original image acts as both keys and values. Within this setup, the convolved HDRI maps are synthesized by integrating image information into the Phong reflectance lobe representation. Specifically, our model utilizes bottleneck features from the Normal Net as a compact image representation. Our approach simplifies the complex task of HDRI reconstruction by instead focusing on estimating interactions with known surface reflective properties.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-n2a304h.png\" alt=\"\" /></p>\n<p>\\\nWe have empirically validated that it significantly enhances albedo prediction across a range of real-world scenarios.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-6fb30la.png\" alt=\"Figure 3. Render Net Overview. Utilizing extracted image intrinsics, it employs the Cook-Torrance model for initial relighting and a neural network for enhanced refinement, producing high-fidelity relit images through a synergistic computational approach.\" /></p>\n<p>\\\n<strong>Specular Net.</strong> The network infers surface attributes associated with the Cook-Torrance specular elements, specifically, the roughness α and Fresnel reflectivity f0. It uses a source image, predicted normal, and albedo maps as inputs.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-etc305w.png\" alt=\"\" /></p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2402.18848\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Beeble Researchers Develop AI That Can Make Any Photo Look Perfectly Lit—Even in the Darkest Room","url":"https://hackernoon.com/beeble-researchers-develop-ai-that-can-make-any-photo-look-perfectly-liteven-in-the-darkest-room?source=rss","date":1734796854,"author":"Auto Encoder: How to Ignore the Signal Noise","unread":true,"desc":"","content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Hoon Kim, Beeble AI, and contributed equally to this work;</p>\n<p>(2) Minje Jang, Beeble AI, and contributed equally to this work;</p>\n<p>(3) Wonjun Yoon, Beeble AI, and contributed equally to this work;</p>\n<p>(4) Jisoo Lee, Beeble AI, and contributed equally to this work;</p>\n<p>(5) Donghyun Na, Beeble AI, and contributed equally to this work;</p>\n<p>(6) Sanghyun Woo, New York University, and contributed equally to this work.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's Note: This is Part 6 of 14 of a study introducing a method for improving how light and shadows can be applied to human portraits in digital images. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<ul>\n<li><a href=\"https://hackernoon.com/preview/Hc7lpYDzMy2xThjUoa2k\">Abstract and 1. Introduction</a></li>\n<li><a href=\"http://hackernoon.com/preview/BKBBCSTLqFavQm5Eoyh8\">2. Related Work</a></li>\n<li><a href=\"http://hackernoon.com/preview/XXnT1h5ctddmY9nrTnBm\">3. SwitchLight and 3.1. Preliminaries</a></li>\n<li><a href=\"http://hackernoon.com/preview/aVXXfGWipMDIduPr3etO\">3.2. Problem Formulation</a></li>\n<li><a href=\"http://hackernoon.com/preview/rRt5z9pIkPLhKrnq1MbF\">3.3. Architecture</a></li>\n<li><a href=\"http://hackernoon.com/preview/i6DMgA4RQkBbi0Id5AZP\">3.4. Objectives</a></li>\n<li><a href=\"http://hackernoon.com/preview/X8vk9VjowrvJSYUzEJE5\">4. Multi-Masked Autoencoder Pre-training</a></li>\n<li><a href=\"http://hackernoon.com/preview/fKxaVAijUM9spIBcvVIN\">5. Data</a></li>\n<li><a href=\"http://hackernoon.com/preview/cUeHVmjeXxzOYcxhw4sU\">6. Experiments</a></li>\n<li><a href=\"http://hackernoon.com/preview/1oJNS5kpGcewcRdvmW3K\">7. Conclusion</a></li>\n</ul>\n<p>\\\nAppendix</p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/R0qEhdQamd6t6A0pi9mj\">A. Implementation Details</a></li>\n<li><a href=\"http://hackernoon.com/preview/UiI85lHciL9Y30TZ2Tjz\">B. User Study Interface</a></li>\n<li><a href=\"http://hackernoon.com/preview/ePN0zoobP6j7wBjjnoQo\">C. Video Demonstration</a></li>\n<li><a href=\"https://hackernoon.com/preview/HBKRTpuRwXxGKUw1oP3l\">D. Additional Qualitative Results & References</a></li>\n</ul>\n<h2 id=\"34objectives\">3.4. Objectives</h2>\n<p>We supervise both intrinsic image attributes and relit images using their corresponding ground truths, obtained from the lightstage. We employ a combination of reconstruction, perceptual [24], adversarial [22], and specular [34] losses.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-dw830fn.png\" alt=\"\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-ma930ba.png\" alt=\"Figure 4. Neural Render Enhancement. Using the CookTorrance model, diffuse and specular renders are computed, which are then composited into a physically-based rendering. Subsequently, a neural network enhances this PBR render, improving aspects such as brightness and specular details.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-wna30a5.png\" alt=\"\" /></p>\n<p>\\\n<strong>Final Loss.</strong> The SwitchLight is trained in an end-to-end manner using the weighted sum of the above losses:</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-3jb30if.png\" alt=\"\" /></p>\n<p>\\\nWe empirically determined the weighting coefficients.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-aqc30un.png\" alt=\"Figure 5. Dynamic Masking Strategies. We have generalized the MAE masks to include overlapping patches of varying sizes, as well as outpainting and free-form masks.\" /></p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2402.18848\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Researchers Build Massive AI Training Dataset to Perfect Lighting on Faces","url":"https://hackernoon.com/researchers-build-massive-ai-training-dataset-to-perfect-lighting-on-faces?source=rss","date":1734796851,"author":"Auto Encoder: How to Ignore the Signal Noise","unread":true,"desc":"","content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Hoon Kim, Beeble AI, and contributed equally to this work;</p>\n<p>(2) Minje Jang, Beeble AI, and contributed equally to this work;</p>\n<p>(3) Wonjun Yoon, Beeble AI, and contributed equally to this work;</p>\n<p>(4) Jisoo Lee, Beeble AI, and contributed equally to this work;</p>\n<p>(5) Donghyun Na, Beeble AI, and contributed equally to this work;</p>\n<p>(6) Sanghyun Woo, New York University, and contributed equally to this work.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's Note: This is Part 8 of 14 of a study introducing a method for improving how light and shadows can be applied to human portraits in digital images. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<ul>\n<li><a href=\"https://hackernoon.com/preview/Hc7lpYDzMy2xThjUoa2k\">Abstract and 1. Introduction</a></li>\n<li><a href=\"http://hackernoon.com/preview/BKBBCSTLqFavQm5Eoyh8\">2. Related Work</a></li>\n<li><a href=\"http://hackernoon.com/preview/XXnT1h5ctddmY9nrTnBm\">3. SwitchLight and 3.1. Preliminaries</a></li>\n<li><a href=\"http://hackernoon.com/preview/aVXXfGWipMDIduPr3etO\">3.2. Problem Formulation</a></li>\n<li><a href=\"http://hackernoon.com/preview/rRt5z9pIkPLhKrnq1MbF\">3.3. Architecture</a></li>\n<li><a href=\"http://hackernoon.com/preview/i6DMgA4RQkBbi0Id5AZP\">3.4. Objectives</a></li>\n<li><a href=\"http://hackernoon.com/preview/X8vk9VjowrvJSYUzEJE5\">4. Multi-Masked Autoencoder Pre-training</a></li>\n<li><a href=\"http://hackernoon.com/preview/fKxaVAijUM9spIBcvVIN\">5. Data</a></li>\n<li><a href=\"http://hackernoon.com/preview/cUeHVmjeXxzOYcxhw4sU\">6. Experiments</a></li>\n<li><a href=\"http://hackernoon.com/preview/1oJNS5kpGcewcRdvmW3K\">7. Conclusion</a></li>\n</ul>\n<p>\\\nAppendix</p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/R0qEhdQamd6t6A0pi9mj\">A. Implementation Details</a></li>\n<li><a href=\"http://hackernoon.com/preview/UiI85lHciL9Y30TZ2Tjz\">B. User Study Interface</a></li>\n<li><a href=\"http://hackernoon.com/preview/ePN0zoobP6j7wBjjnoQo\">C. Video Demonstration</a></li>\n<li><a href=\"https://hackernoon.com/preview/HBKRTpuRwXxGKUw1oP3l\">D. Additional Qualitative Results & References</a></li>\n</ul>\n<h2 id=\"5data\">5. Data</h2>\n<p>We constructed the OLAT (One Light at a Time) dataset using a light stage [10, 49] equipped with 137 programmable LED lights and 7 frontal-viewing cameras. Our dataset comprises images of 287 subjects, with each subject being captured in up to 15 different poses, resulting in a total of 29,705 OLAT sequences. We sourced HDRI dataset from several publicly available archives. Specifically, we acquired 559 HDRI maps from Polyhaven, 76 from Noah Witchell, 364 from HDRMAPS, 129 from iHDRI, and 34 from eisklotz. In addition, we incorporated synthetic HDRIs created using the method proposed in [31]. During training, HDRIs are randomly selected with equal probability from either real-world or synthetic collections.</p>\n<p>\\\nWe produced training pairs by projecting the sampled source and target lighting maps onto the reflectance fields of the OLAT images [10]. To derive the ground truth intrinsics, we applied the photometric stereo method [51] and obtained normal and albedo maps.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2402.18848\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Researchers Develop AI That Can Copy, Adjust, and Perfect Lighting in Any Image","url":"https://hackernoon.com/researchers-develop-ai-that-can-copy-adjust-and-perfect-lighting-in-any-image?source=rss","date":1734796846,"author":"Auto Encoder: How to Ignore the Signal Noise","unread":true,"desc":"","content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Hoon Kim, Beeble AI, and contributed equally to this work;</p>\n<p>(2) Minje Jang, Beeble AI, and contributed equally to this work;</p>\n<p>(3) Wonjun Yoon, Beeble AI, and contributed equally to this work;</p>\n<p>(4) Jisoo Lee, Beeble AI, and contributed equally to this work;</p>\n<p>(5) Donghyun Na, Beeble AI, and contributed equally to this work;</p>\n<p>(6) Sanghyun Woo, New York University, and contributed equally to this work.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's Note: This is Part 13 of 14 of a study introducing a method for improving how light and shadows can be applied to human portraits in digital images. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<ul>\n<li><a href=\"https://hackernoon.com/preview/Hc7lpYDzMy2xThjUoa2k\">Abstract and 1. Introduction</a></li>\n<li><a href=\"http://hackernoon.com/preview/BKBBCSTLqFavQm5Eoyh8\">2. Related Work</a></li>\n<li><a href=\"http://hackernoon.com/preview/XXnT1h5ctddmY9nrTnBm\">3. SwitchLight and 3.1. Preliminaries</a></li>\n<li><a href=\"http://hackernoon.com/preview/aVXXfGWipMDIduPr3etO\">3.2. Problem Formulation</a></li>\n<li><a href=\"http://hackernoon.com/preview/rRt5z9pIkPLhKrnq1MbF\">3.3. Architecture</a></li>\n<li><a href=\"http://hackernoon.com/preview/i6DMgA4RQkBbi0Id5AZP\">3.4. Objectives</a></li>\n<li><a href=\"http://hackernoon.com/preview/X8vk9VjowrvJSYUzEJE5\">4. Multi-Masked Autoencoder Pre-training</a></li>\n<li><a href=\"http://hackernoon.com/preview/fKxaVAijUM9spIBcvVIN\">5. Data</a></li>\n<li><a href=\"http://hackernoon.com/preview/cUeHVmjeXxzOYcxhw4sU\">6. Experiments</a></li>\n<li><a href=\"http://hackernoon.com/preview/1oJNS5kpGcewcRdvmW3K\">7. Conclusion</a></li>\n</ul>\n<p>\\\nAppendix</p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/R0qEhdQamd6t6A0pi9mj\">A. Implementation Details</a></li>\n<li><a href=\"http://hackernoon.com/preview/UiI85lHciL9Y30TZ2Tjz\">B. User Study Interface</a></li>\n<li><a href=\"http://hackernoon.com/preview/ePN0zoobP6j7wBjjnoQo\">C. Video Demonstration</a></li>\n<li><a href=\"https://hackernoon.com/preview/HBKRTpuRwXxGKUw1oP3l\">D. Additional Qualitative Results & References</a></li>\n</ul>\n<h2 id=\"cvideodemonstration\">C. Video Demonstration</h2>\n<p>We present a detailed video demonstration of our SwitchLight framework. Initially, we use real-world videos from Pexels [1] to showcase its robust generalizability and practicality. Then, for state-of-the-art comparisons, we utilize the FFHQ dataset [25] to demonstrate its advanced relighting capabilities over previous methods. The presentation includes several key components:</p>\n<p>\\</p>\n<ol>\n<li><p><strong>De-rendering:</strong> This stage demonstrates the extraction of normal, albedo, roughness, and reflectivity attributes from any given image, a process known as inverse rendering.</p>\n<p>\\</p></li>\n<li><p><strong>Neural Relighting:</strong> Leveraging these intrinsic properties, our system adeptly relights images to align with a new, specified target lighting environment.</p>\n<p>\\</p></li>\n<li><p><strong>Real-time Physically Based Rendering (PBR):</strong> Utilizing the Three.js framework and integrating derived intrinsic properties with the Cook-Torrance reflectance model, we facilitate real-time rendering. This enables achieving 30 fps on a MacBook Pro with an Apple M1 chip (8-core CPU and 8-core GPU) and 16 GB of RAM.</p>\n<p>\\</p></li>\n<li><p><strong>Copy Light:</strong> Leveraging SwitchLight’s ability to predict lighting conditions of a given input image, we explore an intriguing application. This process involves two images, a source and a reference. We first extract their intrinsic surface attributes and lighting conditions. Then, by combining the source intrinsic attributes with the reference lighting condition, we generate a new, relit image. In this image, the source foreground remains unchanged, but its lighting is altered to match that of the reference image.</p>\n<p>\\</p></li>\n<li><p><strong>State-of-the-Art Comparisons:</strong> We benchmark our framework against leading methods, specifically Total Relight [34] and Lumos [52], to highlight substantial performance improvements over these approaches.</p></li>\n</ol>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2402.18848\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Researchers Asked 47 People to Judge AI-Enhanced Portraits—Here’s What They Chose","url":"https://hackernoon.com/researchers-asked-47-people-to-judge-ai-enhanced-portraitsheres-what-they-chose?source=rss","date":1734796844,"author":"Auto Encoder: How to Ignore the Signal Noise","unread":true,"desc":"","content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Hoon Kim, Beeble AI, and contributed equally to this work;</p>\n<p>(2) Minje Jang, Beeble AI, and contributed equally to this work;</p>\n<p>(3) Wonjun Yoon, Beeble AI, and contributed equally to this work;</p>\n<p>(4) Jisoo Lee, Beeble AI, and contributed equally to this work;</p>\n<p>(5) Donghyun Na, Beeble AI, and contributed equally to this work;</p>\n<p>(6) Sanghyun Woo, New York University, and contributed equally to this work.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's Note: This is Part 9 of 14 of a study introducing a method for improving how light and shadows can be applied to human portraits in digital images. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<ul>\n<li><a href=\"https://hackernoon.com/preview/Hc7lpYDzMy2xThjUoa2k\">Abstract and 1. Introduction</a></li>\n<li><a href=\"http://hackernoon.com/preview/BKBBCSTLqFavQm5Eoyh8\">2. Related Work</a></li>\n<li><a href=\"http://hackernoon.com/preview/XXnT1h5ctddmY9nrTnBm\">3. SwitchLight and 3.1. Preliminaries</a></li>\n<li><a href=\"http://hackernoon.com/preview/aVXXfGWipMDIduPr3etO\">3.2. Problem Formulation</a></li>\n<li><a href=\"http://hackernoon.com/preview/rRt5z9pIkPLhKrnq1MbF\">3.3. Architecture</a></li>\n<li><a href=\"http://hackernoon.com/preview/i6DMgA4RQkBbi0Id5AZP\">3.4. Objectives</a></li>\n<li><a href=\"http://hackernoon.com/preview/X8vk9VjowrvJSYUzEJE5\">4. Multi-Masked Autoencoder Pre-training</a></li>\n<li><a href=\"http://hackernoon.com/preview/fKxaVAijUM9spIBcvVIN\">5. Data</a></li>\n<li><a href=\"http://hackernoon.com/preview/cUeHVmjeXxzOYcxhw4sU\">6. Experiments</a></li>\n<li><a href=\"http://hackernoon.com/preview/1oJNS5kpGcewcRdvmW3K\">7. Conclusion</a></li>\n</ul>\n<p>\\\nAppendix</p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/R0qEhdQamd6t6A0pi9mj\">A. Implementation Details</a></li>\n<li><a href=\"http://hackernoon.com/preview/UiI85lHciL9Y30TZ2Tjz\">B. User Study Interface</a></li>\n<li><a href=\"http://hackernoon.com/preview/ePN0zoobP6j7wBjjnoQo\">C. Video Demonstration</a></li>\n<li><a href=\"https://hackernoon.com/preview/HBKRTpuRwXxGKUw1oP3l\">D. Additional Qualitative Results & References</a></li>\n</ul>\n<h2 id=\"6experiments\">6. Experiments</h2>\n<p>This section details our experimental results. We begin with a comparative evaluation of our method against state-ofthe-art approaches using the OLAT dataset. We also employ images from the FFHQ–test [25] for user studies. For qualitative analysis, we utilize copyright-free portrait images from Pexels [1]. Additionally, we conduct ablation studies to validate the efficacy of our pre-training framework and architectural design choice. Subsequently, we detail the additional features and conclude by discussing its limitations. Our evaluation uses the OLAT test set, comprising 35 subjects and 11 lighting environments, ensuring no overlap with the train set.</p>\n<p>\\\n<strong>Evaluation metrics.</strong> We employ several key metrics for evaluating the prediction accuracy; Mean Absolute Error (<strong>MAE</strong>), Mean Squared Error (<strong>MSE</strong>), Structural Similarity Index Measure (<strong>SSIM</strong>) and Learned Perceptual Image Patch Similarity (<strong>LPIPS</strong>). While these metrics offer valuable quantitative insights, they do not fully capture the subtleties of visual quality enhancement. Therefore, we emphasize the importance of qualitative evaluations to gain a comprehensive understanding of model performance.</p>\n<p>\\\n<strong>Baselines</strong>. We compared our approach with three state-of-the-art baselines: Single Image Portrait Relighting (<strong>SIPR</strong>) [45], which uses a single neural network for relighting; Total Relight (<strong>TR</strong>) [34], employing multiple neural networks that incorporate the Phong reflectance model; and <strong>Lumos</strong> [52], a TR adaptation for synthetic datasets. Due to the lack of publicly available code or model from these methods, we either integrated their techniques into our framework or requested the original authors to process our inputs with their models and share the results.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-iw830gs.png\" alt=\"Table 1. Quantitative Evaluation on the OLAT test set.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-p7930hi.png\" alt=\"Table 2. User Study on the FFHQ test set.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-wra30wk.png\" alt=\"Figure 6. Impact of Pre-training. The fine details such as specular highlights, skin tones, and shadows are notably improved.\" /></p>\n<p>\\\n<strong>Quantitative Comparisons.</strong> The results in Table. 1 shows our method outperforming SIPR and TR baselines, demonstrating the significance of incorporating advanced rendering physics and reflectance models. The transition from SIPR to TR emphasizes the value of physics-based design, while the shift from TR to our approach underscores the importance of transitioning from the empirical Phong model to the more accurate Cook-Torrance model. Additionally, pretraining contributes to further enhancements, as evidenced by the improved image details, depicted in Fig 6.</p>\n<p>\\\n<strong>Qualitative Comparisons.</strong> Our relighting method exhibits several key advantages over previous approaches, as showcased in Fig. 7. It effectively harmonizes light direction and softness, avoiding harsh highlights and inaccurate lighting that are commonly observed in other methods. A notable strength of our approach lies in its ability to capture highfrequency details like specular highlights and hard shadows. Additionally, as shown in the second row, it preserves facial details and identity, ensuring high fidelity to the subject’s original features and mitigating common distortions seen in previous approaches. Moreover, our approach excels in handling skin tones, producing natural and accurate results under various lighting conditions. This is clearly demonstrated in the fourth row, where our method contrasts sharply with the over-saturated or pale tones from previous methods. Finally, the nuanced treatment of hair is highlighted in the sixth row, where our approach maintains luster and detail, unlike the flattened effect typical in other methods. More qualitative results are available in our supplementary video demonstration.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-8ib305j.png\" alt=\"Additionally, as shown in the second row, it preserves facial details and identity, ensuring high fidelity to the subFigure 7. Qualitative Comparison on the Pexels images [1].\" /></p>\n<p>\\\n<strong>User Study.</strong> We conducted a human subjective test to evaluate the visual quality of relighitng results, summarized in Table. 2. In each test case, workers were presented with an input image and an environment map. They were asked to compare the relighting results from three methods–Ours, Lumos, and TR–based on three criteria: 1) consistency of lighting with the environment map, 2) preservation of facial details, and 3) maintenance of the original identity. To ensure unbiased evaluations, the order of the methods presented was randomized. To aid in understanding the concept of consistent lighting, relit balls were displayed alongside the images. The study included a total of 256 images, consisting of 32 portraits each illuminated with 8 different HDRIs. Each worker was tasked with selecting the best image for each specific criterion, randomly assessing 30 samples. A total of 47 workers participated in the study. The results indicate a strong preference for our results over the baseline methods across all evaluated metrics.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-e3c3080.png\" alt=\"Table 3. Ablation Studies on the OLAT test set.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-dqd30lh.png\" alt=\"Figure 8. Ablation on DiffuseNet. Our approach successfully infers the albedo on various surfaces (skin, teeth, and accessories).\" /></p>\n<p>\\\n<strong>Ablation Studies.</strong> We analyze our two major design choices in Table. 3: the MMAE pre-training framework and DiffuseNet. The MMAE, which integrates dynamic masking with generative objectives, outperforms MAE. This superiority is mainly due to the incorporation of challenging masks and global coherence objectives, enabling the model to learn richer features during pre-training. Furthermore, our method of predicting diffuse render demonstrates superiority over direct albedo prediction. Firstly, we see it simplifies the learning process, as predicting diffuse render is more closely related to the original image. Secondly, our approach effectively distinguishes between the influences of illumination (diffuse shading) and surface properties (diffuse render). This distinction is crucial for accurately modeling the intrinsic color of surfaces, as it enables independent and precise evaluation of each element (see Eqn. 9). In contrast, methods that predict albedo directly often struggle to differentiate between these factors, leading to significant inaccuracies in color constancy, as shown in Fig. 8.</p>\n<p>\\\n<strong>Applications</strong>. We present two applications using predicted intrinsics in Fig. 9. First, real-time PBR via Cook-Torrance</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-rle306r.png\" alt=\"Figure 9. Applications. We showcase additional features of SwitchLight, powered by the diverse intrinsics features.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-cmf305r.png\" alt=\"Figure 10. Limitations. The model faces challenges in removing strong shadows, misinterpreting reflective surfaces like sunglasses, and inaccurately predicting albedo for face paint.\" /></p>\n<p>\\\ncomponents in Three.js graphics library. Second, switching the lighting environment between the source and reference images. Further details are in the supplementary video.</p>\n<p>\\\n<strong>Limitations</strong>. We identified a few failure cases in Fig. 10. First, our model struggles with neutralizing strong shadows, which leads to inaccurate facial geometry and residual shadow artifacts in both albedo and relit images. Incorporating shadow augmentation techniques [16, 54] during training could mitigate this issue. Second, the model incorrectly interprets reflective surfaces, such as sunglasses, as opaque objects in the normal image. This error prevents the model from properly removing reflective highlights in the albedo and relit images. Lastly, the model inaccurately predicts the albedo for face paint. Implementing a semantic mask [52] to distinguish different semantic regions separately from the skin could help resolving these issues.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2402.18848\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Beeble Researchers Attempt to Manipulate Lighting on Digital Image Using AI","url":"https://hackernoon.com/beeble-researchers-attempt-to-manipulate-lighting-on-digital-image-using-ai?source=rss","date":1734796841,"author":"Auto Encoder: How to Ignore the Signal Noise","unread":true,"desc":"","content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Hoon Kim, Beeble AI, and contributed equally to this work;</p>\n<p>(2) Minje Jang, Beeble AI, and contributed equally to this work;</p>\n<p>(3) Wonjun Yoon, Beeble AI, and contributed equally to this work;</p>\n<p>(4) Jisoo Lee, Beeble AI, and contributed equally to this work;</p>\n<p>(5) Donghyun Na, Beeble AI, and contributed equally to this work;</p>\n<p>(6) Sanghyun Woo, New York University, and contributed equally to this work.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's Note: This is Part 4 of 14 of a study introducing a method for improving how light and shadows can be applied to human portraits in digital images. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<ul>\n<li><a href=\"https://hackernoon.com/preview/Hc7lpYDzMy2xThjUoa2k\">Abstract and 1. Introduction</a></li>\n<li><a href=\"http://hackernoon.com/preview/BKBBCSTLqFavQm5Eoyh8\">2. Related Work</a></li>\n<li><a href=\"http://hackernoon.com/preview/XXnT1h5ctddmY9nrTnBm\">3. SwitchLight and 3.1. Preliminaries</a></li>\n<li><a href=\"http://hackernoon.com/preview/aVXXfGWipMDIduPr3etO\">3.2. Problem Formulation</a></li>\n<li><a href=\"http://hackernoon.com/preview/rRt5z9pIkPLhKrnq1MbF\">3.3. Architecture</a></li>\n<li><a href=\"http://hackernoon.com/preview/i6DMgA4RQkBbi0Id5AZP\">3.4. Objectives</a></li>\n<li><a href=\"http://hackernoon.com/preview/X8vk9VjowrvJSYUzEJE5\">4. Multi-Masked Autoencoder Pre-training</a></li>\n<li><a href=\"http://hackernoon.com/preview/fKxaVAijUM9spIBcvVIN\">5. Data</a></li>\n<li><a href=\"http://hackernoon.com/preview/cUeHVmjeXxzOYcxhw4sU\">6. Experiments</a></li>\n<li><a href=\"http://hackernoon.com/preview/1oJNS5kpGcewcRdvmW3K\">7. Conclusion</a></li>\n</ul>\n<p>\\\nAppendix</p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/R0qEhdQamd6t6A0pi9mj\">A. Implementation Details</a></li>\n<li><a href=\"http://hackernoon.com/preview/UiI85lHciL9Y30TZ2Tjz\">B. User Study Interface</a></li>\n<li><a href=\"http://hackernoon.com/preview/ePN0zoobP6j7wBjjnoQo\">C. Video Demonstration</a></li>\n<li><a href=\"https://hackernoon.com/preview/HBKRTpuRwXxGKUw1oP3l\">D. Additional Qualitative Results & References</a></li>\n</ul>\n<h2 id=\"32problemformulation\">3.2. Problem Formulation</h2>\n<p><img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-z8830fd.png\" alt=\"\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-ap930tv.png\" alt=\"Figure 2. SwitchLight Architecture. The input source image is decomposed into normal map, lighting, diffuse and specular components. Given these intrinsics, images are re-rendered under target lighting. The architecture integrates the Cook-Torrance reflection model; the final output combines physically-based predictions with neural network enhancements for realistic portrait relighting.\" /></p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2402.18848\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"This New Tech Makes Faces in Photos Look Incredibly Realistic","url":"https://hackernoon.com/this-new-tech-makes-faces-in-photos-look-incredibly-realistic?source=rss","date":1734796839,"author":"Auto Encoder: How to Ignore the Signal Noise","unread":true,"desc":"","content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Hoon Kim, Beeble AI, and contributed equally to this work;</p>\n<p>(2) Minje Jang, Beeble AI, and contributed equally to this work;</p>\n<p>(3) Wonjun Yoon, Beeble AI, and contributed equally to this work;</p>\n<p>(4) Jisoo Lee, Beeble AI, and contributed equally to this work;</p>\n<p>(5) Donghyun Na, Beeble AI, and contributed equally to this work;</p>\n<p>(6) Sanghyun Woo, New York University, and contributed equally to this work.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's Note: This is Part 3 of 14 of a study introducing a method for improving how light and shadows can be applied to human portraits in digital images. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<ul>\n<li><a href=\"https://hackernoon.com/preview/Hc7lpYDzMy2xThjUoa2k\">Abstract and 1. Introduction</a></li>\n<li><a href=\"http://hackernoon.com/preview/BKBBCSTLqFavQm5Eoyh8\">2. Related Work</a></li>\n<li><a href=\"http://hackernoon.com/preview/XXnT1h5ctddmY9nrTnBm\">3. SwitchLight and 3.1. Preliminaries</a></li>\n<li><a href=\"http://hackernoon.com/preview/aVXXfGWipMDIduPr3etO\">3.2. Problem Formulation</a></li>\n<li><a href=\"http://hackernoon.com/preview/rRt5z9pIkPLhKrnq1MbF\">3.3. Architecture</a></li>\n<li><a href=\"http://hackernoon.com/preview/i6DMgA4RQkBbi0Id5AZP\">3.4. Objectives</a></li>\n<li><a href=\"http://hackernoon.com/preview/X8vk9VjowrvJSYUzEJE5\">4. Multi-Masked Autoencoder Pre-training</a></li>\n<li><a href=\"http://hackernoon.com/preview/fKxaVAijUM9spIBcvVIN\">5. Data</a></li>\n<li><a href=\"http://hackernoon.com/preview/cUeHVmjeXxzOYcxhw4sU\">6. Experiments</a></li>\n<li><a href=\"http://hackernoon.com/preview/1oJNS5kpGcewcRdvmW3K\">7. Conclusion</a></li>\n</ul>\n<p>\\\nAppendix</p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/R0qEhdQamd6t6A0pi9mj\">A. Implementation Details</a></li>\n<li><a href=\"http://hackernoon.com/preview/UiI85lHciL9Y30TZ2Tjz\">B. User Study Interface</a></li>\n<li><a href=\"http://hackernoon.com/preview/ePN0zoobP6j7wBjjnoQo\">C. Video Demonstration</a></li>\n<li><a href=\"https://hackernoon.com/preview/HBKRTpuRwXxGKUw1oP3l\">D. Additional Qualitative Results & References</a></li>\n</ul>\n<h2 id=\"3switchlight\">3. SwitchLight</h2>\n<p>We introduce SwitchLight, a state-of-the-art framework for human portrait relighting, with its architectural overview presented in Fig. 2. We first provide foundational concepts in Sec. 3.1, and define the problem in Sec. 3.2. This is followed by detailing the architecture in Sec. 3.3, and lastly, we describe the loss functions used in Sec. 3.4.</p>\n<h3 id=\"31preliminaries\">3.1. Preliminaries</h3>\n<p>In this section, vectors n, v, l, and h are denoted as unit vectors. Specifically, n represents the surface normal, v is the view direction, l is the incident light direction, and h is the half-vector computed from l and v. The dot product is clamped between [0..1], indicated by ⟨·⟩.</p>\n<p>\\\n<strong>Image Rendering.</strong> The primary goal of image rendering is to create a visual representation that accurately simulates the interactions between light and surfaces. These complex interactions are encapsulated by the rendering equation:</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-w1830xj.png\" alt=\"\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-4z930ey.png\" alt=\"\" /></p>\n<p>\\\nA surface intrinsically exhibits both diffuse and specular reflections. The diffuse component uniformly scatters light, ensuring consistent illumination regardless of the viewing angle. In contrast, the specular component is viewing angle-dependent, producing shiny highlights that are crucial for achieving a photorealistic effect.</p>\n<p>\\\n<strong>Lambertian Diffuse Reflectance.</strong> Lambertian reflectance, a standard model for diffuse reflection, describes a uniform light scatter irrespective of the viewing angle. This ensures a consistent appearance in brightness:</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-3oa30m5.png\" alt=\"\" /></p>\n<p>\\\nHere, σ is the <em>albedo</em>, indicating the intrinsic color and brightness of the surface.</p>\n<p>\\\n<strong>Cook-Torrance Specular Reflectance.</strong> The Cook-Torrance model, based on microfacet theory, represents surfaces as a myriad of tiny, mirror-like facets. It incorporates a roughness parameter α, which allows precise rendering of surface specular reflectance:</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-kdb30dz.png\" alt=\"\" /></p>\n<p>\\\nIn this model, D is the microfacet distribution function, describing the orientation of the microfacets relative to the half-vector h, G is the geometric attenuation factor, accounting for the shadowing and masking of microfacets, and F is the Fresnel term, calculating the reflectance variation depending on the viewing angle, where f0 is the surface Fresnel reflectivity at normal incidence. A lower α value implies a smoother surface with sharper specular highlights, whereas a higher α value indicates a rougher surface, resulting in more diffused reflections. By adjusting α, the Cook-Torrance model can depict a range of specular reflections.</p>\n<p>\\\n<strong>Image Formation.</strong> Upon the base rendering equation, we include the diffuse and specular components of the BRDF and derive a unified formula:</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-n6c30nd.png\" alt=\"\" /></p>\n<p>\\\nwhere E(l) denotes the incident environmental lighting. This formula represents the core principle that an image is a product of interplay between the BRDF and lighting. To further clarify this concept, we introduce a rendering function R, succinctly modeling the process of image formation:</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-g2d300f.png\" alt=\"\" /></p>\n<p>\\\nIt is important to note that since the BRDF is a function of surface properties, as detailed in Eqn. 3 and 4, we can now clearly understand that image formation is essentially governed by the interaction of surface attributes and lighting.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2402.18848\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"AI Learns to Perfect Lighting in Photos Using Smart Masks and Creative Training","url":"https://hackernoon.com/ai-learns-to-perfect-lighting-in-photos-using-smart-masks-and-creative-training?source=rss","date":1734796836,"author":"Auto Encoder: How to Ignore the Signal Noise","unread":true,"desc":"","content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Hoon Kim, Beeble AI, and contributed equally to this work;</p>\n<p>(2) Minje Jang, Beeble AI, and contributed equally to this work;</p>\n<p>(3) Wonjun Yoon, Beeble AI, and contributed equally to this work;</p>\n<p>(4) Jisoo Lee, Beeble AI, and contributed equally to this work;</p>\n<p>(5) Donghyun Na, Beeble AI, and contributed equally to this work;</p>\n<p>(6) Sanghyun Woo, New York University, and contributed equally to this work.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's Note: This is Part 7 of 14 of a study introducing a method for improving how light and shadows can be applied to human portraits in digital images. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<ul>\n<li><a href=\"https://hackernoon.com/preview/Hc7lpYDzMy2xThjUoa2k\">Abstract and 1. Introduction</a></li>\n<li><a href=\"http://hackernoon.com/preview/BKBBCSTLqFavQm5Eoyh8\">2. Related Work</a></li>\n<li><a href=\"http://hackernoon.com/preview/XXnT1h5ctddmY9nrTnBm\">3. SwitchLight and 3.1. Preliminaries</a></li>\n<li><a href=\"http://hackernoon.com/preview/aVXXfGWipMDIduPr3etO\">3.2. Problem Formulation</a></li>\n<li><a href=\"http://hackernoon.com/preview/rRt5z9pIkPLhKrnq1MbF\">3.3. Architecture</a></li>\n<li><a href=\"http://hackernoon.com/preview/i6DMgA4RQkBbi0Id5AZP\">3.4. Objectives</a></li>\n<li><a href=\"http://hackernoon.com/preview/X8vk9VjowrvJSYUzEJE5\">4. Multi-Masked Autoencoder Pre-training</a></li>\n<li><a href=\"http://hackernoon.com/preview/fKxaVAijUM9spIBcvVIN\">5. Data</a></li>\n<li><a href=\"http://hackernoon.com/preview/cUeHVmjeXxzOYcxhw4sU\">6. Experiments</a></li>\n<li><a href=\"http://hackernoon.com/preview/1oJNS5kpGcewcRdvmW3K\">7. Conclusion</a></li>\n</ul>\n<p>\\\nAppendix</p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/R0qEhdQamd6t6A0pi9mj\">A. Implementation Details</a></li>\n<li><a href=\"http://hackernoon.com/preview/UiI85lHciL9Y30TZ2Tjz\">B. User Study Interface</a></li>\n<li><a href=\"http://hackernoon.com/preview/ePN0zoobP6j7wBjjnoQo\">C. Video Demonstration</a></li>\n<li><a href=\"https://hackernoon.com/preview/HBKRTpuRwXxGKUw1oP3l\">D. Additional Qualitative Results & References</a></li>\n</ul>\n<h2 id=\"4multimaskedautoencoderpretraining\">4. Multi-Masked Autoencoder Pre-training</h2>\n<p>We introduce the Multi-Masked Autoencoder (MMAE), a self-supervised pre-training framework designed to enhance feature representations in relighting models. It aims to improve output quality without relying on additional, costly light stage data. Building upon the MAE framework [19], MMAE capitalizes on the inherent learning of crucial image features like structure, color, and texture, which are essential for relighting tasks. However, adapting MAE to our specific needs poses several non-trivial challenges. Firstly, MAE is primarily designed for vision transformers [15], while our focus is on a UNet, a convolution-based architecture. This convolutional structure, with its hierarchical nature and aggressive pooling, is known to simplify the MAE task, necessitating careful adaptation [50]. Further, the hyperparameters of MAE, particularly the fixed mask size and ratio, are also specific to vision transformers. These factors could introduce biases during training and hinder the model to recognize image features at various scales. Moreover, MAE relies solely on masked region reconstruction loss, limiting the model to understand the global coherence of the reconstructed region in relation to its visible context.</p>\n<p>\\\nTo address these challenges effectively, we have developed two key strategies within the MMAE framework:</p>\n<p>\\\n<strong>Dynamic Masking.</strong> MMAE eliminates two key hyperparameters, mask size and ratio, by introducing a variety of mask types to generalize the MAE. These types, which include overlapping patches of various sizes, outpainting masks [46], and free-form masks [29] (see Fig.5), each contribute to the model’s versatility. With the ability to handle challenging masked regions, MMAE achieves a more comprehensive understanding of image properties.</p>\n<p>\\\n<strong>Generative Target.</strong> In addition to its structural advancements, MMAE incorporates a new loss function strategy. We have adopted perceptual [24] and adversarial losses [22], along with the original reconstruction loss. As a result, MMAE is equipped not only to reconstruct missing image parts but also to ensure synthesis capabilities and their seamless integration with the original context. In practice, the weights for the three losses are equally set.</p>\n<p>\\\nWe pre-train the entire UNet architecture using MMAE, and, unlike MAE, we retain the decoder and fine-tune the entire model on relighting ground truths.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2402.18848\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Scientists Use Portraits and Balls to Test an AI’s Relighting Abilities","url":"https://hackernoon.com/scientists-use-portraits-and-balls-to-test-an-ais-relighting-abilities?source=rss","date":1734796834,"author":"Auto Encoder: How to Ignore the Signal Noise","unread":true,"desc":"","content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Hoon Kim, Beeble AI, and contributed equally to this work;</p>\n<p>(2) Minje Jang, Beeble AI, and contributed equally to this work;</p>\n<p>(3) Wonjun Yoon, Beeble AI, and contributed equally to this work;</p>\n<p>(4) Jisoo Lee, Beeble AI, and contributed equally to this work;</p>\n<p>(5) Donghyun Na, Beeble AI, and contributed equally to this work;</p>\n<p>(6) Sanghyun Woo, New York University, and contributed equally to this work.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's Note: This is Part 12 of 14 of a study introducing a method for improving how light and shadows can be applied to human portraits in digital images. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<ul>\n<li><a href=\"https://hackernoon.com/preview/Hc7lpYDzMy2xThjUoa2k\">Abstract and 1. Introduction</a></li>\n<li><a href=\"http://hackernoon.com/preview/BKBBCSTLqFavQm5Eoyh8\">2. Related Work</a></li>\n<li><a href=\"http://hackernoon.com/preview/XXnT1h5ctddmY9nrTnBm\">3. SwitchLight and 3.1. Preliminaries</a></li>\n<li><a href=\"http://hackernoon.com/preview/aVXXfGWipMDIduPr3etO\">3.2. Problem Formulation</a></li>\n<li><a href=\"http://hackernoon.com/preview/rRt5z9pIkPLhKrnq1MbF\">3.3. Architecture</a></li>\n<li><a href=\"http://hackernoon.com/preview/i6DMgA4RQkBbi0Id5AZP\">3.4. Objectives</a></li>\n<li><a href=\"http://hackernoon.com/preview/X8vk9VjowrvJSYUzEJE5\">4. Multi-Masked Autoencoder Pre-training</a></li>\n<li><a href=\"http://hackernoon.com/preview/fKxaVAijUM9spIBcvVIN\">5. Data</a></li>\n<li><a href=\"http://hackernoon.com/preview/cUeHVmjeXxzOYcxhw4sU\">6. Experiments</a></li>\n<li><a href=\"http://hackernoon.com/preview/1oJNS5kpGcewcRdvmW3K\">7. Conclusion</a></li>\n</ul>\n<p>\\\nAppendix</p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/R0qEhdQamd6t6A0pi9mj\">A. Implementation Details</a></li>\n<li><a href=\"http://hackernoon.com/preview/UiI85lHciL9Y30TZ2Tjz\">B. User Study Interface</a></li>\n<li><a href=\"http://hackernoon.com/preview/ePN0zoobP6j7wBjjnoQo\">C. Video Demonstration</a></li>\n<li><a href=\"https://hackernoon.com/preview/HBKRTpuRwXxGKUw1oP3l\">D. Additional Qualitative Results & References</a></li>\n</ul>\n<h2 id=\"buserstudyinterface\">B. User Study Interface</h2>\n<p>Our user study interface is outlined as follows: Participants are shown an input image next to a diffused ball under the target environment map lighting. The primary objective is to compare our relighting results with baseline methods, as depicted in Fig. 11. Evaluation focuses on three criteria: 1. Consistency of lighting, 2) Preservation of facial details, and 3) Retention of the original identity. This comparison aims to determine which image best matches the lighting of the diffused ball while also maintaining facial details and original identity. To ensure unbiased evaluations, we randomized the order of presentation. Participants evaluated 30 random samples from a set of 256. This dataset included 32 portraits from the FFHQ dataset [25], each illuminated under eight distinct lighting conditions.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-6c8301l.png\" alt=\"Figure 11. User Study Interface comparing relighting results with prior approaches, focusing on consistency in lighting, preservation of facial details, and retention of original identity.\" /></p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2402.18848\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"AI System Uses Teamwork to Create Picture-Perfect Lighting","url":"https://hackernoon.com/ai-system-uses-teamwork-to-create-picture-perfect-lighting?source=rss","date":1734796832,"author":"Auto Encoder: How to Ignore the Signal Noise","unread":true,"desc":"","content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Hoon Kim, Beeble AI, and contributed equally to this work;</p>\n<p>(2) Minje Jang, Beeble AI, and contributed equally to this work;</p>\n<p>(3) Wonjun Yoon, Beeble AI, and contributed equally to this work;</p>\n<p>(4) Jisoo Lee, Beeble AI, and contributed equally to this work;</p>\n<p>(5) Donghyun Na, Beeble AI, and contributed equally to this work;</p>\n<p>(6) Sanghyun Woo, New York University, and contributed equally to this work.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's Note: This is Part 11 of 14 of a study introducing a method for improving how light and shadows can be applied to human portraits in digital images. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<ul>\n<li><a href=\"https://hackernoon.com/preview/Hc7lpYDzMy2xThjUoa2k\">Abstract and 1. Introduction</a></li>\n<li><a href=\"http://hackernoon.com/preview/BKBBCSTLqFavQm5Eoyh8\">2. Related Work</a></li>\n<li><a href=\"http://hackernoon.com/preview/XXnT1h5ctddmY9nrTnBm\">3. SwitchLight and 3.1. Preliminaries</a></li>\n<li><a href=\"http://hackernoon.com/preview/aVXXfGWipMDIduPr3etO\">3.2. Problem Formulation</a></li>\n<li><a href=\"http://hackernoon.com/preview/rRt5z9pIkPLhKrnq1MbF\">3.3. Architecture</a></li>\n<li><a href=\"http://hackernoon.com/preview/i6DMgA4RQkBbi0Id5AZP\">3.4. Objectives</a></li>\n<li><a href=\"http://hackernoon.com/preview/X8vk9VjowrvJSYUzEJE5\">4. Multi-Masked Autoencoder Pre-training</a></li>\n<li><a href=\"http://hackernoon.com/preview/fKxaVAijUM9spIBcvVIN\">5. Data</a></li>\n<li><a href=\"http://hackernoon.com/preview/cUeHVmjeXxzOYcxhw4sU\">6. Experiments</a></li>\n<li><a href=\"http://hackernoon.com/preview/1oJNS5kpGcewcRdvmW3K\">7. Conclusion</a></li>\n</ul>\n<p>\\\nAppendix</p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/R0qEhdQamd6t6A0pi9mj\">A. Implementation Details</a></li>\n<li><a href=\"http://hackernoon.com/preview/UiI85lHciL9Y30TZ2Tjz\">B. User Study Interface</a></li>\n<li><a href=\"http://hackernoon.com/preview/ePN0zoobP6j7wBjjnoQo\">C. Video Demonstration</a></li>\n<li><a href=\"https://hackernoon.com/preview/HBKRTpuRwXxGKUw1oP3l\">D. Additional Qualitative Results & References</a></li>\n</ul>\n<h2 id=\"aimplementationdetails\">A. Implementation Details</h2>\n<p><img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-go830xk.png\" alt=\"\" /></p>\n<p>\\\nWe pre-train a single U-Net architecture during this process. In the subsequent fine-tuning stage, the weights from this pre-trained model are transferred to multiple U-Nets - NormalNet, DiffuseNet, SpecularNet, and RenderNet. In contrast, IllumNet, which does not follow the U-Net architecture, is initialized with random weights. To ensure compatibility with the varying input channels of each network, we modify the weights as necessary. For example, weights pre-trained for RGB channels are copied and adapted to fit networks with 6 or 9 channels.</p>\n<p>\\\n<strong>Data</strong> To generate the relighting training pairs, we randomly select each image from the OLAT dataset. Two randomly chosen HDRI lighting environment maps are then projected onto these images to form a training pair. The images undergo processing in linear space. For managing the dynamic range effectively, we apply logarithmic normalization using the log(1 + x) function.</p>\n<p>\\\n<strong>Architecture</strong> SwitchLight employs a UNet-based architecture, consistently applied across its Normal Net, Diffuse Net, Specular Net, and Render Net. This approach is inspired by recent advancements in diffusion-based models [12]. Unlike standard diffusion methods, we omit the temporal embedding layer. The architecture is characterized by several hyperparameters: the number of input channels, a base channel, and channel multipliers that determine the channel count at each stage. Each downsampling stage features two residual blocks, with attention mechanisms integrated at certain resolutions. The key hyperparameters and their corresponding values are summarized in Table. 4.</p>\n<p>\\\n<strong>IllumNet</strong> is composed of two projection layers, one for transforming the Phong lobe features and another for image features, with the latter using normal bottleneck features as a compact form of image representation. Following this, a cross-attention layer is employed, wherein the Phong lobe serves as the query and the image features function as both key and value. Finally, an output layer generates the final convolved source HDRI.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-zy930kd.png\" alt=\"Table 4. Network Architecture Parameters. This table outlines the key hyperparameters and their corresponding values; initial input channels (In ch), base channels (Base ch), and channel multipliers (Ch mults) that set the stage-specific channel counts. It also indicates the number of residual blocks per stage (Num res), the number of channels per head (Head ch), the stages where attention mechanisms are applied based on feature resolution (Att res), and the final output channels (Out ch).\" /></p>\n<p>\\\nThe <strong>Discriminator</strong> network is utilized during both pretraining and fine-tuning stages, maintaining the same architectural design, although the weights are not shared between these stages. This network is composed of a series of residual blocks, each containing two 3×3 convolution layers, interspersed with Leaky ReLU activations. The number of filters progressively increases across these layers: 64, 128, 256, and 512. Correspondingly, as the channel filter count increases, the resolution of the features decreases, and finally, the network compresses its output with a 3x3 convolution into a single channel, yielding a probability value.</p>\n<p>\\\nRegarding the activation functions across different networks: NormalNet processes its outputs through ℓ2 normalization, ensuring they are unit normal vectors. IllumNet, DiffuseNet, and RenderNet utilize a softplus activation (with β = 20) to generate non-negative pixel values. SpecularNet employs a sigmoid activation fuction, ensuring that both the roughness parameter and Fresnel reflectance values fall within a range of 0 to 1.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2402.18848\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"New AI Relighting Model Outperforms Previous Models","url":"https://hackernoon.com/new-ai-relighting-model-outperforms-previous-models?source=rss","date":1734796830,"author":"Auto Encoder: How to Ignore the Signal Noise","unread":true,"desc":"","content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Hoon Kim, Beeble AI, and contributed equally to this work;</p>\n<p>(2) Minje Jang, Beeble AI, and contributed equally to this work;</p>\n<p>(3) Wonjun Yoon, Beeble AI, and contributed equally to this work;</p>\n<p>(4) Jisoo Lee, Beeble AI, and contributed equally to this work;</p>\n<p>(5) Donghyun Na, Beeble AI, and contributed equally to this work;</p>\n<p>(6) Sanghyun Woo, New York University, and contributed equally to this work.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's Note: This is Part 10 of 14 of a study introducing a method for improving how light and shadows can be applied to human portraits in digital images. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<ul>\n<li><a href=\"https://hackernoon.com/preview/Hc7lpYDzMy2xThjUoa2k\">Abstract and 1. Introduction</a></li>\n<li><a href=\"http://hackernoon.com/preview/BKBBCSTLqFavQm5Eoyh8\">2. Related Work</a></li>\n<li><a href=\"http://hackernoon.com/preview/XXnT1h5ctddmY9nrTnBm\">3. SwitchLight and 3.1. Preliminaries</a></li>\n<li><a href=\"http://hackernoon.com/preview/aVXXfGWipMDIduPr3etO\">3.2. Problem Formulation</a></li>\n<li><a href=\"http://hackernoon.com/preview/rRt5z9pIkPLhKrnq1MbF\">3.3. Architecture</a></li>\n<li><a href=\"http://hackernoon.com/preview/i6DMgA4RQkBbi0Id5AZP\">3.4. Objectives</a></li>\n<li><a href=\"http://hackernoon.com/preview/X8vk9VjowrvJSYUzEJE5\">4. Multi-Masked Autoencoder Pre-training</a></li>\n<li><a href=\"http://hackernoon.com/preview/fKxaVAijUM9spIBcvVIN\">5. Data</a></li>\n<li><a href=\"http://hackernoon.com/preview/cUeHVmjeXxzOYcxhw4sU\">6. Experiments</a></li>\n<li><a href=\"http://hackernoon.com/preview/1oJNS5kpGcewcRdvmW3K\">7. Conclusion</a></li>\n</ul>\n<p>\\\nAppendix</p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/R0qEhdQamd6t6A0pi9mj\">A. Implementation Details</a></li>\n<li><a href=\"http://hackernoon.com/preview/UiI85lHciL9Y30TZ2Tjz\">B. User Study Interface</a></li>\n<li><a href=\"http://hackernoon.com/preview/ePN0zoobP6j7wBjjnoQo\">C. Video Demonstration</a></li>\n<li><a href=\"https://hackernoon.com/preview/HBKRTpuRwXxGKUw1oP3l\">D. Additional Qualitative Results & References</a></li>\n</ul>\n<h2 id=\"7conclusion\">7. Conclusion</h2>\n<p>We introduce SwitchLight, an architecture based on Cook-Torrance rendering physics, enhanced with a selfsupervised pre-training framework. This co-designed approach significantly outperforms previous models. Our future plans include scaling the current model beyond images to encompass video and 3D data. We hope our proposal serve as a new foundational model for relighting tasks.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2402.18848\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"New Framework by Beeble Researchers Promises to Bring Realistic Glow to Digital Portraits Using AI","url":"https://hackernoon.com/new-framework-by-beeble-researchers-promises-to-bring-realistic-glow-to-digital-portraits-using-ai?source=rss","date":1734796827,"author":"Auto Encoder: How to Ignore the Signal Noise","unread":true,"desc":"","content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Hoon Kim, Beeble AI, and contributed equally to this work;</p>\n<p>(2) Minje Jang, Beeble AI, and contributed equally to this work;</p>\n<p>(3) Wonjun Yoon, Beeble AI, and contributed equally to this work;</p>\n<p>(4) Jisoo Lee, Beeble AI, and contributed equally to this work;</p>\n<p>(5) Donghyun Na, Beeble AI, and contributed equally to this work;</p>\n<p>(6) Sanghyun Woo, New York University, and contributed equally to this work.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's Note: This is Part 1 of 14 of a study introducing a method for improving how light and shadows can be applied to human portraits in digital images. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<ul>\n<li><a href=\"https://hackernoon.com/preview/Hc7lpYDzMy2xThjUoa2k\">Abstract and 1. Introduction</a></li>\n<li><a href=\"http://hackernoon.com/preview/BKBBCSTLqFavQm5Eoyh8\">2. Related Work</a></li>\n<li><a href=\"http://hackernoon.com/preview/XXnT1h5ctddmY9nrTnBm\">3. SwitchLight and 3.1. Preliminaries</a></li>\n<li><a href=\"http://hackernoon.com/preview/aVXXfGWipMDIduPr3etO\">3.2. Problem Formulation</a></li>\n<li><a href=\"http://hackernoon.com/preview/rRt5z9pIkPLhKrnq1MbF\">3.3. Architecture</a></li>\n<li><a href=\"http://hackernoon.com/preview/i6DMgA4RQkBbi0Id5AZP\">3.4. Objectives</a></li>\n<li><a href=\"http://hackernoon.com/preview/X8vk9VjowrvJSYUzEJE5\">4. Multi-Masked Autoencoder Pre-training</a></li>\n<li><a href=\"http://hackernoon.com/preview/fKxaVAijUM9spIBcvVIN\">5. Data</a></li>\n<li><a href=\"http://hackernoon.com/preview/cUeHVmjeXxzOYcxhw4sU\">6. Experiments</a></li>\n<li><a href=\"http://hackernoon.com/preview/1oJNS5kpGcewcRdvmW3K\">7. Conclusion</a></li>\n</ul>\n<p>\\\nAppendix</p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/R0qEhdQamd6t6A0pi9mj\">A. Implementation Details</a></li>\n<li><a href=\"http://hackernoon.com/preview/UiI85lHciL9Y30TZ2Tjz\">B. User Study Interface</a></li>\n<li><a href=\"http://hackernoon.com/preview/ePN0zoobP6j7wBjjnoQo\">C. Video Demonstration</a></li>\n<li><a href=\"https://hackernoon.com/preview/HBKRTpuRwXxGKUw1oP3l\">D. Additional Qualitative Results & References</a></li>\n</ul>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-w1830re.png\" alt=\"Figure 1. Be Anywhere at Any Time. SwitchLight processes a human portrait by decomposing it into detailed intrinsic components, and re-renders the image under a designated target illumination, ensuring a seamless composition of the subject into any new environment.\" /></p>\n<h2 id=\"abstract\">Abstract</h2>\n<p><em>We introduce a co-designed approach for human portrait relighting that combines a physics-guided architecture with a pre-training framework. Drawing on the Cook-Torrance reflectance model, we have meticulously configured the architecture design to precisely simulate light-surface interactions. Furthermore, to overcome the limitation of scarce high-quality lightstage data, we have developed a selfsupervised pre-training strategy. This novel combination of accurate physical modeling and expanded training dataset establishes a new benchmark in relighting realism.</em></p>\n<h2 id=\"1introduction\">1. Introduction</h2>\n<p>Relighting is more than an aesthetic tool; it unlocks infinite narrative possibilities and enables seamless integration of subjects into diverse environments (see Fig. 1). This advancement resonates with our innate desire to transcend the physical constraints of space and time, while also providing tangible solutions to practical challenges in digital content creation. It is particularly transformative in virtual (VR) and augmented reality (AR) applications, where relighting facilitates the real-time adaptation of lighting, ensuring that users and digital elements coexist naturally within any environment, offering a next level of telepresence.</p>\n<p>\\\nIn this work, we focus on human portrait relighting. While the relighting task fundamentally demands an indepth understanding of geometry, material properties, and illumination, the challenge is more compounded when addressing human subjects, due to the unique characteristics of skin surfaces as well as the diverse textures and reflectance properties of a wide array of clothing, hairstyles, and accessories. These elements interact in complex ways, necessitating advanced algorithms capable of simulating the subtle interplay of light with these varied surfaces.</p>\n<p>\\\nCurrently, the most promising approach involves the use of deep neural networks trained on pairs of high-quality relit portrait images and their corresponding intrinsic attributes, which are sourced from a light stage setup [10]. Initial efforts approached the relighting process as a ‘black box’ [45, 48], without delving into the underlying mechanisms. Later advancements adopted a physics-guided model design, incorporating the explicit modeling of image intrinsics and image formation physics [32]. Pandey et al. [34] proposed the Total Relight (TR) architecture, also physics-guided, which decomposes an input image into surface normals and albedo maps, and performs relighting based on the Phong specular reflectance model. The TR architecture has become foundational model for image relighting, with most recent and advanced architectures building upon its principle [23, 31, 52].</p>\n<p>\\\nFollowing the physics-guided approach, our contribution lies in a co-design of architecture with a self-supervised pre-training framework. First, our architecture evolves towards a more accurate physical model by integrating the Cook-Torrance specular reflectance model [8], representing a notable advancement from the empirical Phong specular model [37] employed in the Total Relight architecture. The Cook-Torrance model adeptly simulates light interactions with surface microfacets, accounting for spatially varying roughness and reflectivity. Secondly, our pretraining framework scales the learning process beyond the typically hard-to-obtain lightstage data. By revisiting the masked autoencoder (MAE) framework [19], we adept it for the task of relighting. These modifications are crafted to address the unique challenges posed by this task, enabling our model to learn from unlabelled data and refine its ability to produce realistic relit portraits during fine-tuning. To the best of our knowledge, this is the first time applying self-supervised pre-training specifically to the relighting task.</p>\n<p>\\\nTo summarize, our contribution is twofold. Firstly, by enhancing the physical reflectance model, we have introduced a new level of realism in the output. Secondly, by adopting self-supervised learning, we have expanded the scale of the training data and enhanced the expression of lighting in diverse real-world scenarios. Collectively, these advancements have led SwitchLight framework to achieve a new state-of-the-art in human portrait relighting.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2402.18848\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"See the Stunning Details This AI Preserves in Relit Photos","url":"https://hackernoon.com/see-the-stunning-details-this-ai-preserves-in-relit-photos?source=rss","date":1734796824,"author":"Auto Encoder: How to Ignore the Signal Noise","unread":true,"desc":"","content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Hoon Kim, Beeble AI, and contributed equally to this work;</p>\n<p>(2) Minje Jang, Beeble AI, and contributed equally to this work;</p>\n<p>(3) Wonjun Yoon, Beeble AI, and contributed equally to this work;</p>\n<p>(4) Jisoo Lee, Beeble AI, and contributed equally to this work;</p>\n<p>(5) Donghyun Na, Beeble AI, and contributed equally to this work;</p>\n<p>(6) Sanghyun Woo, New York University, and contributed equally to this work.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's Note: This is Part 14 of 14 of a study introducing a method for improving how light and shadows can be applied to human portraits in digital images. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<ul>\n<li><a href=\"https://hackernoon.com/preview/Hc7lpYDzMy2xThjUoa2k\">Abstract and 1. Introduction</a></li>\n<li><a href=\"http://hackernoon.com/preview/BKBBCSTLqFavQm5Eoyh8\">2. Related Work</a></li>\n<li><a href=\"http://hackernoon.com/preview/XXnT1h5ctddmY9nrTnBm\">3. SwitchLight and 3.1. Preliminaries</a></li>\n<li><a href=\"http://hackernoon.com/preview/aVXXfGWipMDIduPr3etO\">3.2. Problem Formulation</a></li>\n<li><a href=\"http://hackernoon.com/preview/rRt5z9pIkPLhKrnq1MbF\">3.3. Architecture</a></li>\n<li><a href=\"http://hackernoon.com/preview/i6DMgA4RQkBbi0Id5AZP\">3.4. Objectives</a></li>\n<li><a href=\"http://hackernoon.com/preview/X8vk9VjowrvJSYUzEJE5\">4. Multi-Masked Autoencoder Pre-training</a></li>\n<li><a href=\"http://hackernoon.com/preview/fKxaVAijUM9spIBcvVIN\">5. Data</a></li>\n<li><a href=\"http://hackernoon.com/preview/cUeHVmjeXxzOYcxhw4sU\">6. Experiments</a></li>\n<li><a href=\"http://hackernoon.com/preview/1oJNS5kpGcewcRdvmW3K\">7. Conclusion</a></li>\n</ul>\n<p>\\\nAppendix</p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/R0qEhdQamd6t6A0pi9mj\">A. Implementation Details</a></li>\n<li><a href=\"http://hackernoon.com/preview/UiI85lHciL9Y30TZ2Tjz\">B. User Study Interface</a></li>\n<li><a href=\"http://hackernoon.com/preview/ePN0zoobP6j7wBjjnoQo\">C. Video Demonstration</a></li>\n<li><a href=\"https://hackernoon.com/preview/HBKRTpuRwXxGKUw1oP3l\">D. Additional Qualitative Results & References</a></li>\n</ul>\n<h2 id=\"dadditionalqualitativeresults\">D. Additional Qualitative Results</h2>\n<p>Further qualitative results are provided in Fig.12, 13, 14, 15, and 16. Each figure illustrates the relighting of a source image in eight distinct target lighting environments. In these figures, our approach is benchmarked against prior stateof-the-art methods, namely SIPR [45], Lumos [52], and TR [34], utilizing images from Pexels [1]. This comparison is enabled by the original authors who applied their models to identical inputs and provided their respective outputs.</p>\n<p>\\\nWe can clearly observe that our method demonstrates notable efficacy in achieving consistent lighting, maintaining softness and high-frequency detail. Additionally, it effectively manages specular highlights and hard shadows, while meticulously preserving facial details, identity, skin tones, and hair texture.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-2j830uz.png\" alt=\"Figure 12. Qualitative Comparisons with state-of-the-art approaches.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-mn930db.png\" alt=\"Figure 13. Qualitative Comparisons with state-of-the-art approaches.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-wma30j2.png\" alt=\"Figure 14. Qualitative Comparisons with state-of-the-art approaches.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-38b30sb.png\" alt=\"Figure 15. Qualitative Comparisons with state-of-the-art approaches.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-pxc30un.png\" alt=\"Figure 16. Qualitative Comparisons with state-of-the-art approaches.\" /></p>\n<h2 id=\"references\">References</h2>\n<p>[1] Pexels. https://www.pexels.com. 6, 7, 10</p>\n<p>\\\n[2] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. 2</p>\n<p>\\\n[3] Jonathan T Barron and Jitendra Malik. Shape, illumination, and reflectance from shading. IEEE transactions on pattern analysis and machine intelligence, 37(8):1670–1687, 2014. 2</p>\n<p>\\\n[4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12299–12310, 2021. 2</p>\n<p>\\\n[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597–1607. PMLR, 2020. 2</p>\n<p>\\\n[6] Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. Activating more pixels in image superresolution transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 22367–22377, 2023. 2</p>\n<p>\\\n[7] Zhaoxi Chen and Ziwei Liu. Relighting4d: Neural relightable human from videos. In European Conference on Computer Vision, pages 606–623. Springer, 2022. 2</p>\n<p>\\\n[8] Robert L Cook and Kenneth E. Torrance. A reflectance model for computer graphics. ACM Transactions on Graphics (ToG), 1(1):7–24, 1982. 2</p>\n<p>\\\n[9] Paul Debevec. Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography. In Acm siggraph 2008 classes, pages 1–10. 2008. 2</p>\n<p>\\\n[10] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and Mark Sagar. Acquiring the reflectance field of a human face. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques, pages 145–156, 2000. 2, 6</p>\n<p>\\\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 2</p>\n<p>\\\n[12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780–8794, 2021. 9</p>\n<p>\\\n[13] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE international conference on computer vision, pages 1422–1430, 2015. 2</p>\n<p>\\\n[14] Julie Dorsey, James Arvo, and Donald Greenberg. Interactive design of complex time dependent lighting. IEEE Computer Graphics and Applications, 15(2):26–36, 1995. 2</p>\n<p>\\\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 2, 6</p>\n<p>\\\n[16] David Futschik, Kelvin Ritland, James Vecore, Sean Fanello, Sergio Orts-Escolano, Brian Curless, Daniel Sykora, and Ro- ` hit Pandey. Controllable light diffusion for portraits. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8412–8421, 2023. 8</p>\n<p>\\\n[17] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. 2</p>\n<p>\\\n[18] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729–9738, 2020. 2</p>\n<p>\\\n[19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable ´ vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000– 16009, 2022. 2, 6, 8</p>\n<p>\\\n[20] Andrew Hou, Ze Zhang, Michel Sarkis, Ning Bi, Yiying Tong, and Xiaoming Liu. Towards high fidelity face relighting with realistic shadows. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14719–14728, 2021. 2</p>\n<p>\\\n[21] Andrew Hou, Michel Sarkis, Ning Bi, Yiying Tong, and Xiaoming Liu. Face relighting with geometrically consistent shadows. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4217– 4226, 2022. 2</p>\n<p>\\\n[22] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125–1134, 2017. 5, 6</p>\n<p>\\\n[23] Chaonan Ji, Tao Yu, Kaiwen Guo, Jingxin Liu, and Yebin Liu. Geometry-aware single-image full-body human relighting. In European Conference on Computer Vision, pages 388–405. Springer, 2022. 2</p>\n<p>\\\n[24] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 694–711. Springer, 2016. 5, 6</p>\n<p>\\\n[25] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401–4410, 2019. 6, 9, 10</p>\n<p>\\\n[26] Taehun Kim, Kunhee Kim, Joonyeong Lee, Dongmin Cha, Jiho Lee, and Daijin Kim. Revisiting image pyramid structure for high resolution salient object detection. In Proceedings of the Asian Conference on Computer Vision, pages 108–124, 2022. 4</p>\n<p>\\\n[27] Wenbo Li, Xin Lu, Shengju Qian, Jiangbo Lu, Xiangyu Zhang, and Jiaya Jia. On efficient transformer-based  image pre-training for low-level vision. arXiv preprint arXiv:2112.10175, 2021. 2</p>\n<p>\\\n[28] Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta, Brian L Curless, Steven M Seitz, and Ira KemelmacherShlizerman. Real-time high-resolution background matting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8762–8771, 2021. 4</p>\n<p>\\\n[29] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irregular holes using partial convolutions. In Proceedings of the European conference on computer vision (ECCV), pages 85–100, 2018. 6</p>\n<p>\\\n[30] Yihao Liu, Jingwen He, Jinjin Gu, Xiangtao Kong, Yu Qiao, and Chao Dong. Degae: A new pretraining paradigm for low-level vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23292–23303, 2023. 2</p>\n<p>\\\n[31] Yiqun Mei, He Zhang, Xuaner Zhang, Jianming Zhang, Zhixin Shu, Yilin Wang, Zijun Wei, Shi Yan, HyunJoon Jung, and Vishal M Patel. Lightpainter: Interactive portrait relighting with freehand scribble. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 195–205, 2023. 2, 6</p>\n<p>\\\n[32] Thomas Nestmeyer, Jean-Franc¸ois Lalonde, Iain Matthews, and Andreas Lehrmann. Learning physics-guided face relighting under directional light. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5124–5133, 2020. 2</p>\n<p>\\\n[33] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European conference on computer vision, pages 69–84. Springer, 2016. 2</p>\n<p>\\\n[34] Rohit Pandey, Sergio Orts Escolano, Chloe Legendre, Christian Haene, Sofien Bouaziz, Christoph Rhemann, Paul Debevec, and Sean Fanello. Total relighting: learning to relight portraits for background replacement. ACM Transactions on Graphics (TOG), 40(4):1–21, 2021. 2, 5, 6, 7, 10</p>\n<p>\\\n[35] Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim, and Sangdoo Yun. What do self-supervised vision transformers learn? arXiv preprint arXiv:2305.00729, 2023. 2</p>\n<p>\\\n[36] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2536–2544, 2016. 2</p>\n<p>\\\n[37] Bui Tuong Phong. Illumination for computer generated pictures. In Seminal graphics: pioneering efforts that shaped the field, pages 95–101. 1998. 2 [38] Puntawat Ponglertnapakorn, Nontawat Tritrong, and Supasorn Suwajanakorn. Difareli: Diffusion face relighting. arXiv preprint arXiv:2304.09479, 2023. 2</p>\n<p>\\\n[39] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. 2</p>\n<p>\\\n[40] Soumyadip Sengupta, Angjoo Kanazawa, Carlos D Castillo, and David W Jacobs. Sfsnet: Learning shape, reflectance and illuminance of facesin the wild’. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6296–6305, 2018. 2</p>\n<p>\\\n[41] Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steven M Seitz, and Ira Kemelmacher-Shlizerman. Background matting: The world is your green screen. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2291–2300, 2020. 4</p>\n<p>\\\n[42] Soumyadip Sengupta, Brian Curless, Ira KemelmacherShlizerman, and Steven M Seitz. A light stage on every desk. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2420–2429, 2021. 2</p>\n<p>\\\n[43] YiChang Shih, Sylvain Paris, Connelly Barnes, William T Freeman, and Fredo Durand. Style transfer for headshot por- ´ traits. 2014. 2</p>\n<p>\\\n[44] Zhixin Shu, Sunil Hadap, Eli Shechtman, Kalyan Sunkavalli, Sylvain Paris, and Dimitris Samaras. Portrait lighting transfer using a mass transport approach. ACM Transactions on Graphics (TOG), 36(4):1, 2017. 2</p>\n<p>\\\n[45] Tiancheng Sun, Jonathan T Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay Busch, Paul Debevec, and Ravi Ramamoorthi. Single image portrait relighting. ACM Transactions on Graphics (TOG), 38(4):1–12, 2019. 2, 6, 7, 10</p>\n<p>\\\n[46] Piotr Teterwak, Aaron Sarna, Dilip Krishnan, Aaron Maschinot, David Belanger, Ce Liu, and William T Freeman. Boundless: Generative adversarial networks for image extension. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10521–10530, 2019. 6</p>\n<p>\\\n[47] Yifan Wang, Aleksander Holynski, Xiuming Zhang, and Xuaner Zhang. Sunstage: Portrait reconstruction and relighting using the sun as a light stage. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20792–20802, 2023. 2</p>\n<p>\\\n[48] Zhibo Wang, Xin Yu, Ming Lu, Quan Wang, Chen Qian, and Feng Xu. Single image portrait relighting via explicit multiple reflectance channel modeling. ACM Transactions on Graphics (TOG), 39(6):1–13, 2020. 2</p>\n<p>\\\n[49] Andreas Wenger, Andrew Gardner, Chris Tchou, Jonas Unger, Tim Hawkins, and Paul Debevec. Performance relighting and reflectance transformation with timemultiplexed illumination. ACM Transactions on Graphics (TOG), 24(3):756–764, 2005. 2, 6</p>\n<p>\\\n[50] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16133– 16142, 2023. 2, 6</p>\n<p>\\\n[51] Robert J Woodham. Photometric method for determining surface orientation from multiple images. Optical engineering, 19(1):139–144, 1980. 6</p>\n<p>\\\n[52] Yu-Ying Yeh, Koki Nagano, Sameh Khamis, Jan Kautz, Ming-Yu Liu, and Ting-Chun Wang. Learning to relight portrait images via a virtual light stage and synthetic-to-real adaptation. ACM Transactions on Graphics (TOG), 41(6): 1–21, 2022. 2, 6, 7, 8, 10</p>\n<p>\\\n[53] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pages 649–666. Springer, 2016. 2</p>\n<p>\\\n[54] Xuaner Zhang, Jonathan T Barron, Yun-Ta Tsai, Rohit Pandey, Xiuming Zhang, Ren Ng, and David E Jacobs. Portrait shadow manipulation. ACM Transactions on Graphics (TOG), 39(4):78–1, 2020. 8</p>\n<p>\\\n[55] Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David W Jacobs. Deep single-image portrait relighting. In Proceedings of the IEEE/CVF international conference on computer vision, pages 7194–7202, 2019. 2</p>\n<p>\\\n[56] Taotao Zhou, Kai He, Di Wu, Teng Xu, Qixuan Zhang, Kuixiang Shao, Wenzheng Chen, Lan Xu, and Jingyi Yu. Relightable neural human assets from multi-view gradient illuminations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4315– 4327, 2023. 2</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2402.18848\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Beeble Researchers Use Physics-Based Models to Achieve Realistic Lighting Effects in Images","url":"https://hackernoon.com/beeble-researchers-use-physics-based-models-to-achieve-realistic-lighting-effects-in-images?source=rss","date":1734796820,"author":"Auto Encoder: How to Ignore the Signal Noise","unread":true,"desc":"","content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Hoon Kim, Beeble AI, and contributed equally to this work;</p>\n<p>(2) Minje Jang, Beeble AI, and contributed equally to this work;</p>\n<p>(3) Wonjun Yoon, Beeble AI, and contributed equally to this work;</p>\n<p>(4) Jisoo Lee, Beeble AI, and contributed equally to this work;</p>\n<p>(5) Donghyun Na, Beeble AI, and contributed equally to this work;</p>\n<p>(6) Sanghyun Woo, New York University, and contributed equally to this work.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's Note: This is Part 2 of 14 of a study introducing a method for improving how light and shadows can be applied to human portraits in digital images. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<ul>\n<li><a href=\"https://hackernoon.com/preview/Hc7lpYDzMy2xThjUoa2k\">Abstract and 1. Introduction</a></li>\n<li><a href=\"http://hackernoon.com/preview/BKBBCSTLqFavQm5Eoyh8\">2. Related Work</a></li>\n<li><a href=\"http://hackernoon.com/preview/XXnT1h5ctddmY9nrTnBm\">3. SwitchLight and 3.1. Preliminaries</a></li>\n<li><a href=\"http://hackernoon.com/preview/aVXXfGWipMDIduPr3etO\">3.2. Problem Formulation</a></li>\n<li><a href=\"http://hackernoon.com/preview/rRt5z9pIkPLhKrnq1MbF\">3.3. Architecture</a></li>\n<li><a href=\"http://hackernoon.com/preview/i6DMgA4RQkBbi0Id5AZP\">3.4. Objectives</a></li>\n<li><a href=\"http://hackernoon.com/preview/X8vk9VjowrvJSYUzEJE5\">4. Multi-Masked Autoencoder Pre-training</a></li>\n<li><a href=\"http://hackernoon.com/preview/fKxaVAijUM9spIBcvVIN\">5. Data</a></li>\n<li><a href=\"http://hackernoon.com/preview/cUeHVmjeXxzOYcxhw4sU\">6. Experiments</a></li>\n<li><a href=\"http://hackernoon.com/preview/1oJNS5kpGcewcRdvmW3K\">7. Conclusion</a></li>\n</ul>\n<p>\\\nAppendix</p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/R0qEhdQamd6t6A0pi9mj\">A. Implementation Details</a></li>\n<li><a href=\"http://hackernoon.com/preview/UiI85lHciL9Y30TZ2Tjz\">B. User Study Interface</a></li>\n<li><a href=\"http://hackernoon.com/preview/ePN0zoobP6j7wBjjnoQo\">C. Video Demonstration</a></li>\n<li><a href=\"https://hackernoon.com/preview/HBKRTpuRwXxGKUw1oP3l\">D. Additional Qualitative Results & References</a></li>\n</ul>\n<h2 id=\"2relatedwork\">2. Related Work</h2>\n<p><strong>Human Portrait Relighting</strong> is an ill-posed problem due to its under-constrained nature. To tackle this, earlier methods incorporated 3D facial priors [44], exploited image intrinsics [3, 40], or framed the task as a style transfer [43]. Light stage techniques [49] offer a more powerful solution by recording subject’s reflectance fields under varying lighting conditions [10, 14], though they are labor-intensive and require specialized equipment. A promising alternative has emerged with deep learning, utilizing neural networks trained on light stage data. Sun et al. [45] pioneered this approach, but their method had limitations in representing non-Lambertian effects. This was improved upon by Nestmeyer et al. [32], who integrated rendering physics into network design, albeit limited to directional light. Building upon this, Pandey et al. [34] incorporated the Phong reflection model and a high dynamic range (HDR) lighting map [9] into their network, enabling a more accurate representation of global illumination. Simultaneously, efforts have been made to explore portrait relighting without light stage data [20, 21, 42, 47, 55]. Moreover, introduction of NeRF [7] and diffusion-based [38] models has opened new avenues in the field. However, networks trained with lightstage data maintain superior accuracy and realism, thanks to physics-based composited relight image training pairs and precise ground truth image intrinsics [56].</p>\n<p>\\\nOur work furthers this domain by integrating the Cook-Torrance model into our network design, shifting from the empirical Phong model to a more physics-based approach, thereby enhancing the realism and detail in relit images.</p>\n<p>\\\n<strong>Self-supervised Pre-training</strong> has become a standard training scheme in the development of large language models like BERT [11] and GPT [39], and is increasingly influential in vision models, aiming to replicate the ‘BERT moment’. This approach typically involves pre-training on extensive unlabeled data, followed by fine-tuning on specific tasks. While early efforts in vision models focused on simple pretext tasks [13, 17, 33, 36, 53], the field has evolved through stages like contrastive learning [5, 18] and masked image modeling [2, 19, 50]. However, the primary focus has remained on visual recognition, with less attention to other domains. Exceptions include low-level image processing tasks [4, 6, 27, 30] using the vision transformer [15].</p>\n<p>\\\nOur research takes a different route, focusing on human portrait relighting—a complex challenge of manipulating illumination in the image. This direction is crucial because acquiring accurate ground truth data, especially from light stage, is both expensive and difficult. We modify the MAE framework [19], previously successful in robust image representation learning and developing locality biases [35], to suit the unique requirements of effective relighting.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2402.18848\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"If climate tech is dead, what comes next?","url":"https://techcrunch.com/2024/12/21/if-climate-tech-is-dead-what-comes-next/","date":1734796800,"author":"Tim De Chant","unread":true,"desc":"","content":"<p>Climate tech is about a decade old, and people are itching for a rebrand.</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"'Open Source Software Funding Report' Finds 86% of Corporate Contributions are Employees' Time","url":"https://news.slashdot.org/story/24/12/21/0016200/open-source-software-funding-report-finds-86-of-corporate-contributions-are-employees-time?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1734795240,"author":"EditorDavid","unread":true,"desc":"","content":"The Linux Foundation partnered with GitHub and Harvard's Laboratory for Innovation Science to research organization-driven investments in open source software &mdash; the how and the why &mdash; surveying over 500 organizations around the world. \nSo what are the highlights from the published report?\n\nThe median responding organization invests $520,600 (2023 USD) of annual value to OSS. \n\nResponding organizations annually invest $1.7 billion in open source, which can be extrapolated to estimate that approximately $7.7 billion is invested across the entire open source ecosystem annually. 86% of investment is in the form of contribution labor by employees and contractors working for the funding organization, with the remaining 14% being direct financial contributions. \nBut the ultimate goal of the research was ideas \"to improve monitoring and investing in open source\" (to \"create a more sustainable and impactful open source economy...\")\n\nIn this research, we discovered a few key obstacles that make this kind of data capture challenging... [O]rganizations have blind spots when it comes to the specifics of their contributions. Many respondents knew where they contribute, but only a portion of those could answer how many labor hours went into their OSS contributions or the percentage of budget that went to OSS. Second, the decentralized nature of organizational contributions, without explicit policies or centralized groups that encourage and organize this effort, make reporting even more challenging... \n[W]e recommend that policies and practices are put in place to encourage employees to self-report their contributions, and do so using their employee email addresses to leave fingerprints on their work. We also suggest that open source work is consolidated under a single banner, such as an Open Source Program Office (OSPO). Finally, we suggest incorporating contribution monitoring into the organization's pipeline. We developed a toolkit to help improve data capture and monitoring.\n<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status='Open+Source+Software+Funding+Report'+Finds+86%25+of+Corporate+Contributions+are+Employees'+Time%3A+https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F21%2F0016200%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F21%2F0016200%2Fopen-source-software-funding-report-finds-86-of-corporate-contributions-are-employees-time%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://news.slashdot.org/story/24/12/21/0016200/open-source-software-funding-report-finds-86-of-corporate-contributions-are-employees-time?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23557559&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"CachyOS Update Now Uses AutoFDO-Optimized Kernel, Rusticl Driver","url":"https://www.phoronix.com/news/CachyOS-December-2024","date":1734794488,"author":"Michael Larabel","unread":true,"desc":"","content":"The CachyOS December 2024 update is out today as the newest monthly release to this performance-optimized, Arch Linux based operating system...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Hollywood angels: Here are the celebrities who are also star VCs","url":"https://techcrunch.com/2024/12/21/hollywood-angels-here-are-the-celebrities-who-are-also-star-vcs/","date":1734793200,"author":"Dominic-Madori Davis","unread":true,"desc":"","content":"<p>Becoming a venture capitalist has become the latest status symbol In Hollywood. <br />\nEveryone from Olivia Wilde to Emma Watson is investing in startups.</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Raspberry Pi HEVC Decoder Driver Posted For Linux Kernel Review","url":"https://www.phoronix.com/news/Raspberry-Pi-HEVC-H265-Decode","date":1734791490,"author":"Michael Larabel","unread":true,"desc":"","content":"The latest work that Raspberry Pi is working to upstream to the mainline Linux kernel is a HEVC/H.265 video decode driver that works on Raspberry Pi 4 and Raspberry Pi 5 single board computers...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Unforgeable Quantum Tokens Delivered Over Fiber Network","url":"https://spectrum.ieee.org/quantum-tokens","date":1734789604,"author":"John Boyd","unread":true,"desc":"","content":"<p>The demonstration redeemed secure tokens across 10 kilometers</p>","flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTM2MDg4OC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc1MDE5Mzg3MH0.tPfJRuK9Oc-Mcf2w7QxQl8wphARYTBcy9WzigFZo0eY/image.jpg?width=600","enclosureMime":""},{"title":"Meet Skyseed, a VC fund and incubator backing the Bluesky and AT Protocol ecosystem","url":"https://techcrunch.com/2024/12/21/meet-skyseed-a-vc-fund-and-incubator-backing-the-bluesky-and-at-protocol-ecosystem/","date":1734789600,"author":"Paul Sawers","unread":true,"desc":"","content":"<p>On November 15, Peter Wang posted a message requesting ideas for a new incubator and fund to support experimental projects built on the burgeoning Bluesky/AT Protocol ecosystem. Four weeks later, Skyseed emerged with an initial commitment of $1 million. This turnaround, a speed underscored by the fact that the fund doesn&#8217;t even have a website [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Curl Drops Support For Hyper Rust HTTP Backend Citing Little Demand","url":"https://www.phoronix.com/news/Curl-Drops-Rust-Hyper-Backend","date":1734788646,"author":"Michael Larabel","unread":true,"desc":"","content":"The widely-used Curl project has removed support for its Rust-written Hyper HTTP back-end that they were experimentally shipping for several years. The removal of this Rust back-end comes from having little end-user and developer interest in this portion of the code...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"AI Writing Is Improving, But It Still Can't Match Human Creativity","url":"https://science.slashdot.org/story/24/12/21/0058254/ai-writing-is-improving-but-it-still-cant-match-human-creativity?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1734786000,"author":"BeauHD","unread":true,"desc":"","content":"sciencehabit shares a report from Science Magazine: With a few keystrokes, anyone can ask an artificial intelligence (AI) program such as ChatGPT to write them a term paper, a rap song, or a play. But don't expect William Shakespeare's originality. A new study finds such output remains derivative -- at least for now. [...] [O]bjectively testing this creativity has been tricky. Scientists have generally taken two tacks. One is to use another computer program to search for signs of plagiarism -- though a lack of plagiarism does not necessarily equal creativity. The other approach is to have humans judge the AI output themselves, rating factors such as fluency and originality. But that's subjective and time intensive. So Ximing Lu, a computer scientist at the University of Washington, and colleagues created a program featuring both objectivity and a bit of nuance.\n \nCalled DJ Search, it collects pieces of text of a minimum length from whatever the AI outputs and searches for them in large online databases. DJ Search doesn't just look for identical matches; it also scans for strings whose words have similar meanings. To evaluate the meaning of a word or phrase, the program itself relies on a separate AI algorithm that produces a set of numbers called an \"embedding,\" which roughly represents the contexts in which words are typically found. Synonymous words have numerically close embeddings. For example, phrases that swap \"anticipation\" and \"excitement\" are considered matches. After removing all matches, the program calculates the ratio of the remaining words to the original document length, which should give an estimate of how much of the AI's output is novel. The program conducts this process for various string lengths (the study uses a minimum of five words) and combines the ratios into one index of linguistic novelty. (The team calls it a \"creativity index,\" but creativity requires both novelty and quality -- random gibberish is novel but not creative.)\n \nThe researchers compared the linguistic novelty of published novels, poetry, and speeches with works written by recent LLMs. Humans outscored AIs by about 80% in poetry, 100% in novels, and 150% in speeches, the researchers report in a preprint posted on OpenReview and currently under peer review. Although DJ Search was designed for comparing people and machines, it can also be used to compare two or more humanmade works. For example, Suzanne Collins's 2008 novel The Hunger Games scored 35% higher in linguistic originality than Stephenie Meyer's 2005 hit Twilight. (You can try the tool online.)<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=AI+Writing+Is+Improving%2C+But+It+Still+Can't+Match+Human+Creativity%3A+https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F21%2F0058254%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F21%2F0058254%2Fai-writing-is-improving-but-it-still-cant-match-human-creativity%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://science.slashdot.org/story/24/12/21/0058254/ai-writing-is-improving-but-it-still-cant-match-human-creativity?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23557589&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"AdaptiveCpp 24.10 Delivers More Performance Optimizations","url":"https://www.phoronix.com/news/AdaptiveCpp-24.10","date":1734781680,"author":"Michael Larabel","unread":true,"desc":"","content":"AdaptiveCpp 24.10 is out today as this implementation of SYCL and C++ standard parallelism for CPUs and GPUs across hardware vendors. This compiler for C++ heterogeneous programming models has tacked on more features and additional performance optimizations with this update...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"GNOME User Sharing Ported From C To Rust, libadwaita Adds Adaptive Preview Mode","url":"https://www.phoronix.com/news/GNOME-Pre-Christmas-2024","date":1734780117,"author":"Michael Larabel","unread":true,"desc":"","content":"It's not only KDE seeing nice improvements ahead of the holidays but GNOME developers were also busy this week preparing new improvements to their open-source desktop stack. There's been some rather exciting changes on the GNOME front as we prepare to cap off the year...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Three Sexy AI API Keys to Access More Models than Hugh Hefner","url":"https://hackernoon.com/three-sexy-ai-api-keys-to-access-more-models-than-hugh-hefner?source=rss","date":1734778816,"author":"Paul from DentroAI","unread":true,"desc":"","content":"<p>\\</p>\n<h2 id=\"simplifyingaidevelopment\">Simplifying AI Development</h2>\n<p>Are you tired of juggling multiple AI API keys across different AI platforms? What if I told you that you only need <strong>three API keys</strong> to access most of the AI capabilities you’ll ever need? Let me show you how to streamline your AI development process.</p>\n<p>\\</p>\n<h2 id=\"theapikeychallenge\">The API Key Challenge</h2>\n<p>Every developer’s journey into AI typically starts the same way: signing up for an OpenAI account. Getting that first AI API key. Building an initial prototype.</p>\n<p>\\n You expand your horizons and experiment with different providers. New account, new api key, new billing information.</p>\n<p>\\n After 2 months, you got an account on 10 different provider. Separate accounts, different billing systems, and various AI API keys <a href=\"https://dentroai.com/ai-in-consulting-6-application-areas/\">make organization a nightmare</a>.</p>\n<p>\\n Let’s end this! Here are 3 AI service providers, that give you the more models than Hugh Hefner for your AI API key.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/null-du022q2.png\" alt=\"This will be your face after signing up on OpenRouter and getting your AI API key \" /></p>\n<p>\\\n\\</p>\n<h2 id=\"thethreekeystofreedom\">The Three Keys to Freedom</h2>\n<p>\\</p>\n<h2 id=\"1openrouteronekeyforalllanguagemodels\">1. OpenRouter: One Key for All Language Models</h2>\n<p>OpenRouter is your gateway to over 200 different language models.  Think of it as a universal aggregator for AI models.</p>\n<p>\\\nWith just <strong>one API key</strong>, you get access to:</p>\n<ul>\n<li>Popular closed-source models (OpenAI, Google Gemini, Anthropic)</li>\n<li>Open-source models (Llama, Qwen)</li>\n<li>The latest models as they’re released</li>\n</ul>\n<p>\\\nOpenRouter charges you like most other LLM providers: per token.</p>\n<p>Top up the account with as little as 5$, which will bring you very far for  prototyping.</p>\n<p>\\\n<strong>Pro Tip</strong>: OpenRouter even allows you to pay in crypto if that’s your thing!</p>\n<p>\\\n\\\n\\</p>\n<h2 id=\"2replicateyourswissarmyknifeforaimodels\">2. Replicate: Your Swiss Army Knife for AI Models</h2>\n<p>Replicate takes a different approach by offering a platform where:</p>\n<p>\\</p>\n<ul>\n<li><p>Access a wide range of GenAI models (not only LLMs)</p></li>\n<li><p>Use simple endpoints for inference</p></li>\n<li><p>Deploy your own models seamlessly</p></li>\n<li><p>Popular models hosted on Replicate include:</p></li>\n<li><p>Image generation series “<a href=\"https://replicate.com/black-forest-labs/flux-1.1-pro\">Flux” by German Black Forest Labs</a></p></li>\n<li><p>The (imo best) image generation model <a href=\"https://replicate.com/ideogram-ai/ideogram-v2\">ideogram-v2</a></p></li>\n<li><p>Vision LLM <a href=\"https://replicate.com/yorickvp/llava-v1.6-mistral-7b\">llava-mistral</a></p>\n<p>\\\n\\</p></li>\n</ul>\n<p>While OpenRouter charges per token, Replicate charges per second of  GPU runtime.</p>\n<p>This is a bit tricky, but fair.</p>\n<p>Stronger GPUs cost more per second, but need less time running an AI model.</p>\n<p>Weaker GPUs are  cheaper, but need more time.</p>\n<p>\\\n\\</p>\n<h2 id=\"3huggingfacethegithubofmachinelearning\">3. Hugging Face: The GitHub of Machine Learning</h2>\n<p>The final piece of the puzzle is Hugging Face, which offers:</p>\n<ul>\n<li>Access to extensive datasets</li>\n<li>Model weights</li>\n<li>GitHub-style discussions</li>\n<li>A community-driven approach to machine learning</li>\n</ul>\n<p>\\\nOpenRouter and Replicate focus on AI model <strong>inference</strong>.</p>\n<p>HuggingFace focuses on AI model <strong>development</strong>.</p>\n<p>So the first two platforms are for people that want to build on top of models.</p>\n<p>HuggingFace is more for Machine Learning devs that want to adapt  the actual models.</p>\n<p>\\\n\\</p>\n<h2 id=\"realworldapplication\">Real-World Application</h2>\n<p>Here’s a real scenario: Imagine discovering a new language model you want to test.</p>\n<p>\\\nBefore you had to go through the hassle of creating new accounts and managing another AI API key.</p>\n<p>\\\nNow you can simply use your OpenRouter API key and start experimenting immediately.</p>\n<p>\\\nNo new accounts, no waiting for approvals from your boss, just pure innovation <em>😇.</em></p>\n<p>\\\n\\\n <img src=\"https://cdn.hackernoon.com/images/null-8b2223m.png\" alt=\"All images in this post were generated with Ideogram-v2 via Replicate!\" /></p>\n<p>\\</p>\n<h2 id=\"gettingstarted\">Getting Started</h2>\n<p>Ready to simplify your AI development? Here’s how:</p>\n<p>\\</p>\n<ol>\n<li>Visit <a href=\"https://openrouter.ai/\">openrouter.ai</a> to get your first unified LLM API key</li>\n<li>Head to <a href=\"https://replicate.com\">replicate.com</a> for your GenAI needs</li>\n<li>Create an account on <a href=\"https://huggingface.co/\">huggingface.co</a> for access to the ML community</li>\n</ol>\n<p>\\\n\\</p>\n<h2 id=\"whythismatters\">Why This Matters</h2>\n<p>By consolidating your AI development around these three key platforms, you can:</p>\n<ul>\n<li><strong>Reduce administrative overhead</strong></li>\n<li><strong>Speed up experimentation</strong></li>\n<li><strong>Simplify billing and accounting</strong></li>\n<li><strong>Stay agile in the fast-moving AI landscape</strong></li>\n</ul>\n<p>\\\nRemember, <a href=\"https://dentroai.com/implement-ai-in-a-company/\">in the world of AI development</a>, simplicity is key.</p>\n<p>\\\nThese three AI API keys give you the power to focus on what matters most: building amazing AI applications.</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"KDE Plasma 6.3 Improving Game Controller Joystick Support, Many Fixes","url":"https://www.phoronix.com/news/Plasma-6.3-Better-Joystick","date":1734778734,"author":"Michael Larabel","unread":true,"desc":"","content":"While the Christmas holidays are quickly approach, KDE developers remain busy working on new features for the upcoming Plasma 6.3 desktop as well as continuing to land many bug fixes...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Startup Set To Brick $800 Kids Robot Is Trying To Open Source It First","url":"https://hardware.slashdot.org/story/24/12/21/0039217/startup-set-to-brick-800-kids-robot-is-trying-to-open-source-it-first?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1734775200,"author":"BeauHD","unread":true,"desc":"","content":"Last week, startup Embodied announced it was closing down, and its product, an $800 robot for kids ages 5 to 10, would soon be bricked. Now, in a blog post published on Friday, CEO Paolo Pirjanian shared that Embodied's technical team is working on a way to open-source the robot, ensuring it can continue operating indefinitely. Ars Technica reports: The notice says that after releasing OpenMoxie, Embodied plans to release \"all necessary code and documentation\" for developers and users. Pirjanian said that an over-the-air (OTA) update is now available for download that will allow previously purchased Moxies to support OpenMoxie. The executive noted that Embodied is still \"seeking long-term answers\" but claimed that the update is a \"vital first step\" to \"keep the door open\" for the robot's continued functionality.\n \nAt this time, OpenMoxie isn't available and doesn't have a release date. Embodied's wording also seems careful to leave an opening for OpenMoxie to not actually release; although, the company seems optimistic. However, there's also a risk of users failing to update their robots in time and properly. Embodied noted that it won't be able to support users who have trouble with the update or with OpenMoxie post-release. Updating the robot includes connecting to Wi-Fi and leaving it on for at least an hour. \"It is extremely important that you update your Moxie with this OTA as soon as possible because once the cloud servers stop working you will not be able to update your robot,\" the document reads. Embodied hasn't said when exactly its cloud servers still stop working.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Startup+Set+To+Brick+%24800+Kids+Robot+Is+Trying+To+Open+Source+It+First%3A+https%3A%2F%2Fhardware.slashdot.org%2Fstory%2F24%2F12%2F21%2F0039217%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fhardware.slashdot.org%2Fstory%2F24%2F12%2F21%2F0039217%2Fstartup-set-to-brick-800-kids-robot-is-trying-to-open-source-it-first%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://hardware.slashdot.org/story/24/12/21/0039217/startup-set-to-brick-800-kids-robot-is-trying-to-open-source-it-first?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23557581&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The TechBeat: Why Does ETH 3.0 Need Lumoz's ZK Computing Network? (12/21/2024)","url":"https://hackernoon.com/12-21-2024-techbeat?source=rss","date":1734765050,"author":"Techbeat","unread":true,"desc":"","content":"<p>How are you, hacker? \n 🪐<strong>Want to know what's trending right now?:</strong>\n <a href=\"https://hackernoon.com/homepage-has-a-new-baby\">The Techbeat by HackerNoon </a> has got you covered with fresh content from our trending stories of the day! Set email preference <a href=\"https://app.hackernoon.com/profile/email-settings\">here</a>.\n ## <strong><a href=\"https://hackernoon.com/why-does-eth-30-need-lumozs-zk-computing-network\">Why Does ETH 3.0 Need Lumoz's ZK Computing Network? </a></strong> <img src=\"https://cdn.hackernoon.com/images/Wls6TtjOLGMbl8aKwQlIbcyfjQF2-sl33y13.jpeg\" alt=\"\" />\n By <a href=\"https://hackernoon.com/u/lumoz\">@lumoz</a> [ 5 Min read ] \n Find out why ETH 3.0 needs Lumoz's Computing Network. <a href=\"https://hackernoon.com/why-does-eth-30-need-lumozs-zk-computing-network\">Read More.</a></p>\n<h2 id=\"strengtheningcybersecuritybreakingdownindrivesbugbountyprogramhttpshackernooncomstrengtheningcybersecuritybreakingdownindrivesbugbountyprogramhttpscdnhackernooncomimages6wbo18yskkbatyblecagy9tmyol2k103dukpng\"><strong><a href=\"https://hackernoon.com/strengthening-cybersecurity-breaking-down-indrives-bug-bounty-program\">Strengthening Cybersecurity: Breaking Down inDrive’s Bug Bounty Program</a></strong> <img src=\"https://cdn.hackernoon.com/images/6WbO18ySKKbatYbleCAgy9Tmyol2-k103duk.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/indrivetech\">@indrivetech</a> [ 6 Min read ] \n Learn how inDrive's bug bounty program strengthens cybersecurity by collaborating with white hat hackers to detect vulnerabilities and optimize security process <a href=\"https://hackernoon.com/strengthening-cybersecurity-breaking-down-indrives-bug-bounty-program\">Read More.</a></p>\n<h2 id=\"mailbirdexpandstomacmanageallyourinboxesandfavoriteappsinoneplacehttpshackernooncommailbirdexpandstomacmanageallyourinboxesandfavoriteappsinoneplacehttpscdnhackernooncomimageshq098u52dzpm2y4uitqcqxtlrak2g40300spng\"><strong><a href=\"https://hackernoon.com/mailbird-expands-to-mac-manage-all-your-inboxes-and-favorite-apps-in-one-place\">Mailbird Expands to Mac: Manage All Your Inboxes and Favorite Apps in One Place</a></strong> <img src=\"https://cdn.hackernoon.com/images/hQ098u52DzPm2Y4UITQcQXtLRAk2-g40300s.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/pressreleases\">@pressreleases</a> [ 2 Min read ] \n Mailbird, the email client trusted by millions of Windows users worldwide, is now available for Mac. <a href=\"https://hackernoon.com/mailbird-expands-to-mac-manage-all-your-inboxes-and-favorite-apps-in-one-place\">Read More.</a></p>\n<h2 id=\"lumozprotocolmainnetlaunchmoztokensandnodeclaimsnowopenhttpshackernooncomlumozprotocolmainnetlaunchmoztokensandnodeclaimsnowopenhttpscdnhackernooncomimageswls6ttjolgmbl8akwqlibcyfjqf21143y7epng\"><strong><a href=\"https://hackernoon.com/lumoz-protocol-mainnet-launch-moz-tokens-and-node-claims-now-open\">Lumoz Protocol Mainnet Launch: MOZ Tokens and Node Claims Now Open!</a></strong> <img src=\"https://cdn.hackernoon.com/images/Wls6TtjOLGMbl8aKwQlIbcyfjQF2-1143y7e.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/lumoz\">@lumoz</a> [ 4 Min read ] \n Today, we are proud to announce the official launch of the Lumoz Protocol mainnet!  <a href=\"https://hackernoon.com/lumoz-protocol-mainnet-launch-moz-tokens-and-node-claims-now-open\">Read More.</a></p>\n<h2 id=\"joinlumozzkverifiernodeminingandshare25billionmozrewardshttpshackernooncomjoinlumozzkverifiernodeminingandshare25billionmozrewardshttpscdnhackernooncomimageswls6ttjolgmbl8akwqlibcyfjqf2cx03ymqpng\"><strong><a href=\"https://hackernoon.com/join-lumoz-zkverifier-node-mining-and-share-25-billion-moz-rewards\">Join Lumoz zkVerifier Node Mining and Share 2.5 Billion MOZ Rewards</a></strong> <img src=\"https://cdn.hackernoon.com/images/Wls6TtjOLGMbl8aKwQlIbcyfjQF2-cx03ymq.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/lumoz\">@lumoz</a> [ 4 Min read ] \n Lumoz Node Network &amp; MOZ staking are live! Join the zkVerifier network, stake MOZ, or run nodes to share 25% of $90M in rewards. Act now! <a href=\"https://hackernoon.com/join-lumoz-zkverifier-node-mining-and-share-25-billion-moz-rewards\">Read More.</a></p>\n<h2 id=\"quicklybulkloadimagetoecommercesiteswiththisguidehttpshackernooncomquicklybulkloadimagetoecommercesiteswiththisguidehttpscdnhackernooncomimages0sm1kikmbzhu9ad2gar4ckywon439l035i7png\"><strong><a href=\"https://hackernoon.com/quickly-bulk-load-image-to-e-commerce-sites-with-this-guide\">Quickly Bulk Load Image to E-commerce Sites With This Guide</a></strong> <img src=\"https://cdn.hackernoon.com/images/0sm1KIKmbZhu9AD2GAr4cKywoN43-9l035i7.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/filestack\">@filestack</a> [ 10 Min read ] \n Manually updating each image one by one is not only time-consuming but also inefficient. This is where bulk quick image upload is helpful. <a href=\"https://hackernoon.com/quickly-bulk-load-image-to-e-commerce-sites-with-this-guide\">Read More.</a></p>\n<h2 id=\"rootstockcollectiveindepthempoweringbitcoinbuildershttpshackernooncomrootstockcollectiveindepthempoweringbitcoinbuildershttpscdnhackernooncomimagesinxbrjris6m1kdhuwcynhiiurxm1j5034qipng\"><strong><a href=\"https://hackernoon.com/rootstockcollective-in-depth-empowering-bitcoin-builders\">RootstockCollective In-Depth: Empowering Bitcoin Builders</a></strong> <img src=\"https://cdn.hackernoon.com/images/InxBRjRIs6M1kdhuWcyNHiiUrxm1-j5034qi.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/rootstock_io\">@rootstock_io</a> [ 7 Min read ] \n Empowering Bitcoin builders with RootstockCollective DAO: Rewarding innovation, stakers, and developers in the Bitcoin sidechain ecosystem. <a href=\"https://hackernoon.com/rootstockcollective-in-depth-empowering-bitcoin-builders\">Read More.</a></p>\n<h2 id=\"clapperexpandstomexicobettingbigonauthenticityandcreatorempowermenthttpshackernooncomclapperexpandstomexicobettingbigonauthenticityandcreatorempowermenthttpscdnhackernooncomimagesgconmczfrmejse031vcfdowm11x2iq035axwebp\"><strong><a href=\"https://hackernoon.com/clapper-expands-to-mexico-betting-big-on-authenticity-and-creator-empowerment\">Clapper Expands to Mexico, Betting Big on Authenticity and Creator Empowerment</a></strong> <img src=\"https://cdn.hackernoon.com/images/gcoNMczFrmejSE031VcfDOwm11X2-iq035ax.webp\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/clapper\">@clapper</a> [ 3 Min read ] \n Positioned as a sanctuary for authentic self-expression and creativity, Clapper’s foray into the Mexican market highlights the platform's growing clout. <a href=\"https://hackernoon.com/clapper-expands-to-mexico-betting-big-on-authenticity-and-creator-empowerment\">Read More.</a></p>\n<h2 id=\"eip7503zeroknowledgewormholesforprivateethereumtransactionshttpshackernooncomeip7503zeroknowledgewormholesforprivateethereumtransactionshttpscdnhackernooncomimageswsqjfcsxoxwphtnq7snevvhdwgu1jm035v1jpeg\"><strong><a href=\"https://hackernoon.com/eip-7503-zero-knowledge-wormholes-for-private-ethereum-transactions\">EIP-7503: Zero-knowledge Wormholes for Private Ethereum Transactions</a></strong> <img src=\"https://cdn.hackernoon.com/images/WSQJfCSXOxWphTNQ7sneVvhdWGu1-jm035v1.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/2077research\">@2077research</a> [ 44 Min read ] \n EIP-7503 is a solution for making on-chain transfers on Ethereum private. Learn how EIP-7503 works and what it means for users, businesses, and the network.  <a href=\"https://hackernoon.com/eip-7503-zero-knowledge-wormholes-for-private-ethereum-transactions\">Read More.</a></p>\n<h2 id=\"goodbyepasswordshellopasskeysthefutureofauthenticationhttpshackernooncomgoodbyepasswordshellopasskeysthefutureofauthenticationhttpscdnhackernooncomimagessecuritysymbolonalaptopscreene9ol03dr3i89katj0uwpomzipng\"><strong><a href=\"https://hackernoon.com/goodbye-passwords-hello-passkeys-the-future-of-authentication\">Goodbye Passwords, Hello Passkeys: The Future of Authentication</a></strong> <img src=\"https://cdn.hackernoon.com/images/security-symbol-on-a-laptop-screen-e9ol03dr3i89katj0uwpomzi.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/radioactive\">@radioactive</a> [ 6 Min read ] \n Discover how passkeys revolutionize online authentication.  <a href=\"https://hackernoon.com/goodbye-passwords-hello-passkeys-the-future-of-authentication\">Read More.</a></p>\n<h2 id=\"depinonethereumredefiningcoordinationsystemshttpshackernooncomdepinonethereumredefiningcoordinationsystemshttpscdnhackernooncomimageswsqjfcsxoxwphtnq7snevvhdwgu1eg036olpng\"><strong><a href=\"https://hackernoon.com/depin-on-ethereum-redefining-coordination-systems\">DePIN On Ethereum: Redefining Coordination Systems</a></strong> <img src=\"https://cdn.hackernoon.com/images/WSQJfCSXOxWphTNQ7sneVvhdWGu1-eg036ol.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/2077research\">@2077research</a> [ 26 Min read ] \n DePIN brings Ethereum's global coordination to the physical world. Learn how Ethereum DePIN networks solve problems across energy, telecom, compute, and more. <a href=\"https://hackernoon.com/depin-on-ethereum-redefining-coordination-systems\">Read More.</a></p>\n<h2 id=\"harnessingsharedsecurityforsecurecrosschaininteroperabilityhttpshackernooncomharnessingsharedsecurityforsecurecrosschaininteroperabilityhttpscdnhackernooncomimageswsqjfcsxoxwphtnq7snevvhdwgu1l903680jpeg\"><strong><a href=\"https://hackernoon.com/harnessing-shared-security-for-secure-cross-chain-interoperability\">Harnessing Shared Security For Secure Cross-Chain Interoperability</a></strong> <img src=\"https://cdn.hackernoon.com/images/WSQJfCSXOxWphTNQ7sneVvhdWGu1-l903680.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/2077research\">@2077research</a> [ 47 Min read ] \n A deep dive on shared security and the role of shared security infrastructure in building robust and secure cross-chain interoperability solutions for users. <a href=\"https://hackernoon.com/harnessing-shared-security-for-secure-cross-chain-interoperability\">Read More.</a></p>\n<h2 id=\"flarenetworklaunchesxrptokenintegrationontestnetworkhttpshackernooncomflarenetworklaunchesxrptokenintegrationontestnetworkhttpscdnhackernooncomimages7remniehnfobfzztumqeroziggh39b036ogpng\"><strong><a href=\"https://hackernoon.com/flare-network-launches-xrp-token-integration-on-test-network\">Flare Network Launches XRP Token Integration on Test Network</a></strong> <img src=\"https://cdn.hackernoon.com/images/7rEmNIeHNFOBfZZtUMQerOZIGGH3-9b036og.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/ishanpandey\">@ishanpandey</a> [ 2 Min read ] \n FXRP, a tokenized version of XRP, has been deployed on the blockchain platform Flare's Songbird test network.  <a href=\"https://hackernoon.com/flare-network-launches-xrp-token-integration-on-test-network\">Read More.</a></p>\n<h2 id=\"understandingthetwitterapisoyoucandesignyourownhttpshackernooncomunderstandingthetwitterapisoyoucandesignyourownhttpscdnhackernooncomimageshkbztu6ttdtc6vjv74oonoegick1co034h7png\"><strong><a href=\"https://hackernoon.com/understanding-the-twitter-api-so-you-can-design-your-own\">Understanding the Twitter API So You Can Design Your Own</a></strong> <img src=\"https://cdn.hackernoon.com/images/hkBZtu6tTdTc6vjV74oOnoegICk1-co034h7.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/trekhleb\">@trekhleb</a> [ 22 Min read ] \n In this article, we explore how the&nbsp;X&nbsp;(Twitter) home timeline (x.com/home) API is designed and what approaches they use to solve multiple challenges. <a href=\"https://hackernoon.com/understanding-the-twitter-api-so-you-can-design-your-own\">Read More.</a></p>\n<h2 id=\"teachingkidsaboutbitcoincanensureitslegacygoesonhttpshackernooncomteachingkidsaboutbitcoincanensureitslegacygoesonhttpscdnhackernooncomimagesehuyzrllaubgargtd43keixtots2zn03673webp\"><strong><a href=\"https://hackernoon.com/teaching-kids-about-bitcoin-can-ensure-its-legacy-goes-on\">Teaching Kids About Bitcoin Can Ensure Its Legacy Goes On</a></strong> <img src=\"https://cdn.hackernoon.com/images/EHUYZRLLAUbgArGtd43kEIxTOTS2-zn03673.webp\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/maken8\">@maken8</a> [ 5 Min read ] \n Bitcoin is the biggest mathematical and technological invention of this century.\nAnd the best way to carry it forward is to teach its foundations to our kids. <a href=\"https://hackernoon.com/teaching-kids-about-bitcoin-can-ensure-its-legacy-goes-on\">Read More.</a></p>\n<h2 id=\"buildanaichatbotfasterthanyourmorningcoffeerunwiththisguidehttpshackernooncombuildanaichatbotfasterthanyourmorningcoffeerunwiththisguidehttpscdnhackernooncomimagesm6oviadfe8xxyn2x7hnbcvmdhak2kl133iijpeg\"><strong><a href=\"https://hackernoon.com/build-an-ai-chatbot-faster-than-your-morning-coffee-run-with-this-guide\">Build an AI Chatbot Faster Than Your Morning Coffee Run With This Guide</a></strong> <img src=\"https://cdn.hackernoon.com/images/m6OvIAdfE8XXYN2x7hNbCvMdhAk2-kl133ii.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/ivmarcos\">@ivmarcos</a> [ 7 Min read ] \n Learn how to build an AI-powered chatbot using React, Vite, and Cohere's API.  <a href=\"https://hackernoon.com/build-an-ai-chatbot-faster-than-your-morning-coffee-run-with-this-guide\">Read More.</a></p>\n<h2 id=\"thedeadinternettheorythedarksideofaiautomationhttpshackernooncomthedeadinternettheorythedarksideofaiautomationhttpscdnhackernooncomimagesytwigsgvncbxvmxegu57knhgflx2l903085png\"><strong><a href=\"https://hackernoon.com/the-dead-internet-theory-the-dark-side-of-ai-automation\">The Dead Internet Theory: The Dark Side of AI Automation</a></strong> <img src=\"https://cdn.hackernoon.com/images/YTwigsgvNcbXVMXEGu57knhGFlX2-l903085.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/antonvoichenkovokrug\">@antonvoichenkovokrug</a> [ 6 Min read ] \n  <a href=\"https://hackernoon.com/the-dead-internet-theory-the-dark-side-of-ai-automation\">Read More.</a></p>\n<h2 id=\"ibuiltanichesiteandrankeditongoogleinashorttimehereshowhttpshackernooncomibuiltanichesiteandrankeditongoogleinashorttimehereshowhttpscdnhackernooncomimages2jqchkrv03exbugklrdzibfm99q24e02swvjpeg\"><strong><a href=\"https://hackernoon.com/i-built-a-niche-site-and-ranked-it-on-google-in-a-short-time-heres-how\">I Built a Niche Site and Ranked it On Google in a Short Time — Here's How</a></strong> <img src=\"https://cdn.hackernoon.com/images/2jqChkrv03exBUgkLrDzIbfM99q2-4e02swv.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/amraandelma\">@amraandelma</a> [ 4 Min read ] \n After five years in the SEO world, I’ve learned one thing: the true measure of an SEO \"expert\" is whether they can rank their own site.  <a href=\"https://hackernoon.com/i-built-a-niche-site-and-ranked-it-on-google-in-a-short-time-heres-how\">Read More.</a></p>\n<h2 id=\"metaversegrowthbringsnewdataprotectionheadacheshttpshackernooncommetaversegrowthbringsnewdataprotectionheadacheshttpscdnhackernooncomimagesydksaogusrdlbtmutjjrekoeg9z2rg036kejpeg\"><strong><a href=\"https://hackernoon.com/metaverse-growth-brings-new-data-protection-headaches\">Metaverse Growth Brings New Data Protection Headaches</a></strong> <img src=\"https://cdn.hackernoon.com/images/YdksaOgUsRdlbtMutjJRekoeG9z2-rg036ke.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/lihisegev\">@lihisegev</a> [ 4 Min read ] \n A world of possibilities is coming, but it’s coupled with the biggest security risks of our time. How can we protect ourselves in a data-miner’s paradise? <a href=\"https://hackernoon.com/metaverse-growth-brings-new-data-protection-headaches\">Read More.</a></p>\n<h2 id=\"democratizingaihowionetsctoisbuildingtheairbnbofgpushttpshackernooncomdemocratizingaihowionetsctoisbuildingtheairbnbofgpushttpscdnhackernooncomimages7remniehnfobfzztumqeroziggh3an0364qjpeg\"><strong><a href=\"https://hackernoon.com/democratizing-ai-how-ionets-cto-is-building-the-airbnb-of-gpus\">Democratizing AI: How IO.NET's CTO is Building the 'Airbnb of GPUs'\"</a></strong> <img src=\"https://cdn.hackernoon.com/images/7rEmNIeHNFOBfZZtUMQerOZIGGH3-an0364q.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/ishanpandey\">@ishanpandey</a> [ 6 Min read ] \n IO.NET is building a platform that could democratize access to AI computing resources while reducing costs by up to 75% compared to traditional providers. <a href=\"https://hackernoon.com/democratizing-ai-how-ionets-cto-is-building-the-airbnb-of-gpus\">Read More.</a> \n 🧑‍💻 What happened in your world this week? It's been said that <a href=\"https://hackernoon.com/developers-the-why-and-how-to-writing-technical-articles-54e824789ef6\">writing can help consolidate technical knowledge</a>, <a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\">establish credibility</a>,<a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\"> and contribute to emerging community standards</a>. Feeling stuck? We got you covered ⬇️⬇️⬇️\n <a href=\"https://app.hackernoon.com/mobile/lZx3fmlPdlPJpVBIdble\">ANSWER THESE GREATEST INTERVIEW QUESTIONS OF ALL TIME</a>\n We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.\n See you on Planet Internet! With love, \n The HackerNoon Team ✌️\n <img src=\"https://cdn.hackernoon.com/images/ezgif.com-gif-maker%20(44).gif\" alt=\"\" /></p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Next Holiday Season, Ignore Everyone Except One Customer","url":"https://hackernoon.com/next-holiday-season-ignore-everyone-except-one-customer?source=rss","date":1734764412,"author":"susie liu","unread":true,"desc":"","content":"<p><strong>Holiday marketing, as it’s preached, is a con.</strong> A ritualized scam sold to startups by ad platforms, agencies, and marketers who profit from your desperation. Do you really think you can outspend Amazon, out-charm Coca-Cola, or outmaneuver Nike? Unless you’re secretly printing money in your garage, no.</p>\n<p>\\\nChristmas isn’t for playing Santa with jingles and 20% off codes—it’s for hunting a whale. <strong>Next year, turn your entire marketing strategy into a public obsession with one person.</strong> Not a “demographic market.” Not a target persona. A single, breathing, flesh-and-blood human being whose “yes”could grant you permanent access to Cuban’s Cabo cocktail club.</p>\n<p>\\\nBet it all on a campaign <strong><em>so precise it’s borderline psychotic</em></strong>. Dangerous? Stupid? Obviously, and probably. <strong>But boring? Never.</strong></p>\n<p>\\</p>\n<blockquote>\n  <p>If it crashes, people will talk. If it flies, you’ll own the year.</p>\n  <p>\\\n  Either is better than begging the Internet for clicks, plus it’ll give you a great story (or a great excuse to drink).</p>\n</blockquote>\n<h2 id=\"holidaymarketingsucks\"><strong>Holiday Marketing Sucks.</strong></h2>\n<p>This is a classic David vs. Goliath scenario—<strong><em>except Goliath has a machine gun and David forgot his slingshot</em></strong>. Here’s why it’s a losing battle:</p>\n<h3 id=\"youreoutgunnedandoutclassed\"><strong>You’re Outgunned And Outclassed</strong></h3>\n<p>Every big brand with deep pockets is jacking up CPMs. Ad costs more than double, competition spikes, and algorithms prioritize whoever spends the most. Yeah, obviously not you. <em>If your $10K campaign could talk, it would whisper: \"Help.\"</em></p>\n<h3 id=\"youresellingtozombies\"><strong>You’re Selling To Zombies</strong></h3>\n<p>By mid-December, your CTA is competing with drunken relatives, shipping delays, and 87 ads about the perfect gift for canine therapists. Your audience is fried and their brains are coasting on autopilot—barely capable of choosing between two brands of wrapping paper.</p>\n<h3 id=\"yourecompetingwithsantaclausnototherbusinesses\"><strong>You’re Competing With Santa Claus—Not Other Businesses</strong></h3>\n<p>Know why Apple kills it every Christmas? It’s not with Siri, but with <em>sentiment</em>. The holiday giants use their war chests to craft masterpieces <strong><em>that ride on the emotional meaning of the season</em></strong>—nostalgia, tradition, family values—and if you’re not on par, you come off like a door-to-door salesman in a Saint Nick suit.</p>\n<h3 id=\"roiregretoninvestment\"><strong>ROI = Regret On Investment</strong></h3>\n<p>December sales bring January sobs. <strong><em>Whatever you make from your festive push will be eaten alive by your CPA</em></strong>—and you’ll wake up next month drowning in remorse, invoices, and wondering where the cash went. (Hint: Zuck’s back pocket)</p>\n<h2 id=\"whyserenadingonepersonisthesmartestplay\"><strong>Why Serenading One Person Is The Smartest Play</strong></h2>\n<p>The best presents don’t come in mass-produced packages. Neither should your holiday marketing. <strong><em>A custom campaign is as much a gift to them as it is to you.</em></strong></p>\n<h3 id=\"itspsychologicalwarfare\"><strong>It’s Psychological Warfare</strong></h3>\n<p>People hate being sold to, but they love feeling chosen. Focusing on one person creates a psychological bind:</p>\n<p>\\</p>\n<ul>\n<li>If they respond, they know they’re winning because you’ve done all the work to court them.</li>\n<li>If they don’t respond, they wonder if someone else will—and if they’ll miss their shot.</li>\n</ul>\n<h3 id=\"itstheultimatelitmustestforyourproduct\"><strong>It’s The Ultimate Litmus Test For Your Product</strong></h3>\n<p>If you can’t convince one perfect-fit customer, how will you convince hundreds? A campaign por uno is the fastest way to figure out if your pitch, product, and strategy are sharp enough to scale.</p>\n<h3 id=\"itsthebestmarketingbootcamp\"><strong>It’s The Best Marketing Boot Camp</strong></h3>\n<p>A one-person campaign is combat training. No crutches of generic strategies, no Canva, no “but our CTR was decent” excuses—survival here relies on raw ingenuity, superhuman agility, and uncompromising precision. <strong>Success isn’t guaranteed, but clarity is.</strong> The hard lessons will stick, and your team becomes immune to mediocrity. After this, they’ll be more than competent: they’ll be dangerous.</p>\n<h3 id=\"itturnsfailureintosuccess\"><strong>It Turns Failure Into Success</strong></h3>\n<p>Even if it “fails,” it doesn’t. People appreciate balls. The audacity of your move becomes its own currency, generating buzz, controversy, and respect—<strong><em>all of which could bag you a new client, partner, or investor.</em></strong> In a one-person campaign, failure is just another kind of success.</p>\n<h2 id=\"howtoexecuteacampaignthatbordersonmaniacal\"><strong>How To Execute A Campaign That Borders On Maniacal</strong></h2>\n<h3 id=\"step1pickyourwhale\"><strong>Step 1: Pick Your Whale</strong></h3>\n<ul>\n<li><p><strong>A Dream Customer:</strong> E.g., the enterprise client whose logo would give your sales team goosebumps.</p></li>\n<li><p><strong>A Key Partner:</strong> E.g., the CMO that could double your reach if they said yes.</p></li>\n<li><p><strong>An Influential Investor:</strong> Not just for the check, but for the name on the cap table.</p>\n<p>\\</p></li>\n</ul>\n<p>Be ambitious, but realistic. Elon’s too busy running Government Efficiency. Aim for the guys two rungs below him.</p>\n<h3 id=\"step2stalkyourprey\"><strong>Step 2: Stalk Your Prey</strong></h3>\n<p>Just because you’re targeting one person <strong><em>doesn’t mean you get to skip the user insight phase</em></strong>. Oh no, this research matters even more when you’re professing love, and it doesn’t stop at their LinkedIn profile. Do they like having their ego massaged? Have a fetish for cryptic puzzles? Are they nosing around for gaming collaborations?</p>\n<p>\\\nStalk (ethically), snoop (strategically), and build a dossier so good it’d make the FBI sweat. <em>Just remember there’s a fine line between research and “why is there a black van outside my house?”</em></p>\n<h3 id=\"step3unleashcontrolledmadness\"><strong>Step 3: Unleash Controlled Madness.</strong></h3>\n<p>Now that you understand your whale’s appetite, cook up the bait. Go brazen, or don’t bother. <strong>But your bravado and performance art means nothing without an unapologetic CTA and irresistible pitch.</strong> That ask needs to be <em>so direct it’s borderline confrontational</em>, and that pitch better feel like a bespoke Italian suit.</p>\n<p>\\\n<strong>Here are some thought-starters:</strong></p>\n<p>\\\n<strong>Sponsor Their Daily Coffee:</strong> Find out where they get their morning coffee and prepay for a week of their orders. Include a personalized note on every receipt that says: “<em>Fuel for your next great decision—working with us.</em>”</p>\n<p>\\\n<strong>Ship A Literal Door:</strong> Send them a door (yes, an actual door) with a key attached and a note: “<em>This is the key to unlocking something extraordinary. Let’s talk.</em>”</p>\n<p>\\\n<strong>Buy An Editorial:</strong> Call up some prestigious publication they subscribe to and splurge on a paid piece. Write an open letter addressed to them, full of specifics about why they should engage with you. Be cheeky: “<em>Hey [Their Name], Playing Small Looks Weird on You</em>.”</p>\n<h3 id=\"step4makesuretheworldknowsyourecrazy\"><strong>Step 4: Make Sure The World Knows You’re Crazy.</strong></h3>\n<p>Exclusivity is no fun if no one’s watching. Turn your serenade into a spectacle.</p>\n<p>\\\n<strong>More thought-starters:</strong></p>\n<p>\\\n<strong>Build a Public Countdown:</strong> Launch a countdown clock on social media, with posts teasing, “<em>6 Days Until [Target’s Name] Changes Everything.</em>” Make sure they’re mentioned, their employees are tagged, their peers are targeted.</p>\n<p>\\\n<strong>Go Guerrilla (Literally)</strong>: Hire street performers or actors to “protest” outside their HQ, carrying signs with slogans like, “<em>[Target’s Name], Take the Meeting!</em>”. Extra karma if it’s themed to their industry or product.</p>\n<p>\\\n<strong>Rent Their Attention:</strong> Buy ad space outside their office. A billboard. Graffiti a building (legally). Be loud to the point you start the water cooler gossip: “<em>Dear [Name], If You Don’t Call Us, We’ll Have to Buy More Billboards. And That’s Awkward for Everyone.</em>”</p>\n<h2 id=\"finalthoughtsmaybeitsnotyoumaybeitsjustdumb\"><strong>Final Thoughts: Maybe It’s Not You—Maybe It’s Just Dumb</strong></h2>\n<p>\\</p>\n<blockquote>\n  <p>Sometimes, you’re not bad at the game—the game is (likely) a waste of time.</p>\n  <p>\\\n  Holiday marketing for startups? That’s the house always winning, with you just the festive confetti.</p>\n</blockquote>\n<p>\\\nStop groveling for attention in a system that turns startups into cannon fodder for ad giants. That’s not persistence—<strong><em>it’s herd mentality dressed up as hustle</em></strong>. Do something irrationally ambitious. <strong>Go unhinged, go nuclear</strong>. <strong>Not because it’s guaranteed success, but because it’s not preordained defeat.</strong> At least you’ll have had some fun, and failed in style—<em>on your own terms, not theirs</em>.</p>\n<h3 id=\"itschristmasusetheseasonforwhatitsmeantforwalkingawayfrombadideas\"><strong>It’s Christmas. Use the season for what it’s meant for—walking away from bad ideas.</strong></h3>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Axiom's Private Space Station Could Arrive As Early As 2028","url":"https://science.slashdot.org/story/24/12/21/0048247/axioms-private-space-station-could-arrive-as-early-as-2028?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1734764400,"author":"BeauHD","unread":true,"desc":"","content":"Axiom Space has revised its plan for assembling its commercial space station by launching the Payload, Power, and Thermal module first, enabling it to operate as a free-flying platform as early as 2028 -- two years ahead of the original timeline. Space.com reports: NASA awarded Axiom Space a contract in 2020 to attach one or more modules to the International Space Station (ISS), which is set to retire by 2030 at the earliest. The original plan called for Axiom to detach a multi-module group from the ISS, creating a commercial outpost in low Earth orbit that will continue operating after the ISS is gone. But that plan has now been altered.\n \nTo create its space station, Axiom plans to launch five modules: a payload/power/thermal element, an airlock, a research/manufacturing hub, and a pair of habitat modules. The original plan was for Axiom to launch the Habitat 1 module to the ISS first, followed by the additional elements. The new assembly sequence will see the Payload, Power and Thermal module launch to the ISS first. This module could detach from the station -- and become a free flyer called Axiom Station -- as soon as 2028, according to the company. After that happens, Axiom will continue assembling the outpost, launching the Habitat 1 module to meet up with it. Habitat 1 will be followed by the airlock, the Habitat 2 module, and then the research and manufacturing facility. Angela Hart, a manager for the Commercial Low Earth Orbit Development Program at NASA's Johnson Space Center in Houston, said: \"The updated assembly sequence has been coordinated with NASA to support both NASA and Axiom Space needs and plans for a smooth transition in low Earth orbit.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Axiom's+Private+Space+Station+Could+Arrive+As+Early+As+2028%3A+https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F21%2F0048247%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F21%2F0048247%2Faxioms-private-space-station-could-arrive-as-early-as-2028%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://science.slashdot.org/story/24/12/21/0048247/axioms-private-space-station-could-arrive-as-early-as-2028?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23557583&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"10 Years Later: Malaysia To Resume Hunt For Flight MH370","url":"https://news.slashdot.org/story/24/12/21/0030258/10-years-later-malaysia-to-resume-hunt-for-flight-mh370?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1734751800,"author":"BeauHD","unread":true,"desc":"","content":"An anonymous reader quotes a report from Reuters: Malaysia has agreed to resume the search for the wreckage of missing Malaysia Airlines Flight MH370, its transport minister said on Friday, more than 10 years after it disappeared in one of the world's greatest aviation mysteries. Flight MH370, a Boeing 777 carrying 227 passengers and 12 crew, vanished en route from Kuala Lumpur to Beijing on March 8, 2014. \n[...] MH370's last transmission was about 40 minutes after it took off from Kuala Lumpur for Beijing. The pilots signed off as the plane entered Vietnamese air space over the Gulf of Thailand and soon after its transponder was turned off. \"Our responsibility and obligation and commitment is to the next of kin,\" Transport Minister Anthony Loke told a press conference. \"We hope this time will be positive, that the wreckage will be found and give closure to the families.\"\n \nFurther reading: Could Sea Explosions Finally Locate the 2014 Crash Site of Flight MH370?<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=10+Years+Later%3A+Malaysia+To+Resume+Hunt+For+Flight+MH370%3A+https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F21%2F0030258%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F21%2F0030258%2F10-years-later-malaysia-to-resume-hunt-for-flight-mh370%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://news.slashdot.org/story/24/12/21/0030258/10-years-later-malaysia-to-resume-hunt-for-flight-mh370?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23557565&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Cory Doctorow's Prescient Novella About Health Insurance and Murder","url":"https://news.slashdot.org/story/24/12/21/0021215/cory-doctorows-prescient-novella-about-health-insurance-and-murder?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1734747000,"author":"BeauHD","unread":true,"desc":"","content":"Five years ago, journalist and sci-fi author Cory Doctorow published a short story that explored the radicalization of individuals denied healthcare coverage. As The Guardian notes in a recent article, the story \"might seem eerily similar\" to the recent shooting of UnitedHealthcare's CEO. While it appears that the alleged shooter never read the story, Doctorow said: \"I feel like the most important thing about that is that it tells you that this is not a unique insight.\" Doctorow continued: \"that the question that I had is a question other people have had.\" As an activist in favor of liberalizing copyright laws and a proponent of the Creative Commons organization, it's important to note that Doctorow advocates for systemic reform through collective action rather than violence. Here's an excerpt from the The Guardian's article: In Radicalized, one of four novellas comprising a science fiction novel of the same name, Doctorow charts the journey of a man who joins an online forum for fathers whose partners or children have been denied healthcare coverage by their insurers after his wife is diagnosed with breast cancer and denied coverage for an experimental treatment. Slowly, over the course of the story, the men of the forum become radicalized by their grief and begin plotting -- and executing -- murders of health insurance executives and politicians who vote against universal healthcare.\n \nIn the wake of the December 4 shooting of UnitedHealthcare CEO Brian Thompson, which unleashed a wave of outrage at the U.S. health system, Doctorow's novella has been called prescient. When the American Prospect magazine republished the story last week, it wrote: \"It is being republished with permission for reasons that will become clear if you read it.\" But Doctorow doesn't think he was on to something that no one else in the U.S. understood. [...]\n \nIn one part of the story, a man whose young daughter died after an insurance company refused to pay for brain surgery bombs the insurer's headquarters. \"It's not vengeance. I don't have a vengeful bone in my body. Nothing I do will bring Lisa back, so why would I want revenge? This is a public service. There's another dad just like me,\" he shares in a video message on the forum. \"And right now, that dad is talking to someone at Cigna, or Humana, or BlueCross BlueShield, and the person on the phone is telling that dad that his little girl has. To. Die. Someone in that building made the decision to kill my little girl, and everyone else in that building went along with it. Not one of them is innocent, and not one of them is afraid. They're going to be afraid, after this.\"\n \n\"Because they must know in their hearts,\" he goes on. \"Them, their lobbyists, the men in Congress who enabled them. They're parents. They know. Anyone who hurt their precious children, they'd hunt that person down like a dog. The only amazing thing about any of this is that no one has done it yet. I'm going to make a prediction right now, that even though I'm the first, I sure as hell will not be the last. There's more to come.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Cory+Doctorow's+Prescient+Novella+About+Health+Insurance+and+Murder%3A+https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F21%2F0021215%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F21%2F0021215%2Fcory-doctorows-prescient-novella-about-health-insurance-and-murder%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://news.slashdot.org/story/24/12/21/0021215/cory-doctorows-prescient-novella-about-health-insurance-and-murder?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23557561&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"AMD ROCm 6.3.1 Released With Instinct MI325X Support, ROCm Runfile Installer","url":"https://www.phoronix.com/news/AMD-ROCm-6.3.1-Released","date":1734745666,"author":"Michael Larabel","unread":true,"desc":"","content":"Building off the ROCm 6.3 release from earlier this month, there's been a Friday night drop of ROCm 6.3.1 with some rather exciting end-of-year improvements...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"'Yes, I am a Human': Bot Detection Is No Longer Working","url":"https://tech.slashdot.org/story/24/12/20/2331225/yes-i-am-a-human-bot-detection-is-no-longer-working?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1734744600,"author":"BeauHD","unread":true,"desc":"","content":"The rise of AI has rendered traditional CAPTCHA tests increasingly ineffective, as bots can now \"[solve] these puzzles in milliseconds using artificial intelligence (AI),\" reports The Conversation. \"How ironic. The tools designed to prove we're human are now obstructing us more than the machines they're supposed to be keeping at bay.\" The report warns that the imminent arrival of AI agents -- software programs designed to autonomously interact with websites on our behalf -- will further complicate matters. From the report: Developers are continually coming up with new ways to verify humans. Some systems, like Google's ReCaptcha v3 (introduced in 2018), don't ask you to solve puzzles anymore. Instead, they watch how you interact with a website. Do you move your cursor naturally? Do you type like a person? Humans have subtle, imperfect behaviors that bots still struggle to mimic. Not everyone likes ReCaptcha v3 because it raises privacy issues -- plus the web company needs to assess user scores to determine who is a bot, and the bots can beat the system anyway. There are alternatives that use similar logic, such as \"slider\" puzzles that ask users to move jigsaw pieces around, but these too can be overcome.\n \nSome websites are now turning to biometrics to verify humans, such as fingerprint scans or voice recognition, while face ID is also a possibility. Biometrics are harder for bots to fake, but they come with their own problems -- privacy concerns, expensive tech and limited access for some users, say because they can't afford the relevant smartphone or can't speak because of a disability. The imminent arrival of AI agents will add another layer of complexity. It will mean we increasingly want bots to visit sites and do things on our behalf, so web companies will need to start distinguishing between \"good\" bots and \"bad\" bots. This area still needs a lot more consideration, but digital authentication certificates are proposed as one possible solution.\n \nIn sum, Captcha is no longer the simple, reliable tool it once was. AI has forced us to rethink how we verify people online, and it's only going to get more challenging as these systems get smarter. Whatever becomes the next technological standard, it's going to have to be easy to use for humans, but one step ahead of the bad actors. So the next time you find yourself clicking on blurry traffic lights and getting infuriated, remember you're part of a bigger fight. The future of proving humanity is still being written, and the bots won't be giving up any time soon.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status='Yes%2C+I+am+a+Human'%3A+Bot+Detection+Is+No+Longer+Working%3A+https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F20%2F2331225%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F20%2F2331225%2Fyes-i-am-a-human-bot-detection-is-no-longer-working%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://tech.slashdot.org/story/24/12/20/2331225/yes-i-am-a-human-bot-detection-is-no-longer-working?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23557515&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Wine 10.0-rc3 Released With A 16 Year Old Bug \"Fixed\"","url":"https://www.phoronix.com/news/Wine-10.0-rc3-Released","date":1734744062,"author":"Michael Larabel","unread":true,"desc":"","content":"The third weekly release candidate of Wine 10.0 is now available for testing with another 15 bugs fixed this week...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"EU Wants Apple To Open AirDrop and AirPlay To Android","url":"https://apple.slashdot.org/story/24/12/20/2320225/eu-wants-apple-to-open-airdrop-and-airplay-to-android?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1734742200,"author":"BeauHD","unread":true,"desc":"","content":"The EU is pushing Apple to make iOS more interoperable with other platforms, requiring features like AirDrop and AirPlay to work seamlessly with Android and third-party devices, while also enabling background app functionality and cross-platform notifications. 9to5Google reports: A new document released (PDF) by the European Commission this week reveals a number of ways the EU wants Apple to change iOS and its features to be more interoperable with other platforms. There are some changes to iOS itself, such as opening up notifications to work on third-party smartwatches as they do with the Apple Watch. Similarly, the EU wants Apple to let iOS apps work in the background as Apple's first-party apps do, as this is a struggle of some apps, especially companion apps for accessories such as smartwatches (other than the Apple Watch, of course). But there are also some iOS features that the EU directly wants Apple to open up to other platforms, including Android. [...]\n \nAs our sister site 9to5Mac points out, Apple has responded (PDF) to this EU document, prominently criticizing the EU for putting out a mandate that \"could expose your private information.\" Apple's document primarily focuses in on Meta, which the company says has made \"more interoperability requests\" than anyone else. Apple says that opening AirPlay to Meta would \"[create] a new class of privacy and security issues, while giving them data about users homes.\" The EU is taking consultation on this case until January 9, 2025, and if Apple doesn't comply when the order is eventually put into effect, it could result in heavy fines.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=EU+Wants+Apple+To+Open+AirDrop+and+AirPlay+To+Android%3A+https%3A%2F%2Fapple.slashdot.org%2Fstory%2F24%2F12%2F20%2F2320225%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fapple.slashdot.org%2Fstory%2F24%2F12%2F20%2F2320225%2Feu-wants-apple-to-open-airdrop-and-airplay-to-android%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://apple.slashdot.org/story/24/12/20/2320225/eu-wants-apple-to-open-airdrop-and-airplay-to-android?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23557513&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"10,000 Amazon Workers Go On Strike Ahead of Holiday Rush","url":"https://news.slashdot.org/story/24/12/20/2253215/10000-amazon-workers-go-on-strike-ahead-of-holiday-rush?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1734739800,"author":"BeauHD","unread":true,"desc":"","content":"An anonymous reader quotes a report from PCMag: Amazon employees are striking after the online retail giant missed a deadline to begin negotiations for a union contract. Roughly 10,000 employees have gone on strike as of Dec. 19. Workers are forming picket lines in New York City, Atlanta, Southern California, San Francisco, and Skokie, IL. Per a press release from the Teamsters, employees at other facilities have authorized strikes as well. Local unions are also putting up picket lines at hundreds of fulfillment centers nationwide, which could cause package delays ahead of the holidays.\n \n\"If your package is delayed during the holidays, you can blame Amazon's insatiable greed. We gave Amazon a clear deadline to come to the table and do right by our members. They ignored it,\" says Teamsters General President Sean M. O'Brien. \"These greedy executives had every chance to show decency and respect for the people who make their obscene profits possible. Instead, they've pushed workers to the limit and now they're paying the price. This strike is on them.\"\n \nThe Teamsters say this is \"the largest strike against Amazon in US history.\" Amazon tells CBS News it doesn't expect it to impact its operations; the company employs 1.5 million people in its warehouses and corporate offices. The workers claim that Amazon has engaged in illegal anti-union behavior while failing to provide employees with better pay and better working conditions. \"They talk a big game about taking care of their workers, but when it comes down to it, Amazon does not respect us and our right to negotiate for better working conditions and wages,\" said Gabriel Irizarry, a driver at DIL7 in Skokie, IL. \"We can't even afford to pay our bills.\" For its part, Amazon claims the Teamsters have \"continued to intentionally mislead the public\" about the situation.\n \nAn Amazon spokesperson told NBC News: \"The truth is that Teamsters have actively threatened, intimidated, and attempted to coerce Amazon employees and third-party drivers to join them, which is illegal and is the subject of multiple pending unfair labor practice charges against the union.\"\n \nYou can read the Teamster's press release here.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=10%2C000+Amazon+Workers+Go+On+Strike+Ahead+of+Holiday+Rush%3A+https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F20%2F2253215%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F20%2F2253215%2F10000-amazon-workers-go-on-strike-ahead-of-holiday-rush%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://news.slashdot.org/story/24/12/20/2253215/10000-amazon-workers-go-on-strike-ahead-of-holiday-rush?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23557485&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Sam Altman disputes Marc Andreessen’s description of AI meetings with Biden administration","url":"https://techcrunch.com/2024/12/20/sam-altman-disputes-marc-andreessens-description-of-ai-meetings-with-biden-administration/","date":1734739456,"author":"Charles Rollet","unread":true,"desc":"","content":"<p>Sam Altman dismissed what he called a \"conspiracy theory\" from a16z co-founder Marc Andreessen. </p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Justice Department Unveils Charges Against Alleged LockBit Developer","url":"https://yro.slashdot.org/story/24/12/20/2231217/justice-department-unveils-charges-against-alleged-lockbit-developer?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1734737400,"author":"BeauHD","unread":true,"desc":"","content":"The U.S. Department of Justice has charged Russian-Israeli national, Rostislav Panev, for his alleged role as a developer in the LockBit ransomware group, accused of designing malware and maintaining infrastructure for attacks that extorted over $500 million and caused billions in global damages. CyberScoop reports: The arrest is part of a broader campaign by international law enforcement agencies to dismantle LockBit. In February, a coordinated operation led by the U.K.'s National Crime Agency in cooperation with the FBI and the U.S. Justice Department disrupted LockBit's infrastructure, seizing websites and servers critical to its operations. These efforts significantly curtailed the group's ability to launch further attacks and extort victims.\n \nPanev is one of several individuals charged in connection with LockBit. Alongside him, other key figures have been indicted, including Dmitry Khoroshev, alleged to be \"LockBitSupp,\" the group's primary creator and administrator. Khoroshev, still at large, is accused of developing the ransomware and coordinating attacks on an international scale. The State Department has offered a reward of up to $10 million for his capture.\n \nMeanwhile, numerous members linked to LockBit remain fugitives, such as Russian nationals Artur Sungatov and Ivan Kondratyev, each facing charges for deploying ransomware against multiple industries globally. Mikhail Matveev, another alleged LockBit affiliate, is also at large, with a $10 million reward for his capture. Matveev was recently charged with computer crimes in Russia. You can read the full criminal complaint against Panev here (PDF).<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Justice+Department+Unveils+Charges+Against+Alleged+LockBit+Developer%3A+https%3A%2F%2Fyro.slashdot.org%2Fstory%2F24%2F12%2F20%2F2231217%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fyro.slashdot.org%2Fstory%2F24%2F12%2F20%2F2231217%2Fjustice-department-unveils-charges-against-alleged-lockbit-developer%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://yro.slashdot.org/story/24/12/20/2231217/justice-department-unveils-charges-against-alleged-lockbit-developer?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23557467&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"EV startup Canoo places remaining employees on a ‘mandatory unpaid break’","url":"https://techcrunch.com/2024/12/20/ev-startup-canoo-places-remaining-employees-on-a-mandatory-unpaid-break/","date":1734737309,"author":"Sean O'Kane","unread":true,"desc":"","content":"<p>Struggling electric van startup Canoo has placed its remaining employees on what it’s calling a “mandatory unpaid break” through at least the end of the year, according to an email obtained by TechCrunch. The company told employees they are being locked out of Canoo’s systems at the end of the day Friday, according to the [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Qualcomm Processors Properly Licensed From Arm, US Jury Finds","url":"https://yro.slashdot.org/story/24/12/20/2216253/qualcomm-processors-properly-licensed-from-arm-us-jury-finds?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1734735000,"author":"BeauHD","unread":true,"desc":"","content":"Jurors delivered a mixed verdict on Friday, ruling that Qualcomm had properly licensed its central processor chips from Arm. This decision effectively concludes Arm's lawsuit against Qualcomm, which had the potential to disrupt the global smartphone and PC chip markets.\n \nThe dispute stemmed from Qualcomm's $1.4 billion acquisition of chip startup Nuvia in 2021. Arm claimed Qualcomm breached contract terms by using Nuvia's designs without permission, while Qualcomm maintained its existing agreement covers the acquired technology. Arm demanded Qualcomm destroy the Nuvia designs created before the acquisition. Reuters reports: An eight-person jury in U.S. federal court deadlocked on the question of whether Nuvia, a startup that Qualcomm purchased for $1.4 billion in 2021, breached the terms of its license with Arm. But the jury found that Qualcomm did not breach Nuvia's license with Arm.\n \nThe jury also found that Qualcomm's chips created using Nuvia technology, which have been central to Qualcomm's push into the personal computer market, are properly licensed under its own agreement with Arm, clearing the way for Qualcomm to continue selling them.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Qualcomm+Processors+Properly+Licensed+From+Arm%2C+US+Jury+Finds%3A+https%3A%2F%2Fyro.slashdot.org%2Fstory%2F24%2F12%2F20%2F2216253%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fyro.slashdot.org%2Fstory%2F24%2F12%2F20%2F2216253%2Fqualcomm-processors-properly-licensed-from-arm-us-jury-finds%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://yro.slashdot.org/story/24/12/20/2216253/qualcomm-processors-properly-licensed-from-arm-us-jury-finds?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23557465&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Arizona's Getting an Online Charter School Taught Entirely By AI","url":"https://news.slashdot.org/story/24/12/20/2211207/arizonas-getting-an-online-charter-school-taught-entirely-by-ai?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1734732660,"author":"BeauHD","unread":true,"desc":"","content":"An anonymous reader quotes a report from TechCrunch: The newest online-only school greenlighted (PDF) by the Arizona State Board for Charter Schools comes with a twist: The academic curriculum will be taught entirely by AI. Charter schools -- independently operated but publicly funded -- typically get greater autonomy compared to traditional public schools when it comes to how subjects are taught. But Unbound Academy's application, which proposes an \"AI-driven adaptive learning technology\" that \"condenses academic instruction into a two-hour window,\" is a first for the model. (Unbound's founders have been running a similar program at a \"high-end private school\" in Texas, which appears to be in-person.)\n \nUnbound's approach leans on edtech platforms like IXL and Khan Academy, and students engage with \"interactive, AI-powered platforms that continuously adjust to their individual learning pace and style.\" There will be humans, just fewer of them, and maybe not actual accredited teachers: It will adopt a \"human-in-the-loop\" approach with \"skilled guides\" monitoring progress who can provide \"targeted interventions\" and coaching for each student. Academic instruction is whittled down to just two hours. The remainder of the students' day will include \"life-skills workshops\" covering areas such as critical thinking, creative problem-solving, financial literacy, public speaking, goal setting, and entrepreneurship. The online-only school targets students from fourth to eighth grades.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Arizona's+Getting+an+Online+Charter+School+Taught+Entirely+By+AI%3A+https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F20%2F2211207%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F20%2F2211207%2Farizonas-getting-an-online-charter-school-taught-entirely-by-ai%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://news.slashdot.org/story/24/12/20/2211207/arizonas-getting-an-online-charter-school-taught-entirely-by-ai?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23557463&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"12 days of OpenAI: The Ars Technica recap","url":"https://arstechnica.com/information-technology/2024/12/12-days-of-openai-the-ars-technica-recap/","date":1734732094,"author":"Benj Edwards","unread":true,"desc":"","content":"\n              <p>Over the past 12 business days, OpenAI has announced a new product or demoed an AI feature every weekday, calling the PR event \"<a href=\"https://arstechnica.com/ai/2024/12/openai-teases-12-days-of-mystery-product-launches-starting-tomorrow/\">12 days of OpenAI</a>.\" We've covered some of the major announcements, but we thought a look at each announcement might be useful for people seeking a comprehensive look at each day's developments.</p>\n<p>The timing and rapid pace of these announcements—particularly in light of <a href=\"https://arstechnica.com/information-technology/2024/12/google-and-openai-blitz-december-with-so-many-ai-releases-its-hard-to-keep-up/\">Google's competing releases</a>—illustrates the intensifying competition in AI development. What might normally have been spread across months was compressed into just 12 business days, giving users and developers a lot to process as they head into 2025.</p>\n<p>Humorously, we asked ChatGPT what it thought about the whole series of announcements, and it was skeptical that the event even took place. \"The rapid-fire announcements over 12 days seem plausible,\" wrote ChatGPT-4o, \"But might strain credibility without a clearer explanation of how OpenAI managed such an intense release schedule, especially given the complexity of the features.\"</p><p><a href=\"https://arstechnica.com/information-technology/2024/12/12-days-of-openai-the-ars-technica-recap/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/information-technology/2024/12/12-days-of-openai-the-ars-technica-recap/#comments\">Comments</a></p>\n\n            ","flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2024/12/12_days_robot_sleigh-1152x648.jpg","enclosureMime":""},{"title":"After causing outrage on the first day of Y Combinator, AI code editor PearAI lands $1M seed","url":"https://techcrunch.com/2024/12/20/after-causing-outrage-on-the-first-day-of-y-combinator-ai-code-editor-pearai-lands-1m-seed/","date":1734730490,"author":"Julie Bort","unread":true,"desc":"","content":"<p>On the first day of Y Combinator the founders of PearAI got “cancelled.\" They used the hate to launch a new product, raise $1 million.</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Third member of LockBit ransomware gang has been arrested","url":"https://techcrunch.com/2024/12/20/third-member-of-lockbit-ransomware-gang-has-been-arrested/","date":1734726017,"author":"Zack Whittaker","unread":true,"desc":"","content":"<p>LockBit is believed tobe responsible for at least $500 million in ransom payments alone. </p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Feds clear the way for robotaxis without steering wheels and pedals","url":"https://techcrunch.com/2024/12/20/feds-clear-the-way-for-robotaxis-without-steering-wheel-and-pedals/","date":1734725885,"author":"Rebecca Bellan","unread":true,"desc":"","content":"<p>The National Highway Traffic Safety Administration (NHTSA) on Friday proposed a new national framework that could make it easier for companies to deploy at scale autonomous vehicles without traditional manual driving controls &#8212; like steering wheels, pedals, and sideview mirrors.  The guidelines also require AV companies to share a whole lot more safety data with [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"ChatGPT: Everything you need to know about the AI-powered chatbot","url":"https://techcrunch.com/2024/12/20/chatgpt-everything-to-know-about-the-ai-chatbot/","date":1734725700,"author":"Kyle Wiggers, Cody Corrall, Alyssa Stringer","unread":true,"desc":"","content":"<p>ChatGPT, OpenAI’s text-generating AI chatbot, has taken the world by storm since its launch in November 2022. What started as a tool to supercharge productivity through writing essays and code with short text prompts has evolved into a behemoth with 300 million weekly active users. 2024 has been a big year for OpenAI, from its [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"VCs pledge not to take money from Russia or China, and Databricks raises a humongous round","url":"https://techcrunch.com/2024/12/20/vcs-pledge-not-to-take-money-from-russia-china-databricks-raises-humongous-round/","date":1734725100,"author":"Anna Heim","unread":true,"desc":"","content":"<p>Welcome to Startups Weekly — your weekly recap of everything you can’t miss from the world of startups. Want it in your inbox every Friday? Sign up here. This week was full of news, likely because it is also the last &#8220;real&#8221; week of 2024. Which is another way for us to say goodbye for [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Linux 6.13-rc4 To Fix A Nasty USB Problem Plaguing The Kernel For A Few Weeks","url":"https://www.phoronix.com/news/Linux-6.13-rc4-USB-Fix","date":1734724500,"author":"Michael Larabel","unread":true,"desc":"","content":"Merged to Linux Git minutes ago and ahead of the Linux 6.13-rc4 tagging on Sunday were this week's set of USB fixes that are particularly noteworthy. Most significant is fixing a USB regression that had been present in the stack since the Linux 6.13 merge window last month...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"OpenAI announces o3 and o3-mini, its next simulated reasoning models","url":"https://arstechnica.com/information-technology/2024/12/openai-announces-o3-and-o3-mini-its-next-simulated-reasoning-models/","date":1734723103,"author":"Benj Edwards","unread":true,"desc":"","content":"\n              <p>On Friday, during Day 12 of its \"<a href=\"https://arstechnica.com/ai/2024/12/openai-teases-12-days-of-mystery-product-launches-starting-tomorrow/\">12 days of OpenAI</a>,\" OpenAI CEO Sam Altman announced its latest AI \"reasoning\" models, o3 and o3-mini, which build upon the <a href=\"https://arstechnica.com/information-technology/2024/09/openais-new-reasoning-ai-models-are-here-o1-preview-and-o1-mini/\">o1 models</a> launched earlier this year. The company is not releasing them yet but will make these models available for public safety testing and research access today.</p>\n<p>The models use what OpenAI calls \"private chain of thought,\" where the model pauses to examine its internal dialog and plan ahead before responding, which you might call \"simulated reasoning\" (SR)—a form of AI that goes beyond basic large language models (LLMs).</p>\n<p>The company named the model family \"o3\" instead of \"o2\" to avoid potential trademark conflicts with British telecom provider O2, according to <a href=\"https://www.theinformation.com/briefings/openai-preps-o3-reasoning-model\">The Information</a>. During Friday's livestream, Altman acknowledged his company's naming foibles, saying, \"In the grand tradition of OpenAI being really, truly bad at names, it'll be called o3.\"</p><p><a href=\"https://arstechnica.com/information-technology/2024/12/openai-announces-o3-and-o3-mini-its-next-simulated-reasoning-models/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/information-technology/2024/12/openai-announces-o3-and-o3-mini-its-next-simulated-reasoning-models/#comments\">Comments</a></p>\n\n            ","flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2024/12/openai_o3_model-1152x648.jpg","enclosureMime":""},{"title":"AMD Launches A YouTube Channel For Developers","url":"https://www.phoronix.com/news/AMD-Developer-Central-YouTube","date":1734721396,"author":"Michael Larabel","unread":true,"desc":"","content":"If you are looking for some interesting technical content to watch over the holidays or end-of-year downtime, AMD shared today that they have launched their own YouTube channel for developer-related content...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Nvidia clears regulatory hurdle to acquire Run:ai","url":"https://techcrunch.com/2024/12/20/nvidia-clears-regulatory-hurdle-to-acquire-runai/","date":1734720893,"author":"Rebecca Szkutak","unread":true,"desc":"","content":"<p>Chip company Nvidia gets the green light from the European Union to complete its acquisition of Run:ai. The EU came to a unanimous decision today that Nvidia could go ahead with its acquisition of Israeli GPU orchestration platform Run:ai, according to reporting from Bloomberg. The European Commission determined that if the merger went through, other [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Google is expanding Gemini’s in-depth research mode to 40 languages","url":"https://techcrunch.com/2024/12/20/google-is-expanding-geminis-in-depth-research-mode-to-40-languages/","date":1734718885,"author":"Ivan Mehta","unread":true,"desc":"","content":"<p>Google said Friday that the company is expanding Gemini&#8217;s latest in-depth research mode to 40 more languages. The company launched the in-depth research mode earlier this month, allowing Google One AI premium plan users to unlock an AI-powered research assistant of sorts. The in-depth function works in a multi-step method, from creating a research plan [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Here’s the full list of 49 US AI startups that have raised $100M or more in 2024","url":"https://techcrunch.com/2024/12/20/heres-the-full-list-of-49-us-ai-startups-that-have-raised-100m-or-more-in-2024/","date":1734718365,"author":"Rebecca Szkutak","unread":true,"desc":"","content":"<p>In the first half of 2024 alone, more than $35.5 billion was invested into AI startups globally.</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Video Friday: Happy Holidays!","url":"https://spectrum.ieee.org/video-friday-holidays-2024","date":1734717605,"author":"Evan Ackerman","unread":true,"desc":"","content":"<p>Your weekly selection of awesome robot videos</p>","flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTM4ODM2My9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc1MzgyMDg3Mn0.04JpzJOBZGw50OWQZcVXINm-ZXzlDB56Pc0JLK58iHs/image.png?width=600","enclosureMime":""},{"title":"OpenAI announces new o3 models","url":"https://techcrunch.com/2024/12/20/openai-announces-new-o3-model/","date":1734717417,"author":"Maxwell Zeff, Kyle Wiggers","unread":true,"desc":"","content":"<p>OpenAI saved its biggest announcement for the last day of its 12-day &#8220;shipmas&#8221; event. On Friday, the company unveiled o3, the successor to the o1 &#8220;reasoning&#8221; model it released earlier in the year. o3 is a model family, to be more precise — as was the case with o1. There&#8217;s o3 and o3-mini, a smaller, [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"OpenSUSE Announces New \"YQPkg\" Package Management Tool","url":"https://www.phoronix.com/news/openSUSE-YQPkg","date":1734714754,"author":"Michael Larabel","unread":true,"desc":"","content":"The openSUSE project announced today YQPkg as a new package management tool for openSUSE Linux distributions...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"OpenAI 2024 event: How to watch new ChatGPT product reveals and demos","url":"https://techcrunch.com/2024/12/20/openai-2024-event-how-to-watch-new-chatgpt-product-reveals-and-demos/","date":1734714000,"author":"Cody Corrall","unread":true,"desc":"","content":"<p>OpenAI is in the holiday spirit, it seems. The ChatGPT series of reveals, called “12 Days of OpenAI,” will be streamed live at 10 a.m. PT each weekday through December 20. So far, we&#8217;ve seen the launch of ChatGPT Pro, OpenAI’s $200 per month subscription plan; the full version of its “reasoning” o1 model; the [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Swizzle Ventures raises $5M for inaugural fund addressing women’s health and wealth","url":"https://techcrunch.com/2024/12/20/swizzle-ventures-raises-5m-for-inaugural-fund-addressing-womens-health-and-wealth/","date":1734711759,"author":"Dominic-Madori Davis","unread":true,"desc":"","content":"<p>There is a new venture fund in town. Swizzle Ventures, founded by Jessica Kamada, former COO of the marketing agency Bamboo, has raised just over $5 million for its Fund I, according to an SEC filing. There was no target raise amount.  The firm, which quietly opened in 2023, is an early-stage firm looking to [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Wayland Protocols 1.39 Released With Data Control & Workspace Additions","url":"https://www.phoronix.com/news/Wayland-Protocols-1.39","date":1734710871,"author":"Michael Larabel","unread":true,"desc":"","content":"Jonas Ådahl of Red Hat just released Wayland Protocols 1.39 as the latest set of updates to this de facto repository for Wayland protocols...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Lerer Hippeau files to raise Fund IX","url":"https://techcrunch.com/2024/12/20/lerer-hippeau-files-to-a-raise-fund-ix/","date":1734710464,"author":"Dominic-Madori Davis","unread":true,"desc":"","content":"<p>Lerer Hippeau, one of New York's most prolific and A-list VC firms, has filed to raise its ninth fund, according to an SEC filing made Wednesday.</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The AI war between Google and OpenAI has never been more heated","url":"https://arstechnica.com/information-technology/2024/12/google-and-openai-blitz-december-with-so-many-ai-releases-its-hard-to-keep-up/","date":1734709445,"author":"Benj Edwards","unread":true,"desc":"","content":"\n              <p>Over the past month, we've seen a rapid cadence of notable AI-related announcements and releases from both Google and OpenAI, and it's been making the AI community's head spin. It has also poured fuel on the fire of the OpenAI-Google rivalry, an accelerating game of one-upmanship taking place unusually close to the Christmas holiday.</p>\n<p>\"How are people surviving with the firehose of AI updates that are coming out,\" <a href=\"https://x.com/BowTiedFox/status/1867673263846969607\">wrote</a> one user on X last Friday, which is still a hotbed of AI-related conversation. \"in the last &lt;24 hours we got gemini flash 2.0 and chatGPT with screenshare, deep research, pika 2, sora, chatGPT projects, anthropic clio, wtf it never ends.\"</p>\n<p>Rumors travel quickly in the AI world, and people in the AI industry had been expecting OpenAI to ship some major products in December. Once OpenAI announced \"<a href=\"https://arstechnica.com/ai/2024/12/openai-teases-12-days-of-mystery-product-launches-starting-tomorrow/\">12 days of OpenAI</a>\" earlier this month, Google jumped into gear and seemingly decided to try to one-up its rival on several counts. So far, the strategy appears to be working, but it's coming at the cost of the rest of the world being able to absorb the implications of the new releases.</p><p><a href=\"https://arstechnica.com/information-technology/2024/12/google-and-openai-blitz-december-with-so-many-ai-releases-its-hard-to-keep-up/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/information-technology/2024/12/google-and-openai-blitz-december-with-so-many-ai-releases-its-hard-to-keep-up/#comments\">Comments</a></p>\n\n            ","flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2024/12/speedy_wind_dog-1-1152x648.jpg","enclosureMime":""},{"title":"Arch Linux Based CachyOS Takes The Lead On Intel Arrow Lake","url":"https://www.phoronix.com/review/intel-arrowlake-cachyos","date":1734708818,"author":"Michael Larabel","unread":true,"desc":"","content":"Following the recent Intel Core Ultra 9 285K Windows 11 vs. Ubuntu Linux benchmarks I wanted to expand the testing to look at how well other Linux distributions as well were performing on this new 24-core Arrow Lake desktop processor. To much surprise Intel's own Clear Linux distribution didn't take the top spot this round but as a surprising upset the Arch Linux based CachyOS distribution outperformed Clear Linux, Ubuntu, Arch Linux, and Fedora Workstation on this flagship Arrow Lake processor.","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"DXVK 2.5.2 Brings Fixes & Optimizations For Direct3D 9 / 10 / 11 Atop Vulkan","url":"https://www.phoronix.com/news/DXVK-2.5.2-Released","date":1734702489,"author":"Michael Larabel","unread":true,"desc":"","content":"DXVK 2.5.2 is out today as the newest point release to this open-source software implementing the Direct3D 9 / 10 / 11 APIs atop Vulkan for powering Windows games on Valve's Steam Play (Proton) as well as being used by other software and some games directly...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Liquorix vs. Linux 6.12 Upstream Kernel Performance Across Many Workloads","url":"https://www.phoronix.com/news/Linux-6.12-Liquorix-Performance","date":1734700495,"author":"Michael Larabel","unread":true,"desc":"","content":"A Phoronix Premium subscriber a while back requested some fresh benchmarks of how the Liquorix downstream of the Linux kernel is comparing against the latest upstream kernel... Here are some benchmarks looking at the Liquorix flavor of the Linux kernel compared to upstream Linux 6.12...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Ryzen AI NPU6 Support Added To AMDXDNA Driver For Linux 6.14 Debut","url":"https://www.phoronix.com/news/Ryzen-AI-NPU6-Linux-6.14","date":1734694248,"author":"Michael Larabel","unread":true,"desc":"","content":"The latest round of drm-misc-next material was sent out yesterday to DRM-Next in advance of the upcoming Linux 6.14 merge window...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Weighted Interleave Auto-Tuning Being Worked On For Linux","url":"https://www.phoronix.com/news/Linux-Weight-Interleave-Auto","date":1734692678,"author":"Michael Larabel","unread":true,"desc":"","content":"Joshua Hahn has posted the latest \"request for comments\" draft working on weightedd interleave auto-tuning for the linux kernel in order to better enhance the performance characteristics of primarily Linux servers with multiple memory nodes...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"OpenMoonRay 1.7 Brings More NVIDIA GPU Acceleration, Additional Features","url":"https://www.phoronix.com/news/OpenMoonRay-1.7","date":1734691680,"author":"Michael Larabel","unread":true,"desc":"","content":"In early 2023 DreamWorks open-sourced their MoonRay renderer as OpenMoonRay. Since then they have continued advancing this award-winning production MCRT renderer. Before closing out 2024 they have now released OpenMoonRay 1.7...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Not to be outdone by OpenAI, Google releases its own “reasoning” AI model","url":"https://arstechnica.com/information-technology/2024/12/not-to-be-outdone-by-openai-google-releases-its-own-reasoning-ai-model/","date":1734644993,"author":"Benj Edwards","unread":true,"desc":"","content":"\n              <p>It's been a really busy month for Google as it apparently endeavors to outshine OpenAI with a blitz of AI releases. On Thursday, Google dropped its latest party trick: <a href=\"https://ai.google.dev/gemini-api/docs/thinking-mode\">Gemini 2.0 Flash Thinking Experimental</a>, which is a new AI model that uses runtime \"reasoning\" techniques similar to <a href=\"https://arstechnica.com/ai/2024/12/openais-new-200-mo-chatgpt-subscription-will-buy-you-more-compute-time/\">OpenAI's o1</a> to achieve \"deeper thinking\" on problems fed into it.</p>\n<p>The experimental model builds on Google's newly released <a href=\"https://arstechnica.com/information-technology/2024/12/google-goes-agentic-with-gemini-2-0s-ambitious-ai-agent-features/\">Gemini 2.0 Flash</a> and runs on its AI Studio platform, but early tests <a href=\"https://techcrunch.com/2024/12/19/google-releases-its-own-reasoning-ai-model/\">conducted</a> by TechCrunch reporter Kyle Wiggers reveal accuracy issues with some basic tasks, such as incorrectly counting that the word \"strawberry\" contains two R's.</p>\n<p>These so-called reasoning models differ from standard AI models by incorporating feedback loops of self-checking mechanisms, similar to techniques we <a href=\"https://arstechnica.com/information-technology/2023/04/hype-grows-over-autonomous-ai-agents-that-loop-gpt-4-outputs/\">first saw in early 2023</a> with hobbyist projects like \"Baby AGI.\" The process requires more computing time, often adding extra seconds or minutes to response times. Companies have turned to reasoning models as traditional scaling methods at training time have been showing diminishing returns.</p><p><a href=\"https://arstechnica.com/information-technology/2024/12/not-to-be-outdone-by-openai-google-releases-its-own-reasoning-ai-model/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/information-technology/2024/12/not-to-be-outdone-by-openai-google-releases-its-own-reasoning-ai-model/#comments\">Comments</a></p>\n\n            ","flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2024/12/the_thinker-1152x648.jpg","enclosureMime":""},{"title":"As firms abandon VMware, Broadcom is laughing all the way to the bank","url":"https://arstechnica.com/information-technology/2024/12/as-firms-abandon-vmware-broadcom-is-laughing-all-the-way-to-the-bank/","date":1734640862,"author":"Scharon Harding","unread":true,"desc":"","content":"\n              <p>Another company has publicly <a href=\"https://arstechnica.com/information-technology/2024/10/a-year-after-broadcoms-vmware-buy-customers-eye-exit-strategies/\">cut ties with Broadcom's VMware</a>. This time, it's Ingram Micro, one of the world's biggest IT distributors. The announcement comes as Broadcom eyes services as a key part of maintaining VMware business in 2025. But even as some customers are reducing reliance on VMware, its <a href=\"https://www.reuters.com/technology/broadcom-rallies-forecast-booming-ai-chip-demand-2024-12-13/\">trillion-dollar</a> owner is laughing all the way to the bank.</p>\n<h2>IT distributor severs VMware ties</h2>\n<p>Ingram is reducing its Broadcom-related business to \"limited engagement with VMware in select regions,\" a spokesperson told <a href=\"https://www.theregister.com/2024/12/16/ingram_micro_vmware_broadcom_deal_ends/\">The Register</a> this week.</p>\n<p>\"We were unable to reach an agreement with Broadcom that would help our customers deliver the best technology outcomes now and in the future while providing an appropriate shareholder return,” the spokesperson said.</p><p><a href=\"https://arstechnica.com/information-technology/2024/12/as-firms-abandon-vmware-broadcom-is-laughing-all-the-way-to-the-bank/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/information-technology/2024/12/as-firms-abandon-vmware-broadcom-is-laughing-all-the-way-to-the-bank/#comments\">Comments</a></p>\n\n            ","flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2024/12/GettyImages-1503144949-1152x648.jpg","enclosureMime":""},{"title":"New physics sim trains robots 430,000 times faster than reality","url":"https://arstechnica.com/information-technology/2024/12/new-physics-sim-trains-robots-430000-times-faster-than-reality/","date":1734639029,"author":"Benj Edwards","unread":true,"desc":"","content":"\n              <p>On Thursday, a large group of university and private industry researchers unveiled <a href=\"https://genesis-embodied-ai.github.io/\">Genesis</a>, a new open source computer simulation system that lets robots practice tasks in simulated reality 430,000 times faster than in the real world. Researchers can also use an AI agent to generate 3D physics simulations from text prompts.</p>\n<p>The accelerated simulation means a neural network for piloting robots can spend the virtual equivalent of decades learning to pick up objects, walk, or manipulate tools during just hours of real computer time.</p>\n<p>\"One hour of compute time gives a robot 10 years of training experience. That's how Neo was able to learn martial arts in a blink of an eye in the Matrix Dojo,\" <a href=\"https://x.com/DrJimFan/status/1869795912597549137\">wrote</a> Genesis paper co-author Jim Fan on X, who says he played a \"minor part\" in the research. Fan has previously worked on <a href=\"https://arstechnica.com/information-technology/2024/03/nvidia-announces-moonshot-to-create-embodied-human-level-ai-in-robot-form/\">several</a> <a href=\"https://arstechnica.com/information-technology/2023/10/eureka-uses-gpt-4-and-massively-parallel-simulations-to-accelerate-robot-training/\">robotics simulation</a> projects for Nvidia.</p><p><a href=\"https://arstechnica.com/information-technology/2024/12/new-physics-sim-trains-robots-430000-times-faster-than-reality/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/information-technology/2024/12/new-physics-sim-trains-robots-430000-times-faster-than-reality/#comments\">Comments</a></p>\n\n            ","flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2024/12/genesis_teapot-1152x648.jpg","enclosureMime":""},{"title":"Intel & Canonical Collaborate On Graphics Preview Stack For Ubuntu 24.10","url":"https://www.phoronix.com/news/Intel-Gfx-Preview-Ubuntu-24.10","date":1734636792,"author":"Michael Larabel","unread":true,"desc":"","content":"Intel and Canonical have been collaborating to provide an early \"Graphics Preview\" stack for Ubuntu 24.10 to provide better support for the new Intel Core Ultra Series 2 \"Lunar Lake\" and Intel Arc B-Series \"Battlemage\" graphics...","flags":null,"enclosureUrl":"","enclosureMime":""}]}